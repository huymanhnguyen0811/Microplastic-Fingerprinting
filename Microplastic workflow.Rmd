---
title: "Microplastic fingerprinting"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width='750px', dpi=200)
```

## Documentation

This repo is accompanying the publication: "Computational fingerprinting workflow for environmental source tracking of microplastic based on representative additives"

Demo of each ML algorithms is shown below.

## Loading Packages and Functions
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# #########################
##   Load Libraries    ##
#########################
library(ggplot2)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(writexl)
library(tidyr)
library(grid)
library(gridExtra)
library(stats)
library(FactoMineR)
library(factoextra)
library(ggforce)
library(cluster)
library(MASS)  # For mvrnorm
library(caret)
library(randomForest)
library(pheatmap)
library(viridis)
library(missForest)
library(pROC)
library(VIM)
library(softImpute)
library(purrr)

# Functions -------------------------------------------------------------------------------------------------------
'%notin%' <- Negate('%in%')

# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp <- function(data, split_time, rtthres1, rtthres2, mzthres, type) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rtthres: RT threshold window
  # mzthres: mz threshold window
  dat <- data[ , , drop = FALSE]  # Shallow copy to avoid modification of original data
  
  # Initialize the compound column filled with NA values
  dat$Feature <- NA
  i <- 1
  j <- 1 # Counter for HPLC data only
  
  # Checkpoint for 'type' parameter
  if (!(type %in% c("ATDGCMS", "HPLCTOFMS"))) {
    stop("Invalid type specified. Must be either 'ATDGCMS' or 'HPLCTOFMS'")
  }

  # Loop through each row to group compounds
  if (type %in% "HPLCTOFMS") {
      # Split HPLC data into RT <= 6 min and RT > 6 min
      dat1 <- dat %>% filter(RT <= split_time)
      dat2 <- dat %>% filter(RT > split_time)
      
      for (row in 1:nrow(dat1)) {
        # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
        rt <- base::as.numeric(dat1[row, "RT"])
        mz <- base::as.numeric(dat1[row, "m.z"])
        
        idx1 <- which(
          dat1$RT <= (rt + rtthres1) & dat1$RT >= (rt - rtthres1) &
            dat1$m.z <= (mz + mzthres) & dat1$m.z >= (mz - mzthres) &
            is.na(dat1$Feature)
        )
        
        if (length(idx1) > 0) {
          dat1[idx1, "Feature"] <- paste0("Compound_RT1_", i, ".", type)
          i <- i + 1
        }  
      }
      
      for (row in 1:nrow(dat2)) {
        # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
        rt <- base::as.numeric(dat2[row, "RT"])
        mz <- base::as.numeric(dat2[row, "m.z"])
        
        idx2 <- which(
          dat2$RT <= (rt + rtthres2) & dat2$RT >= (rt - rtthres2) &
            dat2$m.z <= (mz + mzthres) & dat2$m.z >= (mz - mzthres) &
            is.na(dat2$Feature)
        )
        
        if (length(idx2) > 0) {
          dat2[idx2, "Feature"] <- paste0("Compound_RT2_", j, ".", type)
          j <- j + 1
        }  
      }
      
      dat <- rbind(dat1, dat2)
      
  } else {
    for (row in 1:nrow(dat)) {
      
      # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
      rt <- base::as.numeric(dat[row, "RT"])
      mz <- base::as.numeric(dat[row, "m.z"])
      
      idx <- which(
        dat$RT <= (rt + rtthres1) & dat$RT >= (rt - rtthres1) &
          dat$m.z <= (mz + mzthres) & dat$m.z >= (mz - mzthres) &
          is.na(dat$Feature)
      )
      
      if (length(idx) > 0) {
        dat[idx, "Feature"] <- paste0("Compound_", i, ".", type)
        i <- i + 1
      }  
    }
  }
  
  return(dat)
}

# Filtering similar and unique compound 
comp_filter <- function(data) {
  # Get all unique feature groups once
  unique_comp_groups <- unique(data$Feature)
  
  # Preallocate lists for indices
  all_other_compounds_idx <- vector("list", length(unique_comp_groups))
  all_unique_compounds_idx <- vector("list", length(unique_comp_groups))

  # Loop over unique compound groups, but avoid appending to vectors
  for (i in seq_along(unique_comp_groups)) {
    comp_grp <- unique_comp_groups[i]
    
    # Use vectorized comparison instead of grepl with fixed=TRUE
    idx <- which(data$Feature == comp_grp)
    if (length(unique(data[idx,]$File)) < 2) {
      all_unique_compounds_idx[[i]] <- idx
    } else {
      all_other_compounds_idx[[i]] <- idx
    }
  }
  
  # Unlist and remove empty elements (NULLs)
  all_other_compounds_idx <- unlist(all_other_compounds_idx, use.names = FALSE)
  all_unique_compounds_idx <- unlist(all_unique_compounds_idx, use.names = FALSE)
  
  return(list(all_other_compounds_idx, all_unique_compounds_idx))
}

library(data.table)

comp_filter_opt <- function(data) {
  # Convert the data frame to a data.table
  dt <- as.data.table(data)
  
  # For each feature, compute the number of unique File values
  dt[, n_files := uniqueN(File), by = Feature]
  
  # Extract the indices based on the unique file count
  all_unique_compounds_idx <- which(dt$n_files < 2)
  all_other_compounds_idx  <- which(dt$n_files >= 2)
  
  return(list(all_other_compounds_idx = all_other_compounds_idx, 
              all_unique_compounds_idx  = all_unique_compounds_idx))
}


```

## STEP 1.1: Data import
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic Manuscript 1/microplastic manuscript 1 - data/ATDGCMS")

atdgcms_list <- list.files(pattern = '*.csv', full.names = TRUE) %>%
  .[!str_detect(., "2022")] %>%
  # remove USE samples with only 1 replication
  .[!str_detect(., "USE_01")] %>%
  .[!str_detect(., "USE_02")] %>%
  .[!str_detect(., "USE_05")] %>%
  .[!str_detect(., "USE_06")] %>%
  .[!str_detect(., "USE_09")] %>%
  .[!str_detect(., "USE_10")] %>%
  .[!str_detect(., "USE_11")] %>%
  .[!str_detect(., "USE_12")] %>%
  .[!str_detect(., "USE_13")] %>%
  .[!str_detect(., "USE_15")] %>%
  .[!str_detect(., "USE_16")] %>%
  .[!str_detect(., "USE_17")] %>%
  .[!str_detect(., "USE_18")] %>%
  .[!str_detect(., "USE_19")] %>%
  .[!str_detect(., "USE_20")] %>%
  .[!str_detect(., "USSB_01")] %>%
  .[!str_detect(., "USSB_08")] # %>%
  # .[!str_detect(., "USSB_09")] # %>%
  # .[str_detect(., "_USSB")] #  environmental samples

# Blank samples 
blank_list <- list.files(pattern = '*.csv', full.names = TRUE) %>%
  .[str_detect(., "2022-")]

# Import samples to list
# Function to read each file and add a "File" column
read_and_add_filename <- function(file) {
  df <- read.csv(file) %>%
    mutate(File = basename(file))  # Add a "File" column with the file name
  return(df)
}

# Apply the function to all files and combine them into a single dataframe
atdgcms_step1.1 <- lapply(atdgcms_list, read_and_add_filename) %>%
  bind_rows() %>%  # Combine all dataframes into one
  # dplyr::bind_rows(purrr::map(atdgcms_list, read.csv)) %>%
  dplyr::select(-c("Start", "End", "Width", "Base.Peak", "Cpd", "Label", "Height", "Ions")) %>%
  # # Change all ATDGCMS file name with underscore-separated to hyphen-separated
  dplyr::mutate(File = gsub("_", "-", File)) %>%
  dplyr::mutate(File = gsub(".csv", "", File)) %>%
  mutate(type = "Sample")

atdgcms_blank <- dplyr::bind_rows(purrr::map(blank_list, read.csv)) %>%
  dplyr::select(c("Area", "File", "m.z", "RT")) %>%
  mutate(type = "Blanks")

atdgcms_step1.1 <- rbind(atdgcms_step1.1, atdgcms_blank) %>% 
  arrange(RT) %>%
  mutate(Replicate = ifelse(str_detect(File, "R1"), "R1",
                            ifelse(str_detect(File, "R2"), "R2", "R3"))) %>%
  mutate(simplified_file = map_chr(File, ~ paste(str_split_1(.x, pattern = "-")[c(4, 5)], collapse = "-")))

# HPLCTOFMS

###### Import Batch 0
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/HPLCTOFMS/EF_Non-target data_Batch 0")

hplc_batch0 <- list.files(pattern = '*.xls') %>%
  .[!str_detect(., "Blank")]

# Blank samples 
blank_hplc_batch0 <- list.files(pattern = '*.xls') %>%
  .[str_detect(., "Blank")]

# Import samples to list
hplc_batch0_step1.1 <- dplyr::bind_rows(purrr::map(hplc_batch0, read_xls, skip = 1), 
                                        purrr::map(blank_hplc_batch0, read_xls, skip = 1)) %>%
  dplyr::select(m.z = `m/z`, RT, Height, File) %>%
  mutate(File = gsub("_", "-", File),
         type = ifelse(File %in% names(hplc_batch0), "Sample", "Blanks"),
         Day = case_when(str_detect(File, "Day0") ~ "0",
                         str_detect(File, "Day2") ~ "2",
                         str_detect(File, "Day15") ~ "15",
                         TRUE ~ "34"),
         Replicate = ifelse(str_detect(File, "R1"), "R1", "R2"),
         Batch_number = "0",
         simplified_file = map_chr(File, ~ paste(str_split_1(.x, "-")[1:2], collapse = "-")))

colnames(hplc_batch0_step1.1)[1] <- "m.z"

###### Import Batch 1
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/HPLCTOFMS/EF_Non-target data_Batch 1")

# Import sample data
hplc_batch1 <- list.files(pattern = '*.xls') %>%
  .[!str_detect(., "Blank")]
# # Import Blank data 
blank_hplc_batch1 <- list.files(pattern = '*.xls') %>%
  .[str_detect(., "Blank")]

# Put sample and blank data together and create additional description columns
hplc_batch1_step1.1 <- dplyr::bind_rows(purrr::map(hplc_batch1, read_xls, skip = 1), 
                                        purrr::map(blank_hplc_batch1, read_xls, skip = 1)) %>%
  dplyr::select(m.z = `m/z`, RT, Height, File) %>%
  mutate(File = gsub("_", "-", File),
         type = ifelse(str_detect(File, "Blank"), "Blanks", "Sample"),
         Day = case_when(str_detect(File, "Day0") ~ "0",
                         str_detect(File, "Day2") ~ "2",
                         str_detect(File, "Day15") ~ "15",
                         TRUE ~ "34"),
         Replicate = ifelse(str_detect(File, "R1"), "R1", "R2"),
         Batch_number = "1",
         simplified_file = map_chr(File, ~ paste(str_split_1(.x, "-")[2:3], collapse = "-")))

# Change column name
colnames(hplc_batch1_step1.1)[1] <- "m.z"

###### Import Batch 2
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/HPLCTOFMS/EF_Non-target data_Batch 2")

hplc_batch2 <- list.files(pattern = '*.xls') %>%
  .[!str_detect(., "Blank")]
# Blank samples 
blank_hplc_batch2 <- list.files(pattern = '*.xls') %>%
  .[str_detect(., "Blank")]

hplc_batch2_step1.1 <- dplyr::bind_rows(purrr::map(hplc_batch2, read_xls, skip = 1), 
                                        purrr::map(blank_hplc_batch2, read_xls, skip = 1)) %>%
  dplyr::select(m.z = `m/z`, RT, Height, File) %>%
  mutate(File = gsub("_", "-", File),
         type = ifelse(str_detect(File, "Blank"), "Blanks", "Sample"),
         Day = case_when(str_detect(File, "Day0") ~ "0",
                         str_detect(File, "Day2") ~ "2",
                         str_detect(File, "Day15") ~ "15",
                         TRUE ~ "34"),
         Replicate = ifelse(str_detect(File, "R1"), "R1", "R2"),
         Batch_number = "2",
         simplified_file = map_chr(File, ~ paste(str_split_1(.x, "-")[2:3], collapse = "-")))

colnames(hplc_batch2_step1.1)[1] <- "m.z"

all_hplc <- rbind(hplc_batch0_step1.1,
                  hplc_batch1_step1.1,
                  hplc_batch2_step1.1) %>% arrange(RT)
```


#### Quality assurance: Histogram distribution of Peak values before data normalization
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS - Peak Area
ggplot(data = combined_df1 %>% filter(type == "Sample")) +
  geom_histogram(aes(y= Area)) +
  facet_wrap(~File) +
  theme_minimal(base_size = 20) +
  theme(strip.text = element_blank(),
        axis.text.x = element_text(angle = 90, vjust = 0.5))


# HPLCQTOFMS - Peak Height
ggplot(data = combined_df2 %>% filter(type == "Sample")) +
  geom_histogram(aes(y= Height)) +
  facet_wrap(~File) +
  theme_minimal(base_size = 20) +
  theme(strip.text = element_blank(),
        axis.text.x = element_text(angle = 90, vjust = 0.5))
```


#### Quality assurance: Histogram distribution of retention time before data normalization
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS
ggplot(data = combined_df1 %>% filter(type == "Sample")) +
  geom_histogram(aes(x= RT)) +
  facet_wrap(~File) +
  theme_minimal(base_size = 20) +
  theme(strip.text = element_blank())


# HPLCQTOFMS
ggplot(data = combined_df2 %>% filter(type == "Sample")) +
  geom_histogram(aes(x= RT)) +
  facet_wrap(~File) +
  theme_minimal(base_size = 20) +
  theme(strip.text = element_blank())
```

### Quality assurance for STEP 1.3B: Examine variation in RT of 4 benchmark compounds in HPLC across all samples with histogram distribution of RT of 4 benchmark compounds 

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS data
# Use excel sheets of Toluene-d8 benchmark to create scatterplot 
bm_df_gcms <- atdgcms_step1.1 %>% filter(m.z >= 98 & m.z <= 98.5 & RT < 13.6)

# Plot histogram of RT distribution
hist(bm_df_gcms$RT, 
       xlab = "RT", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
       main = "Toluene-D8")

# HPLC data
# Use excel sheets of benchmark day "0, 2, 15, 34" from Final Leachates Benchmarks and Recovery.xlsx to create scatterplot dataframe
# bm_sheets <- readxl::excel_sheets(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/HPLCTOFMS/Final Leachates Benchmarks and Recovery.xlsx")
# 
# bm_df_hplc_list <- list()
# for (i in 1:length(bm_sheets)) {
#   x <- readxl::read_xlsx("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/Final Leachates Benchmarks and Recovery.xlsx", sheet = bm_sheets[i]) %>% 
#     select(c("Acetaminophen-d4 (m/z 156.0957)", "dTEP (m/z 198.1723)", "dTPrP (m/z 246.2568)",
#              "13C-6PPD (m/z 305.1956)", "dTPP (m/z 342.1722)"))
#   bm_df_hplc_list[[i]] <- x[-1,] %>% pivot_longer(cols = 1:ncol(x), names_to = "Benchmark", values_to = "RT")
# }
# 
# bm_df_hplc <- bind_rows(bm_df_hplc_list)

# Find all peaks in our data that match m/z of benchmark compounds
par(mfrow = c(3, 2))
bm_mz <- c(156.0957, 198.1723, 246.2568, 305.1956, 342.1722)
bm_name <- c("Acetaminophen-d4 (m/z 156.0957)", "dTEP (m/z 198.1723)", "dTPrP (m/z 246.2568)",
             "13C-6PPD (m/z 305.1956)", "dTPP (m/z 342.1722)")
for (i in 1:length(bm_mz)) {
  df <- hplc_batch2_step1.1 %>% # hplc_batch2_step1.1, hplc_batch1_step1.1; hplc_batch0_step1.1
    filter(m.z >= (bm_mz[i] - 0.0005) &
             m.z <= (bm_mz[i] + 0.0005))
  hist(as.numeric(df$RT), 
       xlab = "RT", ylab ="Count",
       main = bm_name[i])
}
```

### Quality assurance for STEP 1.3B compound alignment - HPLC data

(Update 09 Feb 2024:) Meeting with Roxana => Apply different compound alignment to different part of the chromatograms -> split compounds alignment to RT <= 6 Min and RT > 6 min.

There are nine combinations of RT windows that were tested:
                RT <= 6 min           RT > 6 min
1                0.1                   0.02
2                0.1                   0.03
3                0.1                   0.04
4                0.15                  0.02
5                0.15                  0.03
6                0.15                  0.04
7                0.2                   0.02
8                0.2                   0.03
9                0.2                   0.04

(Update 13 & 14 Feb 2024:) I tested out a bunch of RT for Rt > 6 min but it doesn't seem to bring down the

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Testing different combinations of RT window for 2 parts of the chromatogram ----------
combi <- tidyr::crossing(
  # c(0.1,0.15,0.2, 0.25, 0.3, 0.35, 
                           0.4, c(
                             # 0.02,0.03,0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2,
                             0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4))
rt_test_list <- list()
for (row in 1:nrow(combi)) {
  df <- grouping_comp(combined_df2,
                      rtthres1 = as.numeric(combi[row,][1]),
                      rtthres2 = as.numeric(combi[row,][2]),
                      mzthres = 0.0005,
                      type = "HPLCTOFMS")
  
  rt_test_list[[paste0(as.numeric(combi[row,][1]), "_", as.numeric(combi[row,][2]))]] <- df
  
  rt_6 <- df %>% filter(RT <= 6)
  rt_higher6 <- df %>% filter(RT > 6)
  
  print(paste0("RT window ", as.numeric(combi[row,][1]), " No. unique compounds that belongs to Benchmark_1 Acetaminophen-d4 : ", 
               length(unique((rt_6 %>% 
                      filter(m.z >= (156.0957 - 0.0001) &
                               m.z <= (156.0957 + 0.0001)))$Feature))))
  
  print(paste0("RT window ", as.numeric(combi[row,][1]), " No. of unique compounds : ", length(unique(rt_6$Feature))))
  
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_2 dTEP : ", 
               length(unique((rt_higher6 %>% 
                                filter(m.z >= (198.1723 - 0.0001) &
                                         m.z <= (198.1723 + 0.0001)))$Feature))))
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_3 dTPrP : ", 
               length(unique((rt_higher6 %>% 
                      filter(m.z >= (246.2568 - 0.0001) &
                               m.z <= (246.2568 + 0.0001)))$Feature))))
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_4 13C-6PPD : ", 
               length(unique((rt_higher6 %>% 
                      filter(m.z >= (305.1956 - 0.0001) &
                               m.z <= (305.1956 + 0.0001)))$Feature))))
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_5 dTPP : ", 
               length(unique((rt_higher6 %>%
                      filter(m.z >= (342.1722 - 0.0001) &
                               m.z <= (342.1722 + 0.0001)))$Feature))))

  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. of unique compounds : ", length(unique(rt_higher6$Feature))))
}
```


## STEP 1.2: Collapsing compounds based on RT1, RT2, Ion1 threshold --------------------------

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS
atdgcms_grouped <- grouping_comp(atdgcms_step1.1,
                                 rtthres1 = 0.05,
                                 mzthres = 0.05,
                                 type = "ATDGCMS")

# HPLCTOFMS
# Apply different alignment to 2 parts of chromatogram via rtthres1 (RT <= 6 min) and rtthres2 (RT > 6 min)
hplc_batch_grouped <- grouping_comp(all_hplc,
                                    split_time = 6,
                                    rtthres1 = 0.4,
                                    rtthres2 = 0.28,
                                    mzthres = 0.0005,
                                    type = "HPLCTOFMS")
```

## Step 1.3: Remove all the benchmark compounds for HPLC (4) and GC (1 - Toluene)
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
#ATDGCMS
idx <- which(atdgcms_grouped$m.z >= 98 & 
               atdgcms_grouped$m.z <= 98.5 & 
               atdgcms_grouped$RT > 13.4 & 
               atdgcms_grouped$RT < 13.5)
atdgcms_grouped <- atdgcms_grouped[-idx,]

# HPLC
hplc_benchmark_mz <- c(156.0957, 198.1723, 246.2568, 305.1956, 342.1722)
# Search for compound in combined_df2_grouped that fall in the window of benchmark' molecular ions
# Batch 0
hplc_benchmark_idx <- c()
for (mz in hplc_benchmark_mz) {
  idx <- which(hplc_batch_grouped$m.z <= (mz + 0.0005) & hplc_batch_grouped$m.z >= (mz - 0.0005))
  # collect all these indices of benchmark compounds
  hplc_benchmark_idx <- c(hplc_benchmark_idx, idx)
}
hplc_batch_grouped <- hplc_batch_grouped[-hplc_benchmark_idx,]
```


## Step 2: Blank subtraction & Remove all detections with negative peak area

Readjust compound peak area (sample) by substracting it by average blank peak area

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS --------------
# Create list to store temp dfs
process_atdgcms <- function(atdgcms_grouped) {
  result_list <- list()
  idx <- 1
  
  # Iterate through each unique feature
  for (feature in unique(atdgcms_grouped$Feature)) {
    temp <- atdgcms_grouped[atdgcms_grouped$Feature == feature, ]
    
    # If no blanks exist for the current feature, add it as is
    if (identical(which(temp$type == "Blanks"), integer(0))) {
      result_list[[idx]] <- temp
      idx <- idx + 1
      next
    } else {
      # Calculate the mean and sd for blanks
      blank_idx <- which(temp$type == "Blanks")
      blank_mean <- base::mean(temp[blank_idx, "Area"], na.rm = TRUE)
      blank_sd   <- stats::sd(temp[blank_idx, "Area"], na.rm = TRUE)
      
      # Exclude blanks from further processing
      temp <- temp[temp$type != "Blanks", ]
      
      # Adjust the 'Area' for each sample within the feature
      for (sample in unique(temp$File)) {
        sample_idx <- which(temp$File == sample)
        temp[sample_idx, "Area"] <- temp[sample_idx, "Area"] - blank_mean
        
        # Keep only those entries with adjusted Area > 0 and > 3 * blank_sd
        positive_idx <- which(temp$Area > 0 & temp$Area > 3 * blank_sd)
        temp <- temp[positive_idx, ]
      }
    }
    
    # Append processed data frame for this feature
    result_list[[idx]] <- temp
    idx <- idx + 1
  }
   
  output <- dplyr::bind_rows(result_list)
  return(output)
}

# For positive values => combine data again to 1 grand data frame
atdgcms_adjusted_step3 <- process_atdgcms(atdgcms_grouped)

# HPLCTOFMS  --------------
library(dplyr)

adjust_data_step3 <- function(hplc_batch_grouped) {
  
  hplc_batch_grouped %>%
    group_by(Batch_number, Feature) %>%
    # Precompute mean and sd of 'Blanks' once per group
    mutate(
      has_blank  = any(type == "Blanks"),
      blank_mean = if (has_blank) mean(Height[type == "Blanks"], na.rm = TRUE) else NA_real_,
      blank_sd   = if (has_blank) sd(Height[type == "Blanks"], na.rm = TRUE)   else NA_real_
    ) %>%
    ungroup() %>%
    # Subtract blank mean from all rows if the group has blanks
    mutate(
      Height = if_else(has_blank, Height - blank_mean, Height)
    ) %>%
    # Remove the blank rows themselves
    filter(!(type == "Blanks" & has_blank)) %>%
    # If the group has blanks, keep only those rows with Height > 3*blank_sd
    # Otherwise keep everything
    filter(!has_blank | Height > (3 * blank_sd)) %>%
    # Drop the helper columns
    select(-blank_mean, -blank_sd, -has_blank)
}

hplc_adjusted_step3 <- adjust_data_step3(hplc_batch_grouped)
```


### Quality assurance: How many compounds after blank subtraction between different RT windows?
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATD-GC-MS
length(unique(comp_normalized1)$Feature)

# HPLC-TOFMS
length(unique(comp_normalized2)$Feature)
```

### Quality assurance: Plot Percentage coverage and number of peaks after removal of limit observation (may not be necessary) ----------------------------
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Calculate the mean/median percentage coverage
coverage_list <- c()
for (threshold in c(seq(from = 0, to = 1000000, by = 50000))) {
  df_filter_area <- combined_df1 %>%
    dplyr::filter(Area > threshold) %>%
    dplyr::group_by(Sample_name) %>%
    dplyr::summarise(across(Area, base::sum))
  
  coverage <- c()
  for (sample in df_filter_area$Sample_name) {
    coverage <- c(coverage, df_filter_area[which(df_filter_area$Sample_name == sample),]$Area*100/sum(combined_df1[which(combined_df1$Sample_name == sample),]$Area))
  }
  coverage_list <- c(coverage_list, mean(coverage))
}

num_comp_list <- c()

# Precompute sample counts for all thresholds using vectorized operations
for (threshold in seq(from = 0, to = 1000000, by = 50000)) {
  
  # Filter the data based on threshold
  df_filter_area <- combined_df1 %>%
    dplyr::filter(Area > threshold) %>%
    dplyr::group_by(Sample_name) %>%
    dplyr::summarize(num_comp = n())   # Count occurrences per Sample_name
  
  # Compute the mean of num_comp and store it in num_comp_list
  num_comp_list <- c(num_comp_list, mean(df_filter_area$num_comp))
}

df <- data.frame(list(thres = c(seq(from = 0, to = 1000000, by = 50000)), remain = num_comp_list))
plot <- ggplot(data = df,
               aes(x = thres, y = remain)) +
  geom_col(fill = "skyblue") +
  geom_text(aes(label = round(remain, digits = 0)), color = "black", angle = 90, hjust = 1.5, size = 7) +
  geom_text(aes(label = paste0(round(coverage_list, digits =1),"%"), 
                y = remain + 5),  # Adjust 'y = remain + 5' for positioning above bars
            color = "black", size = 7, vjust = 0) +     # Adjust vjust for fine-tuning
  scale_x_continuous(breaks = seq(from = 0, to = 1000000, by = 50000),
                     # remove space between plotted data and xy-axes
                     expand = c(0,0)) +
  # scale_y_continuous(breaks = seq(from = 0, to = 100, by = 25), 
  #                    # remove space between plotted data and xy-axes
  #                    expand = c(0,0)) +
  theme_classic(base_size = 20) +
  theme(axis.text.x = element_text(size = 20, angle = 90, vjust = 0.5),
        axis.text.y = element_text(size = 20),
        strip.text = element_text(size = 30, face = "bold"),   # Make facet label text bold
        strip.background = element_blank()) +
  labs(x = "Threshold of removal for limit observations", 
       y = "Number of peak remains after removal of limit observation") 

ggsave(filename = paste0("Number of compound remain & Percentage coverage", format(Sys.time(), "%d-%m-%Y"),".png"), 
       plot = plot, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 10,       # Height in inches (adjust as needed)
       units = "in")
```




## STEP 3: Further data cleaning and removing feature only appear in 1 sample

(Update 20 Feb 2024:)  Add column to distinguish which sample is from GC or HPLC or ICP-MS datasetss.

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS
idx_list_filter1 <- comp_filter_opt(atdgcms_adjusted_step3)
gc <- atdgcms_adjusted_step3[idx_list_filter1[[1]],] %>%
  dplyr::filter(!is.na(Feature)) %>%
  dplyr::select(File, Feature, m.z, RT, Area) %>%
  mutate(Source = ifelse(str_detect(File, "USE"), "Environmental", "Store-Bought")) %>%
  dplyr::rename(Values = Area)

gc$technique <- "GC"

# HPLCTOFMS
idx_list_filter2 <- comp_filter_opt(hplc_adjusted_step3)
hplc <- hplc_adjusted_step3[idx_list_filter2[[1]],] %>%
  # dplyr::filter(Day %notin% c("0", "2")) %>%
  dplyr::filter(!is.na(Feature)) %>%
  dplyr::select(File, Feature, 
                # Day, 
                m.z, RT, Height) %>%
  dplyr::mutate(Source = ifelse(str_detect(File, "USE"), "Environmental", "Store-Bought")) %>%
  dplyr::rename(Values = Height)

hplc$technique <- "HPLC"
```

## STEP 4.1: Import Trace metal data

#### Data prepping
- Problem with icpms_round1 versus. icpms_round2: round1 has Li [No gas] that round2 does not.
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Trace metal data
icpms_round1 <- readxl::read_excel("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/icpms_round1_rawdata_removal USE-01-rep1 and USE-03 (only2observations).xlsx") %>% 
  column_to_rownames(., var = "File")

icpms_round2_batch1 <- readxl::read_excel("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/icpms_round2_batch1_rawdata.xlsx") %>% 
  column_to_rownames(., var = "File")

icpms_round2_batch2 <- readxl::read_excel("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/icpms_round2_batch2_rawdata.xlsx") %>% 
  column_to_rownames(., var = "File")

# icpms_round2_batch1 <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/ElementData_USA_Plastics_USSB_StoreBought_Round2_Batch1_rawdata.xlsm", sheet = "samples", skip = 6)
# icpms_round2_batch1 <- icpms_round2_batch1[-1,]
# colnames(icpms_round2_batch1 )[4] <- "File"
# icpms_round2_batch1 <- icpms_round2_batch1[, !grepl("\\.\\.\\.", names(icpms_round2_batch1))]
# icpms_round2_batch1 <- icpms_round2_batch1[-c(11:nrow(icpms_round2_batch1)),] %>% 
#   column_to_rownames(., var = "File")

# icpms_round2_batch2  <- read_xlsx(path = "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/USE_Plastics_8800_16Aug2024_Round2_ Batch2_rawdata.xlsm", sheet = "samples", skip = 6)
# icpms_round2_batch2 <- icpms_round2_batch2[-1,]
# colnames(icpms_round2_batch2 )[4] <- "File"
# icpms_round2_batch2 <- icpms_round2_batch2[, !grepl("\\.\\.\\.", names(icpms_round2_batch2))]
# icpms_round2_batch2 <- icpms_round2_batch2[-c(49:nrow(icpms_round2_batch2)),] %>% 
#   column_to_rownames(., var = "File")

# 1) Select [H2] instead of [H2 HMI]----------------------
library(stringr)

remove_H2_HMI_modus <- function(icpms_df) {
  strings <- colnames(icpms_df)
  pattern <- "^(.*\\[ )([A-Za-z0-9 ]+)( \\])$"
  metal_namedata <- data.frame(
    original = strings,
    prefix = str_extract(strings, "^.*\\["),
    content = str_match(strings, pattern)[,3],
    stringsAsFactors = FALSE
  )
  
  metal_namedata <- metal_namedata %>% dplyr::filter(!str_detect(content, pattern = "H2 HMI"))
  df <- icpms_df %>% dplyr::select(metal_namedata$original)
  return(df)
}

icpms_round1 <- remove_H2_HMI_modus(icpms_round1)
icpms_round2_batch1 <- remove_H2_HMI_modus(icpms_round2_batch1)
icpms_round2_batch2 <- remove_H2_HMI_modus(icpms_round2_batch2)

# 2) Between the ones that use [N2O] as analyzing modus -> figure out within the same metal element, which one has the highest "Recovery [%]" or if Recovery [%] is missing then lowest "U(k=2)" ---------------------------------------

recovery_uncertainty_round1 <- readxl::read_excel("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/Trace metal data_Recovery_Uncertainty_round1.xlsx") %>%
  filter(`Element+Modus` %in% colnames(icpms_round1))

recovery_uncertainty_round2_batch1 <- readxl::read_excel("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/Trace metal data_Recovery_Uncertainty_round2_batch1.xlsx") %>%
  filter(`Element+Modus` %in% colnames(icpms_round2_batch1))

recovery_uncertainty_round2_batch2 <- readxl::read_excel("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/ICPMS_Trace metal/Trace metal data_Recovery_Uncertainty_round2_batch2.xlsx") %>%
  filter(`Element+Modus` %in% colnames(icpms_round2_batch2))

get_final_metal_names <- function(recovery_uncertainty) {
  
  # 1) Extract the first number
  first_numbers <- sub(" .*", "", recovery_uncertainty$`Element+Modus`)
  
  # 2) Define a function to find duplicate indices
  duplicate_indices <- function(vector) {
    unique_elements <- unique(vector)
    result <- list()
    
    for (element in unique_elements) {
      indices <- which(vector == element)
      if (length(indices) > 1) {
        result[[element]] <- indices
      }
    }
    
    return(result)
  }
  
  # Apply duplicate_indices to first_numbers
  res <- duplicate_indices(first_numbers)
  
  # 3) For duplicates, pick either the row with the highest "Recovery [%]"
  #    or if that is empty, the row with the lowest "U(k=2)"
  recovery_uncertainty_checked <- c()
  for (ele in seq_along(res)) {
    temp <- recovery_uncertainty[res[[ele]], ]
    idx1 <- temp[which.max(temp$`Recovery [%]`), ]
    
    # if no maximum Recovery row is found, pick the minimum U(k=2) row instead
    if (identical(idx1, character(0))) {
      chosen <- temp[which.min(temp$`U(k=2)`), ]$`Element+Modus`
    } else {
      chosen <- temp[which.max(temp$`Recovery [%]`), ]$`Element+Modus`
    }
    recovery_uncertainty_checked <- c(recovery_uncertainty_checked, chosen)
  }
  
  # 4) Combine the unique rows (non-duplicates) plus the chosen duplicates
  final_metal_names <- c(
    recovery_uncertainty[-unlist(res, use.names = FALSE), ]$`Element+Modus`,
    recovery_uncertainty_checked
  )
  
  return(final_metal_names)
}

final_metal_names_round1 <- get_final_metal_names(recovery_uncertainty_round1)
final_metal_names_round2_batch1 <- get_final_metal_names(recovery_uncertainty_round2_batch1)
final_metal_names_round2_batch2 <- get_final_metal_names(recovery_uncertainty_round2_batch2)

# subset dataframe for further analyses
final_icpms_round1 <- icpms_round1 %>% 
  dplyr::select(all_of(final_metal_names_round1))

final_icpms_round2_batch1 <- icpms_round2_batch1 %>% 
  dplyr::select(all_of(final_metal_names_round2_batch1))

final_icpms_round2_batch2 <- icpms_round2_batch2 %>% 
  dplyr::select(all_of(final_metal_names_round2_batch2))

common_cols <- Reduce(intersect, list(names(final_icpms_round1), 
                                      names(final_icpms_round2_batch1), 
                                      names(final_icpms_round2_batch2)))
combined_icpms_round1_round2 <- rbind(final_icpms_round1[common_cols], 
                                      final_icpms_round2_batch1[common_cols],
                                      final_icpms_round2_batch2[common_cols])
View(combined_icpms_round1_round2)

# 3) Final cleaning of name of metal elements (OPTIONAL) ----------------------------------
# Function to process the strings
process_string <- function(name_vector) {
  # Remove everything after the square bracket
  cleaned_vector <- str_extract(name_vector, "^[^\\[]+")
  # Trim any trailing whitespace
  cleaned_vector <- str_trim(cleaned_vector)
  # Use a regular expression to remove the numerical parts
  # Explanation of Regular Expression:
  # 
  # \\d+ matches one or more digits.
  # *(-> *\\d+)? * optionally matches the pattern -> digits with any number of spaces around it.
  # gsub function is used to find and replace the matching parts in the string with an empty string ("")
  cleaned_vector <- gsub("\\d+ *(-> *\\d+)? *", "", cleaned_vector)
  return(cleaned_vector)
}

# Apply the function to each element of the vector
# colnames(combined_icpms_round1_round2) <- sapply(colnames(combined_icpms_round1_round2), process_string)

# 4) Discard all metal that may appear in seawater - Sodium (Na), Magnesium (Mg), Potassium (K), Calcium (Ca), Sr (optional) --------------------------
strings_to_remove <- c("Na", "Mg", "K", "Ca") #, "Sr")
cols_to_remove <- sapply(strings_to_remove, function(x) grep(x, colnames(combined_icpms_round1_round2)))
cols_to_remove <- unique(unlist(cols_to_remove))
combined_icpms_round1_round2 <- combined_icpms_round1_round2[, -cols_to_remove]

icp <- combined_icpms_round1_round2 %>%
  rownames_to_column(., var = "File")  %>% 
  pivot_longer(., cols = 2:ncol(.), names_to = "Feature", values_to = "Values")

icp$technique <- "ICP"
icp <- icp %>%
  mutate(Source = ifelse(str_detect(File, "USE"), "Environmental", "Store-Bought")) %>%
  mutate(Values = as.numeric(Values))
```

## Combine Label Info with raw data
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
p <- "C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic-Fingerprinting-scripts and data/data/Sample Labelling_all data_GC+HPLC+ICP.xlsx"

sampinfo <- readxl::read_excel(path = p)

# joining data frame with sampinfo df
icp <- left_join(x = icp ,
                 y = sampinfo %>% dplyr::select(c(File, Plastic_type, Subcategory, Polymer)), 
                 by = 'File') # %>% dplyr::filter(!is.na(Similar_type))

gc <- left_join(x = gc ,
                y = sampinfo %>% dplyr::select(c(File, Plastic_type, Subcategory, Polymer)), 
                by = 'File') # %>% dplyr::filter(!is.na(Similar_type))

hplc <- left_join(x = hplc ,
                  y = sampinfo %>% dplyr::select(c(File, Plastic_type, Subcategory, Polymer)), 
                  by = 'File') # %>% dplyr::filter(!is.na(Similar_type))

## Aggregate Day 15 and 34 by mean
# new_hplc <- hplc %>%
#   group_by(Feature, Subcategory, Polymer, technique) %>%
#   summarise(Mean_Values = mean(Values, na.rm = TRUE), .groups = "drop")

gc_hplc <- bind_rows(gc, hplc)

gc_icp <- bind_rows(icp, 
                    gc %>% dplyr::select(-c("m.z", "RT"))) # %>% dplyr::filter(!is.na(Similar_type))

hplc_icp <- bind_rows(icp, 
                      hplc %>% dplyr::select(-c("m.z", "RT"))) # %>% dplyr::filter(!is.na(Similar_type))

gc_hplc_icp <- bind_rows(icp, 
                         gc_hplc %>% dplyr::select(-c("m.z", "RT"))) # %>% dplyr::filter(!is.na(Similar_type))

```


## Data cleaning

### Feature filtering: Regular 80% rule - Keep feature columns where there are more than 80% non-MVs

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
df <- gc_hplc_icp %>% 
    dplyr::select(File, Subcategory, Source, Polymer, Feature, technique, Values) %>%
    # If multiple values of the same compound in a single sample, take their mean
    dplyr::group_by(File, Source, Polymer, Subcategory, Feature, technique) %>%
    dplyr::summarise(across(Values, base::mean), .groups = "drop") %>%
    tidyr::pivot_wider(names_from = Feature, values_from = Values) %>%
    tibble::column_to_rownames(var = "File") %>%
    relocate(Subcategory, technique, Source, Polymer, .before = 1)

column_to_keep <- c()
for (feature in 5:ncol(df)) {
  if (sum(!is.na(df[, feature]))/dim(df)[1] >= 0.8) {
    column_to_keep <- c(column_to_keep, feature)
  }
}

regular_80_rule <- df[, c(1,2,3,4, column_to_keep)]
# View(regular_80_rule)
print(paste0("Number of chemical features before Regular 80% rule filtering: ", ncol(regular_80_rule) - 4))
print(paste0("Missing values before Regular 80% rule filtering (%) ", sum(is.na(regular_80_rule))/(dim(regular_80_rule)[1]*(dim(regular_80_rule)[2]-3))))
```

### Feature filtering: Modified 80% rule
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Assuming your dataframe is called `df`, and the class column is named `Class`
# Define a function to filter features based on non-missing values within each class
filter_features_by_class <- function(data, class_col, threshold = 0.8) {
  # Split the data by classes
  class_split <- split(data, data[[class_col]])
  
  # Initialize a list to store columns that meet the threshold in each class
  columns_meeting_threshold <- list()
  
  for (class_name in names(class_split)) {
    # Calculate non-missing proportion for each feature within the class
    class_data <- class_split[[class_name]]
    feature_non_missing <- colMeans(!is.na(class_data))
    
    # Select columns that meet the threshold
    columns_meeting_threshold[[class_name]] <- names(feature_non_missing[feature_non_missing >= threshold])
  }
  
  # Find common columns that meet the threshold across all classes
  common_columns <- Reduce(intersect, columns_meeting_threshold)
  
  # Combine the common columns with the class column into one vector for selection
  selected_columns <- c(common_columns, class_col)
  
  # Subset the original data to keep only the selected columns
  filtered_data <- data %>%
    dplyr::select(all_of(selected_columns))
  
  return(filtered_data)
}

# Apply the function to your dataframe
modified_80_rule <- filter_features_by_class(df, "Subcategory", threshold = 0.8)

View(modified_80_rule)
print(paste0("Number of chemical features before Modified 80% rule filtering: ", ncol(modified_80_rule) - 4))
print(paste0("Missing values before filtering (%) ", sum(is.na(modified_80_rule))/(dim(modified_80_rule)[1]*(dim(modified_80_rule)[2]-3))))
```

### Pearson Correlation Coefficient of number of missing values
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
coredata <- gc_hplc_icp
# Step 1: Summarize and reshape the data
df <- coredata %>%
  dplyr::select(File, Subcategory, Feature, technique, Values) %>% #, m.z, RT) %>%
  dplyr::group_by(File, Subcategory, Feature, technique) %>%
  dplyr::summarise(Values = mean(Values), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = Feature, values_from = Values) %>%
  tibble::column_to_rownames(var = "File") %>%
  dplyr::select(-Subcategory, -technique)  # Drop unnecessary columns directly

# Step 2: Calculate NA counts and mean response
na_counts <- colSums(is.na(df))
mean_response <- colMeans(df, na.rm = TRUE)

# Step 3: Build gc_na_df with NA counts and mean response
na_df <- data.frame(
  Feature = names(na_counts),
  num_missing_values = na_counts,
  mean_response = mean_response
)

# Step 4: Merge m/z and RT values based on 'Feature' from original 'gc' dataset
new_na_df <- na_df %>%
  dplyr::left_join(
    coredata %>% dplyr::group_by(Feature) %>%
      dplyr::summarise(m_z = mean(m.z, na.rm = TRUE),
                       retention_time = mean(RT, na.rm = TRUE),
                       .groups = "drop"),
    by = "Feature"
  )

# Calculate Pearson correlation for (iii) mean response vs. number of missing values
cor_response_missing <- cor(new_na_df$mean_response, new_na_df$num_missing_values, method = "pearson")

# Calculate Pearson correlation for (i) m/z vs. number of missing values
cor_mz_missing <- cor(new_na_df$m_z, new_na_df$num_missing_values, method = "pearson")

# Calculate Pearson correlation for (ii) retention time vs. number of missing values
cor_rt_missing <- cor(new_na_df$retention_time, new_na_df$num_missing_values, method = "pearson")

# Display the results
cat("Pearson correlation data (mean response vs. number of missing values):", cor_response_missing, "\n")
cat("Pearson correlation data (m/z vs. number of missing values):", cor_mz_missing, "\n")
cat("Pearson correlation data (retention time vs. number of missing values):", cor_rt_missing, "\n")

# pearson_correlation_report <- data.frame(
#   cor_mz_missing = cor_mz_missing,
#   cor_rt_missing = cor_rt_missing,
#   cor_response_missing = cor_response_missing
# )
```

### Check Correlation between predictors aka. multicollinearity to choose correct ML algorithms
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Define a function to compute the overall correlation magnitude for a dataset
overall_corr_magnitude <- function(data) {
  # Subset the data to include only numeric columns
  data_numeric <- data[sapply(data, is.numeric)]
  
  # Check if there are at least two numeric columns to correlate
  if(ncol(data_numeric) < 2) {
    warning("Data must contain at least two numeric columns.")
    return(NA)
  }
  
  # Compute the correlation matrix (using Pearson by default)
  corr_matrix <- stats::cor(data_numeric, use = "pairwise.complete.obs", method = "pearson")
  
  # Set the diagonal to NA to exclude self-correlations
  diag(corr_matrix) <- NA
  
  # Calculate the overall magnitude as the mean of the absolute correlations (ignoring NA)
  overall_magnitude <- mean(abs(corr_matrix), na.rm = TRUE)
  
  return(overall_magnitude)
}

overall_corr_magnitude(gc_clean) # hplc_clean, etc.

# Determine classifier type based on correlation
# High correlations (absolute value > 0.8) may suggest multicollinearity, favoring non-linear models.
high_corr <- any(abs(cor_matrix[upper.tri(cor_matrix)]) > 0.8)
if (high_corr) {
  cat("Consider using a non-linear machine learning classifier.\n")
} else {
  cat("Linear models may suffice based on correlation analysis.\n")
}
```

### Comprehensive_filtering 
```{r , echo=FALSE, warning = FALSE, message=FALSE}
comprehensive_filtering <- function(input_df) {
  #-----------------------------#
  # STEP 0: Choose whether to filter data by Source
  #         "Environmental" vs. "Store-Bought"
  #-----------------------------#
  filter_source_decision <- utils::askYesNo(
    "Step 0: Filter data by 'Source == Environmental'? (Yes = Environmental, No = Store-Bought, Cancel = keep all)"
  )
  
  if (is.na(filter_source_decision)) {
    # User cancels -> keep all data (no filtering by Source)
    message("User canceled at Step 0. Keeping all data.")
    df_in <- input_df
  } else if (filter_source_decision) {
    # TRUE -> user wants 'Environmental'
    message("Filtering data to Source == 'Environmental'...")
    df_in <- dplyr::filter(input_df, Source == "Environmental")
  } else {
    # FALSE -> user wants 'Store-Bought'
    message("Filtering data to Source == 'Store-Bought'...")
    df_in <- dplyr::filter(input_df, Source == "Store-Bought")
  }
  
  #-----------------------------#
  # STEP 0 (continued): Pivot data 
  #-----------------------------#
  df <- df_in %>% 
    dplyr::select(File, Plastic_type, Subcategory, Source, Polymer, Feature, technique, Values) %>%
    # If multiple values of the same compound in a single sample, take their mean
    dplyr::group_by(File, Plastic_type, Source, Polymer, Subcategory, Feature, technique) %>%
    dplyr::summarise(across(Values, base::mean), .groups = "drop") %>%
    tidyr::pivot_wider(names_from = Feature, values_from = Values) %>%
    tibble::column_to_rownames(var = "File") %>%
    relocate(Plastic_type, Subcategory, technique, Source, Polymer, .before = 1)
  
  #-----------------------------#
  # STEP 1: Identify columns whose features appear in >90% of samples for at least one label
  #-----------------------------#
  col_idx_90_in_a_product <- c()
  
  for (i in 6:ncol(df)) {
    # Subset the label columns and each feature column
    temp <- df[, c(1, 2, i)]
    
    if (str_detect(colnames(temp)[3], "GC")) {
      ele_count <- c()
      for (subcat in unique(temp[which(temp$technique == "GC"), ]$Subcategory)) {
        count <- sum(!is.na(temp[which(temp$Subcategory == subcat & temp[,3] != 0), 3])) /
          length(temp[which(temp$Subcategory == subcat & temp$technique == "GC"), 3])
        ele_count <- c(ele_count, count)
      }
      # If the feature has > 90% non-missing value in any of the Subcategory
      if (any(ele_count > 0.9)) { 
        col_idx_90_in_a_product <- c(col_idx_90_in_a_product, i)
      }
    } else if (str_detect(colnames(temp)[3], "HPLC")) {
      ele_count <- c()
      for (subcat in unique(temp[which(temp$technique == "HPLC"), ]$Subcategory)) {
        count <- sum(!is.na(temp[which(temp$Subcategory == subcat & temp[,3] != 0), 3])) /
          length(temp[which(temp$Subcategory == subcat & temp$technique == "HPLC"), 3])
        ele_count <- c(ele_count, count)
      }
      if (any(ele_count > 0.9)) {
        col_idx_90_in_a_product <- c(col_idx_90_in_a_product, i)
      }
    } else {
      # Assume "ICP" or other technique:
      ele_count <- c()
      for (subcat in unique(temp[which(temp$technique == "ICP"), ]$Subcategory)) {
        count <- sum(!is.na(temp[which(temp$Subcategory == subcat & temp[,3] != 0), 3])) /
          length(temp[which(temp$Subcategory == subcat & temp$technique == "ICP"), 3])
        ele_count <- c(ele_count, count)
      }
      if (any(ele_count > 0.9)) {
        col_idx_90_in_a_product <- c(col_idx_90_in_a_product, i)
      }
    }
  }
  
  #-----------------------------#
  # STEP 2: Identify & remove features that are >90% missing overall 
  #         (per technique) 
  #-----------------------------#
  colid_90_missingvalues <- c()
  
  for (i in 6:ncol(df)) {
    # Based on technique column:
    if (str_detect(colnames(df)[i], "GC")) {
      df[which(df$technique != "GC" & is.na(df[, i])), i] <- 0
      if (length(df[which(df$technique == "GC" & !is.na(df[, i])), i]) /
          length(df[which(df$technique == "GC"), i]) < 0.1) {
        colid_90_missingvalues <- c(colid_90_missingvalues, i)
      }
    } else if (str_detect(colnames(df)[i], "HPLC")) {
      df[which(df$technique != "HPLC" & is.na(df[, i])), i] <- 0
      if (length(df[which(df$technique == "HPLC" & !is.na(df[, i])), i]) /
          length(df[which(df$technique == "HPLC"), i]) < 0.1) {
        colid_90_missingvalues <- c(colid_90_missingvalues, i)
      }
    } else {
      # Assume "ICP" or other:
      df[which(df$technique != "ICP" & is.na(df[, i])), i] <- 0
      if (length(df[which(df$technique == "ICP" & !is.na(df[, i])), i]) /
          length(df[which(df$technique == "ICP"), i]) < 0.1) {
        colid_90_missingvalues <- c(colid_90_missingvalues, i)
      }
    }
  }
  
  #-----------------------------#
  # STEP 3: Merge back columns 
  #         (i) features found in >=90% of at least one product
  #         (ii) features that are NOT >90% missing 
  #-----------------------------#
  test  <- df[, col_idx_90_in_a_product]
  if (!is.null(colid_90_missingvalues)) {
    test1 <- df[, -colid_90_missingvalues]
  } else {
    test1 <- df
  }
  
  # Merge the two filtered sets
  merged_df <- df %>%
    dplyr::select(., base::union(colnames(test), colnames(test1))) %>%
    relocate(Subcategory, technique, Source, Polymer, .before = 1)
  
  df <- merged_df
  
  #-----------------------------#
  # STEP 4 (Conditional): Remove features that only have values 
  #                       in a single row (occurs in 1 sample)
  #-----------------------------#
  perform_step4 <- utils::askYesNo("Step 4: Remove features with values in only a single sample?")
  # If user cancels (NA), return data as-is
  if (is.na(perform_step4)) {
    message("User canceled at Step 4. Returning current dataset.")
    return(df)
  }
  
  if (perform_step4) {
    df_step4 <- df[, colSums(!is.na(df) & df != 0) > 1]
  } else {
    # Skipped step 4 - do nothing
    df_step4 <- df
  }
  
  #-----------------------------#
  # STEP 5 (Conditional): Remove features that only appear in 
  #                       a single label (Subcategory)
  #-----------------------------#
  perform_step5 <- utils::askYesNo("Step 5: Remove features that appear in only one label (Subcategory)?")
  if (is.na(perform_step5)) {
    message("User canceled at Step 5. Returning dataset (post-step 4).")
    return(df_step4)
  }

  if (perform_step5) {
    # Identify feature columns
    feature_columns <- names(df_step4[, -c(1,2,3,4,5)])

    # For each feature, check how many unique labels it appears in
    keep_features <- sapply(df_step4[, feature_columns], function(feature) {
      unique_labels_with_values <- unique(df_step4$Subcategory[!is.na(feature) & (feature != 0)])
      length(unique_labels_with_values) > 1
    })

    # Keep only those features that appear in more than one label
    df_step5 <- df_step4[, c(names(df_step4)[1:5], names(keep_features)[keep_features])]
  } else {
    df_step5 <- df_step4
  }
  
  #-----------------------------#
  # STEP 6 (Conditional): Remove features that do NOT have >=3 data 
  #         points in each of at least two different labels 
  #-----------------------------#
  perform_step6 <- utils::askYesNo("Step 6: Remove features lacking >= 3 data points in at least two labels?")
  if (is.na(perform_step6)) {
    message("User canceled at Step 6. Returning dataset (post-step 5).")
    return(df_step5)
  }
  
  if (perform_step6) {
    selected_col <- integer()
    categorical_combinations <- t(utils::combn(unique(df_step5$Subcategory), 2))
    
    # Loop over feature columns (from 5 onward)
    for (feature_col in 6:ncol(df_step5)) {
      feature_data <- df_step5[[feature_col]]  # Extract once for efficiency
      # Loop through label pairs
      for (combo in seq_len(nrow(categorical_combinations))) {
        p_1 <- categorical_combinations[combo, 1]
        p_2 <- categorical_combinations[combo, 2]
        
        vec1 <- as.numeric(feature_data[df_step5$Subcategory == p_1])
        vec2 <- as.numeric(feature_data[df_step5$Subcategory == p_2])
        
        if (sum(!is.na(vec1)) >= 3 & sum(!is.na(vec2)) >= 3) {
          selected_col <- union(selected_col, feature_col)
          break  # No need to check other label combos once it qualifies
        }
      }
    }
    
    df_step6 <- cbind(df_step5[, c(1:5)], df_step5[, selected_col])
  } else {
    df_step6 <- df_step5
  }
  
  #-----------------------------#
  # Return the final dataset
  #-----------------------------#
  return(df_step6)
}
```

### Cleaning all datasets
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# gc_clean <- comprehensive_filtering(gc)
# hplc_clean <- comprehensive_filtering(hplc)
# icp_clean <- comprehensive_filtering(icp)
# gc_hplc_clean <- comprehensive_filtering(gc_hplc)
# gc_icp_clean <- comprehensive_filtering(gc_icp)
# hplc_icp_clean <- comprehensive_filtering(hplc_icp)
# gc_hplc_icp_clean <- comprehensive_filtering(gc_hplc_icp)

# Find shared features between SB and ENV
SB <- gc %>% dplyr::filter(Source == "Store-Bought")
ENV <- gc %>% dplyr::filter(Source == "Environmental")

shared_cols_SB_ENV <- Reduce(intersect, list(unique(SB$Feature), 
                                             unique(ENV$Feature)))

combined_SB_ENV_shared_cols_ntr <- rbind(SB %>% 
                                           dplyr::filter(Feature %in% shared_cols_SB_ENV),
                                         ENV %>% 
                                           dplyr::filter(Feature %in% shared_cols_SB_ENV))

# Then do data cleaning - change variable name accordingly to data combinations
hplc_combined_SB_ENV_shared_cols_ntr_clean <- comprehensive_filtering(combined_SB_ENV_shared_cols_ntr)

# Pairwise test as feature filtering
gc_pairwise_significance_tests <- pairwise_significance_tests(gc_combined_SB_ENV_shared_cols_ntr_clean, start_col_index = 6)

hplc_pairwise_significance_tests <- pairwise_significance_tests(hplc_combined_SB_ENV_shared_cols_ntr_clean, start_col_index = 6)

icp_pairwise_significance_tests <- pairwise_significance_tests(icp_combined_SB_ENV_shared_cols_ntr_clean, start_col_index = 6)

gc_hplc_pairwise_significance_tests <- pairwise_significance_tests(gc_hplc_combined_SB_ENV_shared_cols_ntr_clean, start_col_index = 6)

gc_icp_pairwise_significance_tests <- pairwise_significance_tests(gc_icp_combined_SB_ENV_shared_cols_ntr_clean, start_col_index = 6)

hplc_icp_pairwise_significance_tests <- pairwise_significance_tests(hplc_icp_combined_SB_ENV_shared_cols_ntr_clean, start_col_index = 6)

gc_hplc_icp_pairwise_significance_tests <- pairwise_significance_tests(gc_hplc_icp_combined_SB_ENV_shared_cols_ntr_clean, start_col_index = 6)

### Filter data combos based on pairwise test significant features
### Here only ICP and GC-ICP data has reduced number of features after pairwise test
icp_combined_SB_ENV_shared_cols_ntr_clean_pairwise <- cbind(icp_combined_SB_ENV_shared_cols_ntr_clean[,1:5], icp_combined_SB_ENV_shared_cols_ntr_clean[, 6:ncol(icp_combined_SB_ENV_shared_cols_ntr_clean)] %>% dplyr::select(unique(icp_pairwise_significance_tests$corrected[[1]]$Feature)))

gc_icp_combined_SB_ENV_shared_cols_ntr_clean_pairwise <- cbind(gc_icp_combined_SB_ENV_shared_cols_ntr_clean[,1:5], gc_icp_combined_SB_ENV_shared_cols_ntr_clean[, 6:ncol(gc_icp_combined_SB_ENV_shared_cols_ntr_clean)] %>% dplyr::select(unique(gc_icp_pairwise_significance_tests$corrected[[1]]$Feature)))

```

# Assessing for best Imputation and Normalization methods
### Define functions
```{r , echo=FALSE, warning = FALSE, message=FALSE}
#########################
##   Load Libraries    ##
#########################
library(missForest)      # for random forest imputation
# library(pcaMethods)      # for Bayesian PCA (bpca)
library(VIM)             # for kNN imputation (kNN())
library(softImpute)      # for SVD-based imputation (softImpute)
library(dplyr)
library(tidyr)
library(purrr)
library(randomForest)
library(caret)           # for model training & cross-validation
library(stats)           # for basic stats tests, prcomp, etc.
library(ggplot2)         # if you want to visualize (optional)
library(plyr)            # for round_any, etc. if needed

#########################
##   1. Imputation     ##
#########################

## 1.1 Random Forest Imputation (similar to Python's IterativeImputer with ExtraTrees)
random_forest_imputation <- function(df){
  # missForest returns a list: $ximp is the completed data
  imputed <- missForest(as.data.frame(df), verbose = FALSE)$ximp
  return(imputed)
}

## 1.2 Bayesian PCA Imputation
bayesian_pca_imputation <- function(df){
  # The pcaMethods package offers "bpca" for Bayesian PCA
  # We handle numeric columns only
  df_numeric <- as.data.frame(df)
  
  # If all columns are numeric, you can proceed directly:
  bpca_res <- pca(df_numeric, method = "bpca", nPcs = min(ncol(df_numeric) - 1, nrow(df_numeric) - 1))
  # Complete the data from the model
  df_imputed <- completeObs(bpca_res)
  return(df_imputed)
}

## 1.3 Mean Imputation
mean_imputation <- function(df){
  df_numeric <- df
  for(i in seq_along(df_numeric)){
    col_data <- df_numeric[[i]]
    # Replace NA with mean
    col_data[is.na(col_data)] <- mean(col_data, na.rm = TRUE)
    df_numeric[[i]] <- col_data
  }
  return(df_numeric)
}

## 1.4 kNN Imputation
knn_imputation <- function(df, k = 5){
  # VIM::kNN modifies the data in place; to avoid that, use a copy
  df_copy <- df
  # kNN returns a data.frame with the imputed columns appended with ".imp"
  # The original columns remain but with no missing values replaced in place
  # So we can do something like:
  imputed_data <- kNN(df_copy, k = k, imp_var = FALSE)  # no new columns
  return(imputed_data)
}

## 1.5 SVD Imputation (SoftImpute)
svd_imputation <- function(df, rank.max = 5, lambda = 1, thresh = 1e-5, maxit = 100){
  # Convert to matrix
  mat <- as.matrix(df)
  
  # Use softImpute
  fit <- softImpute(mat, rank.max = rank.max, lambda = lambda, maxit = maxit, thresh = thresh)
  
  # Reconstruct the completed matrix
  mat_imputed <- softImpute::complete(fit, mat)
  
  # Return as data.frame with original names
  df_imputed <- as.data.frame(mat_imputed)
  colnames(df_imputed) <- colnames(df)
  rownames(df_imputed) <- rownames(df)
  
  return(df_imputed)
}

## 1.6 Fill with zeros
zero_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    col_data[is.na(col_data)] <- 0
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.7 Fill with half-min of non-missing values
half_min_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    non_zero <- col_data[col_data > 0]
    if(length(non_zero) > 0){
      half_min_val <- min(non_zero, na.rm = TRUE) / 2
      col_data[is.na(col_data)] <- half_min_val
    } else {
      # if entire column is zero or NA, just set to 0 or small value
      col_data[is.na(col_data)] <- 0
    }
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.8 Fill with median
median_imputation <- function(df){
  df_imputed <- df
  for(i in seq_along(df_imputed)){
    col_data <- df_imputed[[i]]
    col_data[is.na(col_data)] <- median(col_data, na.rm = TRUE)
    df_imputed[[i]] <- col_data
  }
  return(df_imputed)
}

## 1.9 No imputation (pass-through)
no_imputation <- function(x) {
  return(x)
}


###############################
##   2. Normalization        ##
###############################

## 2.1 TSN normalization (divide each row by sum of row)
TSN_normalize_features <- function(df){
  # for each row, divide each element by the row sum
  row_sums <- rowSums(df, na.rm = TRUE)
  # Replace any zero row sums with 1 to avoid division by zero
  row_sums[row_sums == 0] <- 1
  df_norm <- sweep(df, 1, row_sums, FUN = "/")
  return(df_norm)
}

## 2.2 Min-Max scaler 
scale_minmax <- function(df){
  # We can do it manually or use caret::preProcess
  # Here is a manual approach for numeric columns
  df_mat <- as.matrix(df)
  mins <- apply(df_mat, 2, min, na.rm = TRUE)
  maxs <- apply(df_mat, 2, max, na.rm = TRUE)
  
  scaled_mat <- sweep(df_mat, 2, mins, FUN = "-")
  ranges <- maxs - mins
  scaled_mat <- sweep(scaled_mat, 2, ranges, FUN = "/")
  
  # convert back to data frame
  df_scaled <- as.data.frame(scaled_mat)
  colnames(df_scaled) <- colnames(df)
  rownames(df_scaled) <- rownames(df)
  
  return(df_scaled)
}

## 2.3 Z-score normalization
z_score_normalization <- function(df){
  # scale() in R does mean-center and unit variance by column
  df_scaled <- scale(df, center = TRUE, scale = TRUE)
  return(as.data.frame(df_scaled))
}

## 2.4 Log10 normalization
log_normalize_rows <- function(df, offset = 1){
  if(offset <= 0){
    stop("Offset must be > 0 to avoid log(0).")
  }
  df_mat <- as.matrix(df)
  df_log <- log10(df_mat + offset)
  
  df_out <- as.data.frame(df_log)
  colnames(df_out) <- colnames(df)
  rownames(df_out) <- rownames(df)
  return(df_out)
}

## 2.5 No normalization (pass-through)
# no_normalization <- function(x){
#   return(x)
# }


###############################
##  3. Model Stability (RF)  ##
###############################

evaluate_model_stability <- function(X, y,
                                     ntree_candidates =c(100,500, 1000, 2500)) {
  # We want to replicate StratifiedKFold with n_splits = min number of samples in any class
  
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  
  folds <- min(table(y))
  # If folds < 2, cross-validation can't be done; return NA
  if(folds < 2){
    return(list(cv_mean = NA, cv_std = NA))
  }

  # Prepare data
  data_in <- data.frame(X, Class = factor(y))

  # Define trainControl
  ctrl <- trainControl(method = "cv", number = folds, classProbs = TRUE, savePredictions = "final")
  
  # collect all per-fold accuracies
  all_accs <- c()
  
  # Loop through each combo
  for (nt in ntree_candidates) {
    for (m in mtry_candidates) {
      
      set.seed(sample(1:999, 1)) 
      rf_fit <- caret::train(
        Class ~ .,
        data = data_in,
        method = "rf",
        trControl = ctrl,
        metric = "Accuracy",
        tuneGrid = data.frame(mtry= m),
        ntree = nt
      )
      
      # append this combo's fold-wise accuracies
      all_accs <- c(all_accs, rf_fit$resample$Accuracy)
    }
  }
  
  # View model predictions made for each fold: Caret stores resample accuracies in rf_fit$resample$Accuracy
  # accuracy_vals <- rf_fit$resample$Accuracy
  cv_mean <- mean(all_accs, na.rm = TRUE)
  cv_std  <- sd(all_accs, na.rm = TRUE)

  return(list(cv_mean = cv_mean, cv_std = cv_std))
}

###############################
##  4. Cluster resolution  ##
###############################

## Calculate cluster resolution using the first two PCA components
calculate_cluster_resolution <- function(data1, data2) {
  centroid1 <- colMeans(data1)
  centroid2 <- colMeans(data2)
  
  # Computes the Euclidean distance With Dim.1 as x-axis and Dim.2 as y-axis
  centroid_distance <-  sqrt((centroid1[1][[1]] - centroid2[1][[1]])^2 + (centroid1[2][[1]] - centroid2[2][[1]])^2)
  
  avg_distance1 <- mean(sqrt(rowSums((as.matrix(data1) - 
                       matrix(centroid1, nrow = nrow(data1), ncol = ncol(data1), byrow = TRUE))^2)))
  avg_distance2 <- mean(sqrt(rowSums((as.matrix(data2) - 
                       matrix(centroid2, nrow = nrow(data2), ncol = ncol(data2), byrow = TRUE))^2)))
  
  resolution <- centroid_distance / (avg_distance1 + avg_distance2)
  return(resolution)
}

## Compute average cluster resolution over all pairs of groups
calculate_average_cluster_resolution <- function(X, group_vector) {
  pca_model <- res.pca <- FactoMineR::PCA(
    X,
    scale.unit = FALSE,
    graph = FALSE)
  scores <- as.data.frame(pca_model$ind$coord[, 1:2])
  
  group <- as.factor(group_vector)
  group_levels <- levels(group)
  
  pair_resolutions <- c()
  
  for (i in 1:(length(group_levels) - 1)) {
    for (j in (i + 1):length(group_levels)) {
      data1 <- scores[group == group_levels[i], , drop = FALSE]
      data2 <- scores[group == group_levels[j], , drop = FALSE]
      
      if (nrow(data1) > 1 && nrow(data2) > 1) {
        res <- calculate_cluster_resolution(data1, data2)
        pair_resolutions <- c(pair_resolutions, res)
      }
    }
  }
  
  avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
  return(avg_resolution)
}

###################################################
##   5. Pairwise Significance Testing            ##
###################################################

pairwise_significance_tests <- function(input_df, 
                                        group_col,
                                        start_col_index = 2){
  # For each feature from start_col_index onward,
  #   for each pair of group categories,
  #     check if both groups have >=3 data points
  #     do Shapiro test for normality
  #     if not normal => Mann-Whitney (wilcox.test)
  #     else => t.test
  # Collect p-values and do multiple corrections
  
  df_results <- data.frame(Feature=character(), comparison_pair=character(), pval=integer())
  
  # get unique group combos
  group_pairs <- utils::combn(unique(as.character(input_df[[group_col]])), 2)

  for(feature_col in start_col_index:ncol(input_df)){
    for (col in 1:ncol(group_pairs)){
      p_1 <- group_pairs[1,col]
      p_2 <- group_pairs[2,col]

      vec1 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_1), feature_col]))
      vec2 <- as.numeric(unlist(input_df[which(input_df[[group_col]] == p_2), feature_col]))
      
      # ensure at least 3 non-NA data points
      if(sum(!is.na(vec1)) >= 3 && sum(!is.na(vec2)) >= 3){
        # skip if all values are the same
        # if(all(vec1 == vec1[1]) || all(vec2 == vec2[1])){
        #   next
        # }
        
        # normality tests
        p_shapiro1 <- tryCatch(shapiro.test(vec1)$p.value, error = function(e) NA)
        p_shapiro2 <- tryCatch(shapiro.test(vec2)$p.value, error = function(e) NA)
        
        # decide parametric vs non-parametric
        if(is.na(p_shapiro1) || is.na(p_shapiro2)){
          # if for some reason test failed, skip
          next
        }
        
        if(p_shapiro1 < 0.05 || p_shapiro2 < 0.05){
          # mann-whitney
          pval_test <- tryCatch(wilcox.test(vec1, vec2)$p.value, error = function(e) NA)
        } else {
          # t-test
          pval_test <- tryCatch(t.test(vec1, vec2, var.equal = FALSE)$p.value, error = function(e) NA)
        }
        
        if(!is.na(pval_test)){
          # append to df
          df_results <- rbind(df_results, data.frame(
            Feature = colnames(input_df)[feature_col],
            comparison_pair = paste0(p_1, " & ", p_2),
            pval = as.numeric(pval_test),
            stringsAsFactors = FALSE
          ))
        }
      }
    }
  }
  
  # multiple corrections
  pvalue_correction_methods <- "bonferroni" #, c("holm", "hochberg", "hommel", "BH", "BY")
  
  # For convenience, store all results in a list
  # results_list <- list()
  # j <- 1
  # for(method in pvalue_correction_methods){
    temp <- df_results
    temp$adjusted_pvalue <- p.adjust(temp$pval, method = pvalue_correction_methods)
    significant <- temp %>% dplyr::filter(adjusted_pvalue < 0.05) %>% arrange(adjusted_pvalue)
    # results_list[[j]] <- significant
    
    if(nrow(significant) == 0){
      cat(paste0(pvalue_correction_methods, " did not result in any significant compound\n"))
    } else {
      # cat(paste0("Significant results for method: ", method, "\n"))
      # print(significant)
      cat(paste0(pvalue_correction_methods, " has ", length(unique(significant$Feature)), " significant features\n"))
      # j <- j + 1
    }
  # }
  
  return(list(
    uncorrected = df_results,
    corrected   = unique(significant$Feature)  # results_list
  ))
}

evaluate_rf_accuracy_kappa_auc <- function(data, 
                                           type_col, 
                                           remove_cols, 
                                           train_proportion,
                                           use_store_vs_environmental_split = FALSE,
                                           use_source_split = FALSE,
                                           ntree_candidates = c(100, 500, 1000, 2500), 
                                           seed  = NULL, 
                                           metric = "Accuracy") {
  
  # Train/Test Split
  if(use_store_vs_environmental_split){
    cat("\n### Using Store-vs-Environmental Splitting ###\n")
    
    train_data <- data[data$Source == "Store-Bought", ]
    test_data  <- data[data$Source == "Environmental", ]
    
    if(nrow(train_data) < 1) stop("No 'Store-Bought' samples for training!")
    if(nrow(test_data) < 1) stop("No 'Environmental' samples for testing!")
    
  } else if(use_source_split){
    cat("\n### Using Source-based Splitting with Environmental 50:50 Split ###\n")
    
    store_bought_set <- data[data$Source == "Store-Bought", ]
    environmental_set <- data[data$Source == "Environmental", ]

    set.seed(123)
    
    env_train_index <- caret::createDataPartition(environmental_set[[type_col]], p = 0.5, list = FALSE)
    environmental_training_set <- environmental_set[env_train_index, ]
    environmental_hold_out_testing_set <- environmental_set[-env_train_index, ]
    
    train_data <- rbind(store_bought_set, environmental_training_set)
    test_data  <- environmental_hold_out_testing_set
    
    if(nrow(train_data) < 1) stop("Training data is empty!")
    if(nrow(test_data) < 1) stop("Test data is empty!")
  } else {
    cat("\n### Using Original Train/Test Split ###\n")
    # X <- data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
    y <- data[[type_col]]
    set.seed(123)
    train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
    train_data <- data[train_index, ]
    test_data  <- data[-train_index, ]
  }
  
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  min_class_count <- min(table(y_train))
  folds <- ifelse(min_class_count > 1, min_class_count, 2)
  cv_ctrl <- trainControl(
    method     = "cv",
    number     = folds,
    classProbs = TRUE
  )
  
  num_features    <- ncol(X_train)
  mtry_candidates <- c(floor(sqrt(num_features)), floor(log2(num_features)))
  rf_grid         <- expand.grid(mtry = mtry_candidates)
  
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test,
    cv_ctrl, ntree_candidates, metric
  )
  
  eval_metrics <- all_feats$eval_metrics
  
  return(eval_metrics)
}

##################################################################################
##   6. Final Workflow: Iterate Over Datasets, Imputation, and Normalization    ##
##################################################################################
# Define imputation and normalization methods as lists of functions
imputation_methods <- list(
  random_forest_imputation   = random_forest_imputation,
  # bayesian_pca_imputation    = bayesian_pca_imputation,
  mean_imputation            = mean_imputation,
  knn_imputation             = knn_imputation,
  # svd_imputation             = svd_imputation,
  zero_imputation            = zero_imputation,
  half_min_imputation        = half_min_imputation,
  median_imputation          = median_imputation
  # No_imputation              = no_imputation
)

normalization_methods <- list(
  TSN_Normalization    = TSN_normalize_features,
  Min_Max_Scaler       = scale_minmax,
  Z_Score_Normalization= z_score_normalization,
  # Log10_Normalization  = log_normalize_rows,
  No_normalization     = no_normalization
)

find_best_impute_normalize <- function(df,
                                       type_col,
                                       remove_cols,
                                       group_for_significance,
                                       use_store_vs_environmental_split,
                                       use_source_split) {
  # 1) Extract numeric features X and target y
  X       <- df %>% dplyr::select(-all_of(c(remove_cols, type_col))) %>% as.data.frame()
  y       <- df[[type_col]]
  
  # We'll store the results
  results_list <- list()
  
  # 2) Loop over imputation + normalization
  for(imp_name in names(imputation_methods)){
    impute_fun <- imputation_methods[[imp_name]]
    
    for(norm_name in names(normalization_methods)){
      norm_fun <- normalization_methods[[norm_name]]
      
      # Impute
      if(imp_name == "No_imputation"){
        X_imputed <- X
      } else {
        X_imputed <- tryCatch(
          impute_fun(X),
          error = function(e) {
            message("Error in ", imp_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_imputed)) next
      }
      
      # Normalize
      if(norm_name == "No_normalization"){
        X_norm <- X_imputed
      } else {
        X_norm <- tryCatch(
          norm_fun(X_imputed),
          error = function(e){
            message("Error in ", norm_name, ": ", e$message)
            return(NULL)
          }
        )
        if(is.null(X_norm)) next
      }
      
      if(any(is.na(X_norm))){
        stop("Combo", " Imputation method", imp_name, " and Normalization method ", norm_name, " produced NAs!")
      }
      if(any(!is.finite(as.matrix(X_norm)))){
        stop("Combo", " Imputation method", imp_name, " and Normalization method ", norm_name, " produced Inf/NaN!")
      }
      
      # Evaluate cluster resolution (using PCA on numeric features)
      cluster_res <- calculate_average_cluster_resolution(X_norm, y)
      
      # Evaluate significance (# of significant features)
      # We need the entire df to do the pairwise test, so let's combine X_norm with original grouping col:
      # df_temp <- df
      
      # Overwrite numeric columns with X_norm
      # numeric_cols <- colnames(X)
      # df_temp[numeric_cols] <- X_norm

      # sig_res  <- pairwise_significance_tests(df_temp,
      #                                         group_col=group_for_significance,
      #                                         start_col_index=start_col_index)
      # "max_signif" is the maximum number of sig features across correction methods
      # sig_count <- max(
      #   sapply(sig_res$corrected, function(df) length(unique(df$Feature)))
      # )
      
      ###############################
      ##  New: Correlation Score: Computes the Pearson correlation between the uppertriangle elements of the correlation matrix of your original data (computed with pairwise complete observations) and that of your imputed/normalized data. A higher score means that the imputation has preserved the pairwise relationships better.      The correlation score is computed as a Pearson correlation coefficient, which can range from -1 to 1. In this context, a score close to 1 indicates that the imputed/normalized data have preserved the original pairwise relationships very well. A score of 0 would indicate no linear relationship between the two sets of correlation values, and a negative value would suggest that the dependency structure is inversely related.##
      ###############################
      # Compute correlation matrix on original data using pairwise complete observations
      orig_corr <- cor(X, use = "pairwise.complete.obs")
      # Compute correlation matrix on imputed/normalized data
      imp_corr <- cor(X_norm)
      # Extract upper-triangle values (excluding diagonal) for both matrices
      orig_upper <- orig_corr[upper.tri(orig_corr)]
      imp_upper  <- imp_corr[upper.tri(imp_corr)]
      # Compute correlation between the two sets of values
      corr_score <- cor(orig_upper, imp_upper, use = "complete.obs")
      
      ###############################
      ##  New: KS Test p-value: Compare the distribution of imputed values against that of the observed data ##
      ###############################
      # Combine observed values and imputed values across all numeric columns
      observed_all <- c()
      imputed_all  <- c()
      
      for(col in names(X)) {
        missing_idx <- which(is.na(X[[col]]))
        # Only consider columns that originally had missing values and at least one observed value
        if(length(missing_idx) >= 1 && length(X[[col]][!is.na(X[[col]])]) >= 1){
          obs_values <- X[[col]][!is.na(X[[col]])]
          imp_values <- X_norm[[col]][missing_idx]
          observed_all <- c(observed_all, obs_values)
          imputed_all  <- c(imputed_all, imp_values)
        }
      }
      
      if(length(observed_all) < 2 || length(imputed_all) < 2) {
        ks_p <- NA
      } else {
        ks_result <- tryCatch({
          ks.test(observed_all, imputed_all)
        }, error = function(e) NULL)
        if(!is.null(ks_result)){
          ks_p <- ks_result$p.value
        } else {
          ks_p <- NA
        }
      }
      
      ###############################
      ##  New: RF Test Accuracy    ##
      ###############################
      # Use the imputed/normalized data (X_norm) and class vector y to compute test accuracy.
      df_input_test_acc <- cbind(X_norm, df[,c(type_col, remove_cols)])

      acc_kappa_auc <- evaluate_rf_accuracy_kappa_auc(data = df_input_test_acc,
                                                      type_col=type_col,
                                                      remove_cols=remove_cols,
                                                      train_proportion = train_proportion,
                                                      use_store_vs_environmental_split = use_store_vs_environmental_split,
                                                      use_source_split = use_store_vs_environmental_split)
      
      
      
      # Evaluate CV accuracy
      rf_stability <- evaluate_model_stability(X_norm, y)
      cv_mean <- rf_stability$cv_mean
      cv_std  <- rf_stability$cv_std
      
      results_list[[length(results_list)+1]] <- data.frame(
        Imputation   = imp_name,
        Normalization= norm_name,
        ClusterRes    = cluster_res,
        CorrScore     = corr_score,
        KS_p          = ks_p,
        CVmean       = cv_mean,
        CVstd        = cv_std,
        TestAcc       = acc_kappa_auc$Accuracy,
        Kappa = acc_kappa_auc$Kappa,
        AUC = acc_kappa_auc$AUC,
        stringsAsFactors=FALSE
      )
    }
  }
  
  df_results <- do.call(rbind, results_list)
  
  # Build an ordering index: primary by TestAcc, secondary by ClusterRes, etc.
  ord <- order(
    -df_results$CVmean,
    df_results$CVstd,
    -df_results$ClusterRes,
    -df_results$CorrScore,   # larger = better CorrScore
    df_results$KS_p           # smaller  = better KS_p
  )
  
  # Initialize and fill combined_rank so that best row gets 1
  df_results$combined_rank <- NA_integer_
  df_results$combined_rank[ord] <- seq_along(ord)
  
  # Pick the best
  best_row <- df_results[ which.min(df_results$combined_rank) , ]
  
  # Return the full table + the best combo row
  return(list(
    results_table = df_results,
    best_combo    = best_row
  ))
}
```

## Gower Distance and PAM
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Load necessary package
library(cluster)

# Create a sample dataset with mixed predictors
set.seed(123)
data_mixed <- data %>% dplyr::select(-c("Source", "technique"))
data_mixed$Polymer <- as.factor(data_mixed$Polymer)
subcat <- as.character(data_mixed$Subcategory)
data_mixed$Subcategory <- ave(subcat, subcat, FUN = function(x) {
  if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
})

# Compute Gower distance; note that qualitative variables must be factors.
gower_dist <- daisy(data_mixed[, -1], metric = "gower")  # Exclude 'target' from clustering

# Perform PAM clustering; choose a number of clusters (here, k = 3)
pam_result <- pam(gower_dist, k = 6)

# Compute silhouette information for the PAM clustering result
sil <- silhouette(pam_result$clustering, dist = gower_dist)

# Convert the silhouette object to a data frame and include the target label.
# The rownames of the silhouette object should correspond to the order in data_mixed.
sil_df <- data.frame(
  observation = data_mixed$Subcategory,
  cluster = sil[, "cluster"],
  neighbor = sil[, "neighbor"],
  sil_width = sil[, "sil_width"]
)
# Add the target column (assuming the row order matches)
sil_df$Subcategory <- data_mixed$Subcategory

# Create a silhouette plot with ggplot2.
# Here, we order observations by silhouette width and facet by cluster.
ggplot(sil_df, aes(x = reorder(observation, sil_width), y = sil_width, fill = Subcategory)) +
  geom_bar(stat = "identity", width = 1) +
  facet_wrap(~ cluster, scales = "free_y") +
  coord_flip() +
  labs(
    x = "Observations (ordered by silhouette width)",
    y = "Silhouette Width",
    # title = "Silhouette Plot with Target Labels"
  ) +
  theme_minimal(base_size = 25) +
   theme(legend.position = 'hidden')
```

## PCA
```{r , echo=FALSE, warning = FALSE, message=FALSE}
select_feats <- c(run_rf_analysis_list_SB_and_ENV_train[[1]]$top_importance_results$top_100$selected_features, 
                  colnames(run_rf_analysis_list_SB_and_ENV_train[[1]]$final_imp_norm_dat)[1:5])

df <- run_rf_analysis_list_SB_and_ENV_train[[1]]$final_imp_norm_dat %>% 
       dplyr::select(all_of(select_feats)) %>%
       dplyr::relocate(Subcategory, Source, Polymer, technique, Plastic_type, .before = 1)

res.pca <- FactoMineR::PCA(
  df %>% dplyr::select(-c("Subcategory", "Source", "Plastic_type", "Polymer", "technique")),
  scale.unit = FALSE,
  graph = FALSE)

# Scree plot
# fviz_screeplot(res.pca, ncp=10)

# Biplot
pca_plot <- factoextra::fviz_pca_biplot(res.pca,
                            geom = c("point", "text"),
                            label = "none",
                            invisible = "var",
                            repel = TRUE,
                            labelsize = 10,
                            habillage = factor(df$Subcategory),
                            # addEllipses = TRUE,
                            # ellipse.level=0.95,
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = "",
                            # xlim = c(-0.5, 0.8),
                            # ylim = c(-0.5, 1.2)
                            ) +
  guides(fill=guide_legend(ncol=6)) + # Specify the number of column of legend text labels
  # if has error "Too few points to calculate an ellipse"
  ggforce::geom_mark_ellipse(aes(fill = Groups,
                                 color = Groups),
                             label.buffer = unit(40, 'mm')) +
  theme(legend.position = 'bottom',
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()
        )
pca_plot

ggsave(filename = paste0("GC_ENVonly_Plastic_type.png"), 
       plot = pca_plot, 
       device = "png",
       dpi = 600,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 10,       # Height in inches (adjust as needed)
       units = "in")
```

### Barplot of Top 10 Variable contributions to first n dimensions

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Variable contributions to first n dimensions
## To PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
## To PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)

# Extract top 10 contribution to PC1
dim_df <- as.data.frame(res.pca$var$contrib) %>% 
  rownames_to_column(var = "Feature") %>%
  arrange(desc(Dim.1), desc(Dim.2))

top <- dim_df[1:10,]$Feature

# Plot bar plot with original raw metal measurements, before data normalization 
data_summary <- metal %>% rownames_to_column(., var = "File") %>%
  select(all_of(c("File", top))) 

p <- "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/Table of product categorization_NewFileName.xlsx"

sampinfo <- readxl::read_excel(path = p, 
                               sheet = excel_sheets(path = p)[1])

# joining data frame with sampinfo df
data_summary <- left_join(x = data_summary, 
                          y = sampinfo %>% select(all_of(c("File", "Subcategory"))), 
                          by = 'File')

data_summary <- data_summary %>%
  mutate(file_subcat = paste0(File, "_", Subcategory)) %>% 
  relocate(file_subcat, Subcategory, .after = 1) %>%
  pivot_longer(cols = 4:ncol(.), names_to = "Features", values_to = "Values")
# group_by(Subcategory, Features) %>%
# summarise(mean = mean(Values), 
#           sd = sd(Values), .groups = 'drop')

ggplot(data = data_summary, aes(x = file_subcat, y = Values, fill = Features)) +
  geom_bar(stat = "identity", position = 'dodge') +
  # geom_errorbar(aes(ymin = mean - sd,
  #                   ymax = mean + sd),
  #               position = position_dodge(width = 0.9), width = 0.5) +
  # scale_fill_manual(values = c("#a6cee3",
  #                              "#1f78b4",
  #                              "#b2df8a",
  #                              "#33a02c",
  #                              "#fb9a99",
  #                              "#e31a1c",
  #                              "#fdbf6f",
  #                              "#ff7f00",
  #                              "#cab2d6",
  #                              "#6a3d9a",
  #                              "#003c30",
  #                              "#35978f",
  #                              "#80cdc1",
  #                              "#bf812d",
  #                              "#b15928")) +
labs(x = "Plastic product group",
     y = "Concentration [g/kg of samples]",
     fill = "Trace Metals"
     ) + 
  theme_classic(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0, color = "black", face = "bold"),
        axis.ticks.length.x = unit(0, "cm")) +
  scale_y_continuous(expand = c(0,0))
```

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# 3D PCa plot ===============================================================================
# FactoMineR_p <- FactoMineR::PCA(df_pca,
#                                 scale.unit = FALSE)
# 
# scores <- as.data.frame(FactoMineR_p$ind$coord[,1:3]) %>%
#   rownames_to_column(., var = "File")
# category <- sapply(scores$File, function(f) {
#   unique(gc_hplc_list[[1]]$Category[match(f, gc_hplc_list[[1]]$File)])
# })
# 
# # Add to scores data frame
# scores$Category <- as.factor(category)
# 
# rgl::plot3d(scores$Dim.1, scores$Dim.2, scores$Dim.3,
#        col = as.integer(scores$Category),
#        xlab = "PC1",
#        ylab = "PC2",
#        zlab = "PC3",
#        type = "p")
# 
# # Add legend
# rgl::legend3d("topright", legend = levels(scores$Category), col = 1:length(levels(scores$Category)), pch = 16)

# factoextra::fviz_screeplot(FactoMineR_p)
# factoextra::fviz_pca_biplot(FactoMineR_p,
#                             habillage = unique(gc_hplc_list[[1]]$File))
```

### Cluster resolution
```{r , echo=FALSE, warning = FALSE, message=FALSE}
input1 <- test2 # %>% dplyr::select(-c("technique")) # filter(Subcategory %notin% c("Various plastic waste", "Polystyrene food waste"))
data <- input1[, 4:ncol(input1)]
group <- factor(input1$Subcategory)
max_resolution <- 0
resolutions <- c()

pca_model <- prcomp(data, center = TRUE, scale. = FALSE)
scores <- pca_model$x

group_levels <- levels(group)
pair_resolutions <- c()

# Calculate cluster resolution for each pair of groups
for (g1 in 1:(length(group_levels) - 1)) {
  for (g2 in (g1 + 1):length(group_levels)) {
    data1 <- scores[group == group_levels[g1], 1:2]
    data2 <- scores[group == group_levels[g2], 1:2]
    resolution <- calculate_cluster_resolution(data1, data2)
    pair_resolutions <- c(pair_resolutions, resolution)
  }
}

avg_resolution <- mean(pair_resolutions, na.rm = TRUE)
resolutions <- c(resolutions, avg_resolution)
print(resolutions)
```

## Venn Diagram: top PCA loadings vs. top ML contributors
```{r echo=FALSE, warning=FALSE, message=FALSE}
library(VennDiagram)
significant_features <- rfa_important_features

# Calculate overlaps and unique features
overlap_features <- intersect(significant_features, top_pca_loadings)
only_significant <- setdiff(significant_features, top_pca_loadings)
only_pca <- setdiff(top_pca_loadings, significant_features)

# Helper function to split text into chunks of 5 and add blank lines
format_chunks <- function(text, chunk_size = 5) {
  chunks <- split(text, ceiling(seq_along(text) / chunk_size))
  formatted_text <- sapply(chunks, function(chunk) paste(chunk, collapse = ", "))
  paste(formatted_text, collapse = "\n\n")
}

# Format annotations
overlap_text <- format_chunks(overlap_features, chunk_size = 5)
only_significant_text <- format_chunks(only_significant, chunk_size = 5)
only_pca_text <- format_chunks(only_pca, chunk_size = 5)

venn.plot <- venn.diagram(
  x = list(
    "Significant Features" = significant_features,
    "Top PCA Loadings" = top_pca_loadings
  ),
  filename = NULL, # Set to NULL for drawing in R plot window
  col = "black",
  fill = c("skyblue", "pink"),
  alpha = 0.5,
  cex = 1.5,
  cat.cex = 1.2,
  cat.pos = c(-20, 20),
  cat.dist = 0.05,
  main = "Overlap Between Significant Features and PCA Loadings"
)

# Draw the diagram in the plot window
grid.newpage()
grid.draw(venn.plot)

# Add annotations
grid.text(
  label = paste("Overlapping Features:\n", overlap_text),
  x = 0.5, y = 0.7, gp = gpar(fontsize = 10), just = "center"
)

grid.text(
  label = paste("Significant Only:\n", only_significant_text),
  x = 0.25, y = 0.3, gp = gpar(fontsize = 10), just = "center"
)

grid.text(
  label = paste("PCA Only:\n", only_pca_text),
  x = 0.75, y = 0.3, gp = gpar(fontsize = 10), just = "center"
)
```


## HCA

If we filter unique compounds by File (compounds appear in at least 2 files), then all File in df in gc_hplc_list are the same. That's why here we used gc_hplc_list[[1]]

This exercise does not seems to affect the HCa. Keep in mind that for all 3 cases of data combinations: none of them has compounds that occur in all samples. NONE of THEM!!!!!

### Clustering of samples

(Jan 11th 2024): 
\ICP only data shows misgrouping between the following Subcategories: 
- Cigarillo tip//Bottle caps
- Food wrapper// Food packaging// Food containers//plastic drinking straws//Various plastic waste

\GC+ICP data show clear separation between GC and ICP data into 2 clusters. From ICP cluster, the misgrouping between the following Subcategories is the same as the case of ICP only. From the GC cluster, the grouping is more or less the same as the case of GC only.

\HPLC+ICP data, the cluster of both ICP and HPLC side change a bit. However for ICP side, misgrouping between the following Subcategories still remains as the case of ICP only. For HPLC side, the organization of the sample into grouping change a bit as the plastic toy balls are separated into their own top cluster (same height level with ICP and rest of HPLC branches) 

\GC+HPLC+ICP data, for ICP side, misgrouping between the following Subcategories still remains as the case of ICP only. For GC + HPLC side, it looks almost exactly like the case of GC+HPLC.

(UPdate 06 Feb 2024:) common metrics to evaluate performance of clustering models:
- Silhouette Score (https://stackoverflow.com/questions/33999224/silhouette-plot-in-r): Higher score => better clustering
- Calinski-Harabaz Index (https://search.r-project.org/CRAN/refmans/fpc/html/calinhara.html): Higher score => better clustering
- Davies-Bouldin Index ():

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Data prep for HCA ---------------------
df <- gc_combined_SB_ENV_shared_cols_ntr_clean_0.001_TSN

hc_df <- df %>% 
  dplyr::select(-c("Subcategory", "Source", "Plastic_type", "Polymer", "technique"))

## Dissimilarity Indices Calculated by vegan::vegdist()
hca_samp <- stats::hclust(vegan::vegdist(hc_df,
                                         method = "robust.aitchison")) # Since our data is continous -> canberra // manhattan // aitchison // robust.aitchison

par(cex = 1)

plot(hca_samp,
     labels = df$Subcategory, # rownames(df_percentage),# df_percentage$Subcategory,
     hang = -1,
     main = "")
```


A smaller Aitchison distance between two samples would indicate that their compositional structures are similar in terms of the ratios between components.

For selecting Dissimilarity Indices Calculated by vegan::vegdist(), since our data is continous, not normally distributed, and the data is sparse (meaning our data has lots of NA or NULL before data imputation, This basically because there are compounds that occur in some samples and not in the others). Manhattan dissimilarity index separating ATDGCMS and HPLCTOFMS into 2 big clusters, while Canberra//Aitchison//Robust.aitchison grouped ATDGCMS as a subgroup of a part of HPLCTOFMS data.

Since for each observations of each sample, the sum of values of collapsed compounds is sum up to 100%, Aitchison or Robust.Aitchison are currently the best option for dissimilarity index.

***Why Aitchison or Robust.Aitchison for OUR DATA, AKA. Compositional Data?***

Compositional data have particular characteristics and constraints that standard distance metrics like Euclidean don't handle well. Here are some reasons why Aitchison distance is more suitable:

**Closure Constraint:** Because the components in compositional data sum to a constant value (like 1 or 100%), the data is subject to a closure constraint. This constraint violates the assumption of independence between variables that many standard statistical techniques rely on. Aitchison distance accounts for this constraint.

*Implications of Closure Constraint* *Loss of Degrees of Freedom:* Because of the closure constraint, one variable is effectively 'dependent' on the others. For instance, in our example, knowing the proportions of Additive A and Additive B is enough to determine the proportion of Additive C.

*Spurious Correlations:* The closure constraint can lead to spurious or misleading correlations between variables. For example, an increase in the proportion of Additive A will necessarily result in a decrease in the sum of the proportions of Additives B and C, even if there's no biological or chemical reason for them to be inversely related.

*Non-Independence of Components:* In compositional data, the components are not independent of each other, violating the assumptions of independence that many statistical tests and measures (like the Euclidean distance) rely on.

*Comparisons are Relative:* In compositional data, what matters is the relative sizes of the components, not their absolute sizes. Any analysis should, therefore, focus on these relative proportions rather than absolute values.

For these reasons, specialized statistical methods like Aitchison distance are often used for compositional data. These methods take the closure constraint into account and focus on the relative proportions of the different components, providing a more accurate and interpretable analysis.

In this workflow on tracking microplastic additives in aquatic environments, understanding the closure constraint can be pivotal. If you're looking at the relative abundances of different types of plastic additives in water samples, those abundances are compositional data subject to the closure constraint. Using standard statistical techniques without accounting for this constraint could lead to misleading conclusions.

**Relative Importance:** In compositional data, what's generally important is the relative proportion between components, rather than their absolute values. The Aitchison distance is designed to capture the relationships between these relative values effectively.

**Log-Ratio Transformation:** The Aitchison distance is based on log-ratio transformations, which have been proven to be an effective way to analyze compositional data. The log-ratio removes the closure constraint, making the data easier to analyze statistically.

**Handling Zeros:** The robust variant of Aitchison distance often includes techniques for handling zeros in compositional data, which are common and challenging to deal with.

**Scale-Invariance:** Aitchison distance is scale-invariant, meaning that multiplying a component by a constant will not change the distance. This is crucial for compositional data, where only the relative relationships between components matter.

**Interpretable:** The distance measure reflects compositional difference in a way that's meaningful within the context of the data, making it easier to understand and interpret the results.



So with the dimensional reduction of compounds from n = 13046 to 33, we still achieve a somewhat good clustering result in which sample of the same category tends to cluster together.

# Balanced Accuracy helper function
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
compute_balanced_accuracy <- function(true, pred) {
  cm <- table(true, pred)
  if(nrow(cm) == 2 && ncol(cm) == 2) {
    sensitivity <- cm[2,2] / sum(cm[2,])
    specificity <- cm[1,1] / sum(cm[1,])
    return((sensitivity + specificity) / 2)
  } else {
    # For multiclass: compute recall (sensitivity) for each class and then take the mean.
    recalls <- diag(cm) / rowSums(cm)
    return(mean(recalls, na.rm = TRUE))
  }
}
```

# Recursive Feature Addition
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
recursive_feature_addition <- function(X_train, y_train,
                                       X_test,  y_test,
                                       base_features,
                                       cv_ctrl,
                                       ntree_candidates,
                                       metric) {
  # start
  remaining   <- setdiff(colnames(X_train), base_features)
  best_feats  <- base_features
  
  # get baseline accuracy on base_features alone
  base_res    <- tune_rf_subset(
    X_train[, base_features, drop=FALSE], y_train,
    X_test [, base_features, drop=FALSE], y_test,
    cv_ctrl, ntree_candidates, metric
  )
  best_accuracy  <- base_res$Accuracy
  best_kappa <- base_res$Kappa
  best_auc <- base_res$AUC
  
  # iterative addition
  while(length(remaining) > 0) {
    improved <- FALSE
    for(feat in remaining) {
      trial_feats <- c(best_feats, feat)
      res         <- tune_rf_subset(
        X_train[, trial_feats, drop=FALSE], y_train,
        X_test [, trial_feats, drop=FALSE], y_test,
        cv_ctrl, ntree_candidates, metric
      )
      if((res$Accuracy > best_accuracy) & (res$Kappa > best_kappa) & (res$AUC > best_auc)) { # if there is Accuracy improvement, then add to base_features 
        best_accuracy <- res$Accuracy
        best_kappa <- res$Kappa
        best_auc <- res$AUC
        best_feats <- trial_feats
        improved   <- TRUE
        break
      }
    }
    # whether or not we improved, drop that last-tested feat
    remaining <- setdiff(remaining, feat)
    if(!improved) break
  }
  
  # final model on the chosen set
  final_res <- tune_rf_subset(
    X_train[, best_feats, drop=FALSE], y_train,
    X_test [, best_feats, drop=FALSE], y_test,
    cv_ctrl, ntree_candidates, metric
  )
  
  list(
    best_accuracy    = best_accuracy,
    best_kappa = best_kappa,
    best_auc = best_auc,
    best_features_rf = best_feats,
    final_model      = final_res$model
  )
}
```

# Random Forest
```{r , echo=FALSE, warning = FALSE, message=FALSE}
#
# Helper: retune mtry (based on subset size) + ntree for any train/test split
tune_rf_subset <- function(X_train, y_train, X_test, y_test,
                           cv_ctrl, ntree_candidates, metric) {
  p <- ncol(X_train)
  rf_grid <- expand.grid(
    mtry = unique(pmax(1, c(floor(sqrt(p)), floor(log2(p)))))
  )
  
  best_mod   <- NULL
  best_val   <- -Inf
  best_trees <- NA
  
  for(nt in ntree_candidates) {
    set.seed(123) # set.seed(sample.int(999,1))
    tmp <- caret::train(
      x         = X_train,
      y         = as.factor(y_train),
      method    = "rf",
      trControl = cv_ctrl,
      tuneGrid  = rf_grid,
      metric    = metric,
      ntree     = nt,
      importance= TRUE
    )
    this_val <- max(tmp$results[[metric]])
    if(this_val > best_val) {
      best_val   <- this_val
      best_mod   <- tmp
      best_trees <- nt
    }
  }

  preds    <- predict(best_mod, newdata = X_test, type = "raw")
  prob_matrix <- as.matrix(predict(best_mod, newdata=X_test, type="prob"))

  eval_metrics <- eval_metrics(true_labels = y_test, 
                               prob_matrix = prob_matrix, 
                               pred_labels = preds)
  
  return(list(model    = best_mod,
              ntree    = best_trees,
              p_m = prob_matrix,
              preds = preds,
              eval_metrics = eval_metrics,
              Accuracy = eval_metrics$Accuracy,
              Kappa = eval_metrics$Kappa,
              AUC = eval_metrics$AUC))
}
#
eval_metrics <- function(true_labels, prob_matrix, pred_labels){
  # 1) explicit length check with a custom error message
  if (length(pred_labels) != length(true_labels)) {
    stop(
      sprintf(
        "Length mismatch: predicted = %d,   true = %d",
        length(pred_labels),
        length(true_labels)
      )
    )
  }
  
  # 2) proceed as before
  cm <- tryCatch(
    caret::confusionMatrix(data = factor(pred_labels, levels = levels(true_labels)),
                           reference = true_labels),
    error = function(e) NULL
  )
  
  if (!is.null(cm)) {
    acc   <- as.numeric(cm$overall["Accuracy"])
    kappa <- as.numeric(cm$overall["Kappa"])
  } else {
    # fallback: simple accuracy & NA for kappa
    acc   <- mean(pred_labels == true_labels)
    kappa <- NA
  }
  
  # 3) multiclass ROC  AUC
  #    multiclass.roc() returns an object whose $auc slot is the
  #    oneversusall average AUC by default.
  auc_val <- tryCatch({
    roc_obj <- pROC::multiclass.roc(response = true_labels,
                                    predictor = prob_matrix)
    as.numeric(roc_obj$auc)
  }, error = function(e) NA)
  
  return(list(
    Accuracy = acc,
    Kappa    = kappa,
    AUC      = auc_val
  ))
}

####################################################
##########        CONFUSION MATRIX       ###########
####################################################
conf_mat_plot <- function(y_test, preds, conf_mat_title, accuracy) {
  # 1) Compute raw counts table: rows = treated labels, cols = predicted pristine labels
  cm_tab <- table(
    Actual    = as.character(y_test),
    Predicted = as.character(preds)
  )
  
  # 2) Turn it into a data.frame for ggplot
  cm_df <- as.data.frame(cm_tab, stringsAsFactors = FALSE) %>%
    dplyr::rename(Freq = Freq)
  
  # 3) Add total-per-row and percent-per-cell
  cm_df$Total <- NA
  cm_df$Percent <- NA
  for (label in unique(cm_df$Actual)){
    cm_df[which(cm_df$Actual == label),]$Total <- sum(cm_df[which(cm_df$Actual == label),]$Freq)
  }
  
  cm_df <- cm_df %>%
    mutate(
      Percent = round(Freq / Total * 100, 2)
    )
  
  
  cm_df <- cm_df %>%
    mutate(
      Label = ifelse(Predicted == Actual,
                     paste0(round(Percent,1),"%"),
                     ""
      )
    )
  
  # 5) Plot
  conf_mat <- ggplot(cm_df, aes(x = Predicted, y = Actual, fill = Percent)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") + #, limits = c(0,1)) +
    geom_text(aes(label = Percent), size = 5, show.legend = FALSE) +
    # explicitly drop any size guide, keep only the fill (Recall) legend
    guides(size = "none") +
    labs(
      title = paste0("Confusion Matrix of ENV  SB Mapping | ", conf_mat_title),
      subtitle = paste0(
        sprintf("Overall linking accuracy: %.2f%%\n", accuracy) 
        # # " Recall per treated class (N=", cm_df$Total[1], 
        # " replicates each)"
      ),
      x = "Predicted Class (Store-Bought)",
      y = "Actual Class (Environmental)",
      fill = "Percentage of Classification (%)" # "Recall %"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      axis.text.x  = element_text(angle = 45, hjust = 1),
      plot.title   = element_text(face = "bold", hjust = 0.5),
      plot.subtitle= element_text(hjust = 0.5)
    )
  return(conf_mat)
}


#

run_rf_analysis_manuscript1 <- function(data,
                            type_col,
                            remove_cols,
                            train_proportion        = 0.5,
                            use_source_split        = FALSE,               # Option 1: environmental 50:50 split and combine with store-bought
                            use_store_vs_environmental_split = FALSE,       # Option 2: use store-bought for training and environmental for testing
                            use_polymer             = FALSE,               # Whether to include "Polymer" as a predictor
                            do_pairwise_test        = FALSE,               # Run pairwise/ANOVA tests on imputed/normalized data
                            do_top_importance_selection = FALSE,           # Run additional classification using top-N features by importance
                            top_importance_counts   = c(100, 50, 25, 10),   # Which top features to try
                            data_name,              # Name of the input data combination for figure titles
                            seed                    = 123,
                            ntree_candidates        = c(100, 500, 1000, 2500),
                            metric                  = "Accuracy",
                            do_rfe                  = FALSE,
                            rfe_folds               = 5,
                            do_impute_norm_screen   = TRUE,
                            group_for_significance) {
  

  # 1) Prepare Target Variable and Remove Classes with a Single Sample
  data[[type_col]] <- as.factor(data[[type_col]])
  levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique = TRUE)
  
  class_counts  <- table(data[[type_col]])
  keep_classes  <- names(class_counts[class_counts > 1])
  data          <- data[data[[type_col]] %in% keep_classes, ]
  data[[type_col]] <- droplevels(data[[type_col]])
  rownames(data) <- NULL
  data <- data %>% 
    mutate(sample_id = paste0(Plastic_type, "_", technique)) %>% 
    mutate(sample_id = ave(sample_id, sample_id, FUN = function(x) {
      if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
    })) %>% 
    tibble::column_to_rownames(., var = "sample_id")
  
  # 3) Imputation and Normalization on Clean Data
  best_imp  <- "No_imputation"
  best_norm <- "No_normalization"
  if(do_impute_norm_screen){
    start_time_imp_norm <- Sys.time()
    cat("\n### Screening Best Imputation + Normalization Combo ###\n")
    
    # Identify numeric columns (excluding remove_cols and target)
    numeric_cols <- names(which(sapply(data, is.numeric)))
    numeric_cols <- setdiff(numeric_cols, c(remove_cols, type_col))
    
    best_res_imp_norm <- find_best_impute_normalize(
      df = data,
      type_col = type_col,
      group_for_significance = group_for_significance,
      remove_cols = remove_cols
    )
    cat("\n*** Summary of Imputation/Normalization Combos ***\n")
    print(best_res_imp_norm$results_table)
    cat("\n*** Best Combo ***\n")
    print(best_res_imp_norm$best_combo)
    
    best_imp  <- best_res_imp_norm$best_combo$Imputation
    best_norm <- best_res_imp_norm$best_combo$Normalization
    
    X_original <- data[, numeric_cols, drop = FALSE]
    X_imputed <- imputation_methods[[best_imp]](as.data.frame(X_original))
    X_final <- normalization_methods[[best_norm]](X_imputed)
    data[, numeric_cols] <- X_final
    
    end_time_imp_norm   <- Sys.time()
    time_imp_norm      <- as.numeric(difftime(end_time_imp_norm,
                                              start_time_imp_norm, 
                                              units="secs"))
  } else {
    cat("\n### Skipping Imputation + Normalization; Using Raw Data ###\n")
  }
  
  cat("NAs in original data:", sum(is.na(data)), "\n")
  cat("NAs after imputation+normalization:", sum(is.na(data)), "\n")
  
  # 4) Run Pairwise/ANOVA Tests on Cleaned Data (if selected)
  if(do_pairwise_test) {
    cat("\n### Running Pairwise Significance Tests on Cleaned Data ###\n")
    pairwise_res <- pairwise_significance_tests(input_df = data, 
                                                group_col = group_for_significance, 
                                                start_col_index = 6)
    pairwise_sig_features <- pairwise_res$corrected
    if(length(pairwise_sig_features) == 0) {
      cat("No significant features found from pairwise tests. Skipping pairwise feature selection.\n")
      do_pairwise_test <- FALSE
    }
  }
  
  # 5) Train/Test Split
  if(use_store_vs_environmental_split){
    cat("\n### Using Store-vs-Environmental Splitting ###\n")
    
    train_data <- data[data$Source == "Store-Bought", ]
    test_data  <- data[data$Source == "Environmental", ]
    
    if(nrow(train_data) < 1) stop("No 'Store-Bought' samples for training!")
    if(nrow(test_data) < 1) stop("No 'Environmental' samples for testing!")
    
  } else if(use_source_split){
    cat("\n### Using Source-based Splitting with Environmental 50:50 Split ###\n")
    
    store_bought_set <- data[data$Source == "Store-Bought", ]
    environmental_set <- data[data$Source == "Environmental", ]
    
    if(!is.null(seed)) {
      set.seed(seed)
    } else {
      set.seed(sample.int(999, 1))
    }
    env_train_index <- caret::createDataPartition(environmental_set[[type_col]], p = 0.5, list = FALSE)
    environmental_training_set <- environmental_set[env_train_index, ]
    environmental_hold_out_testing_set <- environmental_set[-env_train_index, ]
    
    train_data <- rbind(store_bought_set, environmental_training_set)
    test_data  <- environmental_hold_out_testing_set
    
    if(nrow(train_data) < 1) stop("Training data is empty!")
    if(nrow(test_data) < 1) stop("Test data is empty!")
  } else {
    cat("\n### Using Original Train/Test Split ###\n")
    # X <- data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
    y <- data[[type_col]]
    if(is.null(seed)) {
      set.seed(123) # set.seed(sample.int(999, 1))
    }
    train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
    train_data <- data[train_index, ]
    test_data  <- data[-train_index, ]
  }
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # 7) Setup Cross-Validation Control
  min_class_count <- min(table(y_train))
  folds <- ifelse(min_class_count > 1, min_class_count, 2)
  cv_ctrl <- caret::trainControl(method = "cv", number = folds, classProbs = TRUE)
  
  # 8) Full-Feature Classification: Grid Search over (mtry, ntree)
  cat("\n### Train/test with Full original features ###\n")
  start_time_full_features <- Sys.time()
  
  all_feats <- tune_rf_subset(
    X_train, y_train, X_test, y_test,
    cv_ctrl, ntree_candidates, metric
  )
  
  best_model_all_feats      <- all_feats$model
  best_trees_all_feats      <- all_feats$ntree
  acc_full        <- all_feats$Accuracy
  
  # --- Generate Heatmap (Full Features) ---
  {prob_matrix_all_feats <- all_feats$p_m
   conf_mat_all_feats <- conf_mat_plot(y_test, all_feats$preds, conf_mat_title = paste0("All Features - ", data_name), accuracy = acc_full)
   # print(conf_mat_all_feats)
    # pheatmap(
    #   prob_matrix_all_feats,
    #   fontsize = 20,
    #   fontsize_number = 30,
    #   cluster_rows    = FALSE,
    #   cluster_cols    = FALSE,
    #   color           = viridis(100),
    #   display_numbers = TRUE,
    #   number_format   = "%.2f",
    #   main = paste("Classification Probability (All Features) -", data_name, sprintf("| Test Accuracy: %.2f", acc_full)),
    #   breaks = seq(0, 1, length.out = 101)
    # )
  }
  
  eval_metrics_all_feats <- all_feats$eval_metrics
    
  end_time_full_features   <- Sys.time()
  time_all_feats       <- as.numeric(difftime(end_time_full_features,
                                              start_time_full_features, 
                                              units="secs"))
  
  # 9) Classification with Pairwise/ANOVA Selected Features (if requested)
  cat("\n### Train/test with Pair-wise features ###\n")
  acc_sig <- NA; sig_feats <- NULL; final_sig_model <- NULL
  
  if(do_pairwise_test){
    start_time_sig <- Sys.time()

    feats0<- pairwise_sig_features
    
    sig_feats <- intersect(feats0, colnames(X_train))
    
    if(length(sig_feats)>0) {
      
      tmp_sig <- tune_rf_subset(
        X_train[, sig_feats, drop=FALSE], y_train,
        X_test [, sig_feats, drop=FALSE], y_test,
        cv_ctrl, ntree_candidates, metric
      )
      acc_sig         <- tmp_sig$Accuracy
      final_sig_model <- tmp_sig$model$finalModel
    }
    
    {prob_matrix_sig <- tmp_sig$p_m
      conf_mat_sig <- conf_mat_plot(y_test, tmp_sig$preds, conf_mat_title = paste0("Pair-wise Significance-Based Features - ", data_name), accuracy = acc_sig)
      # pheatmap(
      #   prob_matrix_sig,
      #   fontsize = 20,
      #   fontsize_number = 30,
      #   cluster_rows    = FALSE,
      #   cluster_cols    = FALSE,
      #   color           = viridis(100),
      #   display_numbers = TRUE,
      #   number_format   = "%.2f",
      #   main = paste("Classification Probability (Pair-wise Significance-Based Features) -", data_name, sprintf("| Test Accuracy: %.2f", acc_sig)),
      #   breaks = seq(0, 1, length.out = 101)
      # )
    }
    
    eval_metrics_sig <- tmp_sig$eval_metrics
    
    end_time_sig   <- Sys.time()
    time_sig       <- as.numeric(difftime(end_time_sig, start_time_sig, units="secs"))
  }
  
  # 10) Classification with Top-N Features by Importance (if requested)
  if(do_top_importance_selection){
    start_time_top_100_50_25_10 <- base::Sys.time()
    cat("\n### Running Classification on Top-N Features by Importance ###\n")
    
    rf_importances <- as.data.frame(best_model_all_feats$finalModel$importance)
    rf_importances$Feature <- rownames(rf_importances)
    rf_importances <- rf_importances[order(rf_importances$MeanDecreaseGini, decreasing = TRUE), ]

    top_importance_results <- list()
    total_features <- nrow(rf_importances)
    
    for(top_n in top_importance_counts){
      if(total_features < top_n){
        selected_features <- rf_importances$Feature
        cat(sprintf("Only %d features available; using all.\n", total_features))
      } else {
        selected_features <- head(rf_importances$Feature, top_n)
      }
      
      tmp_top <- tune_rf_subset(
        X_train[, selected_features, drop = FALSE], y_train,
        X_test[, selected_features, drop = FALSE], y_test,
        cv_ctrl, ntree_candidates, metric
      )
      
      acc_top         <- tmp_top$Accuracy
      final_rf_top <- tmp_top$model$finalModel

      prob_matrix_top <- tmp_top$p_m
      conf_mat_top <- conf_mat_plot(y_test, tmp_top$preds, conf_mat_title = paste0("Top ", length(selected_features), " Features - ", data_name), accuracy = acc_top)
      eval_metrics_top <- tmp_top$eval_metrics
      
      # {pheatmap::pheatmap(prob_matrix_top,
      #                     fontsize = 20,
      #                     fontsize_number = 30,
      #                     cluster_rows = FALSE,
      #                     cluster_cols = FALSE,
      #                     color = viridis::viridis(100),
      #                     display_numbers = TRUE,
      #                     number_format = "%.2f",
      #                     main = paste("Classification Probability (Top", length(selected_features), "Features) -", data_name, sprintf("| Test Accuracy: %.2f", acc_top)),
      #                     breaks = seq(0, 1, length.out = 101))
      # }
      
      top_importance_results[[paste0("top_", top_n)]] <- list(
        best_model = final_rf_top,
        selected_features = selected_features,
        prob_matrix_top = prob_matrix_top,
        conf_mat_top=conf_mat_top,
        eval_metrics_top = eval_metrics_top
      )
    }
    end_time_top_100_50_25_10   <- Sys.time()
    time_top_100_50_25_10   <- as.numeric(difftime(end_time_top_100_50_25_10,
                                                   start_time_top_100_50_25_10, 
                                                   units="secs"))
  }
  
  
  
  # 11) (Optional) RFE with Fixed Hyperparameters --------------
  cat("\n### Train/test with RFE features ###\n")
  acc_rfe <- NA; rfe_feats <- NULL; final_rfe_model <- NULL
  
  if(do_rfe){
    start_time_rfe <- Sys.time()
    
    myFuncs <- rfFuncs
    myFuncs$fit <- function(x, y, first, last, ...) {
      res <- tune_rf_subset(
        X_train = x, y_train = y,
        X_test  = x, y_test  = y,
        cv_ctrl, ntree_candidates, metric
      )
      res$model$finalModel
    }
    rfe_ctl <- rfeControl(
      functions   = myFuncs,
      method      = "cv",
      number      = rfe_folds,
      saveDetails = TRUE,
      returnResamp= "final"
    )
    rfe_out <- rfe(
      x          = X_train,
      y          = y_train,
      sizes      = seq_len(ncol(X_train)),
      rfeControl = rfe_ctl
    )
    rfe_feats <- rfe_out$optVariables
    
    # final retune+test on RFE set
    tmp_rfe <- tune_rf_subset(
      X_train[, rfe_feats, drop=FALSE], y_train,
      X_test[, rfe_feats, drop=FALSE], y_test,
      cv_ctrl, ntree_candidates, metric
    )
    acc_rfe          <- tmp_rfe$Accuracy
    final_rfe_model  <- tmp_rfe$model$finalModel

    # --- Generate Heatmap (RFE) ---
    {prob_matrix_rfe <- tmp_rfe$p_m
      conf_mat_rfe <-  conf_mat_plot(y_test, tmp_rfe$preds, conf_mat_title = paste0("RFE Features - ", data_name), accuracy = acc_rfe)
      # pheatmap(
      #   prob_matrix_rfe,
      #   fontsize = 20,
      #   fontsize_number = 30,
      #   cluster_rows    = FALSE,
      #   cluster_cols    = FALSE,
      #   color           = viridis(100),
      #   display_numbers = TRUE,
      #   number_format   = "%.2f",
      #   main = paste("Classification Probability (RFE Features) -", data_name, sprintf("|  Test Accuracy: %.2f", acc_rfe)),
      #   breaks = seq(0, 1, length.out = 101)
      # )
      
    }
  
    eval_metrics_rfe <- tmp_rfe$eval_metrics
    
    end_time_rfe   <- Sys.time()
    time_rfe       <- as.numeric(difftime(end_time_rfe, start_time_rfe, units="secs"))
  }
  
  # Return a list of final results
  result_list <- list(
    final_imp_norm_dat    = data,
    best_model_all_feats      = best_model_all_feats,
    final_rf_all_feats    = best_model_all_feats$finalModel,
    prob_matrix = prob_matrix_all_feats,
    all_features_acc     = acc_full,
    time_all_features = time_all_feats,
    eval_metrics_all_feats = eval_metrics_all_feats,
    conf_mat_all_feats = conf_mat_all_feats
  )
  
  if(do_impute_norm_screen) {
    result_list$imp_norm_res_table <- best_res_imp_norm$results_table
    result_list$time_imp_norm <- time_imp_norm
    result_list$best_imputatation     <- best_imp
    result_list$best_normalization    <- best_norm
  }
  
  if(do_pairwise_test){
    result_list$sig_model    <- final_sig_model
    result_list$sig_selected_feats <- sig_feats
    result_list$prob_matrix_sig <- prob_matrix_sig
    result_list$acc_sig <- acc_sig
    result_list$time_sig          <- time_sig
    result_list$eval_metrics_sig <- eval_metrics_sig
    result_list$conf_mat_sig <- conf_mat_sig
    
  }

  if(do_rfe) {
    result_list$rfe_model             <- rfe_out
    result_list$final_rf_rfe          <- final_rfe_model
    result_list$rfe_selected_features <- rfe_feats
    result_list$prob_matrix_rfe    <- prob_matrix_rfe
    result_list$acc_rfe  <- acc_rfe
    result_list$time_rfe              <- time_rfe
    result_list$eval_metrics_rfe <- eval_metrics_rfe
    result_list$conf_mat_rfe <- conf_mat_rfe
  }
  
  if(do_top_importance_selection) {
    result_list$top_importance_results <- top_importance_results
    result_list$time_top_100_50_25_10 <-  time_top_100_50_25_10
  }
  
  return(result_list)
}
```

### Option 1: Use SB to predict ENV - Only select shared features between SB and ENV 

[UPDATE 27th January, 2025 ]--> doesn't result in good prediction !!
[UPDATE 24th Feb, 2025] -> Use USSB and USE as training and testing on USE
[UPDATE 11th Mar, 2025]:
1) The option of finding shared features between SB and ENV prior to data cleaning/filtering went well for GC only data.

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
###################################################################################
## Explanation of the parameter of run_rf_analysis()
# split environmental into 50:50 and combine with store_bought -> use_source_split = FALSE
# use store-bought as training and environmental as testing -> use_store_vs_environmental_split = TRUE
###################################################################################

data_combinations <- list(
  gc                 = gc_combined_SB_ENV_shared_cols_ntr_clean,
  hplc               = hplc_combined_SB_ENV_shared_cols_ntr_clean,
  icp                = icp_combined_SB_ENV_shared_cols_ntr_clean ,
  gc_hplc            = gc_hplc_combined_SB_ENV_shared_cols_ntr_clean,
  gc_icp             = gc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  hplc_icp           = hplc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  gc_hplc_icp        = gc_hplc_icp_combined_SB_ENV_shared_cols_ntr_clean
)

# Predefine the result list with names from data_combinations
run_rf_analysis_list_SB_train <- vector("list", length(data_combinations))
names(run_rf_analysis_list_SB_train) <- c("gc_USSB_train",
                                          "hplc_USSB_train",
                                          "icp_USSB_train",
                                          "gc_hplc_USSB_train",
                                          "gc_icp_USSB_train",
                                          "hplc_icp_USSB_train",
                                          "gc_hplc_icp_USSB_train")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]] <- run_rf_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_store_vs_environmental_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    group_for_significance = "Plastic_type", 
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_rfe = FALSE,
    do_impute_norm_screen = FALSE,
    seed = 123)
  
  excel_list <- list(
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$prob_matrix),
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$prob_matrix_pair),
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$top_importance_results$top_100$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$top_importance_results$top_50$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$top_importance_results$top_25$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$top_importance_results$top_10$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$prob_matrix_opt),
    as.data.frame(run_rf_analysis_list_SB_train[[names(run_rf_analysis_list_SB_train)[i]]]$prob_matrix_gbfe))
  write_xlsx(excel_list, path = paste0(names(run_rf_analysis_list_SB_train)[i], "_all_prob_matrices.xlsx"))
}
```

### Scatter plot features importance -------
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# use MeanDecreaseAccuracy as metric for demonstrating feature importances -> removing that feature will decrease the accuracy by a certain value 
rf_importances <- as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[3]]$final_rf_all_feats$importance)
rf_importances$Feature <- rownames(rf_importances)
rf_importances <- rf_importances[order(rf_importances$MeanDecreaseAccuracy, decreasing = TRUE), ]

feat_imp_plot <- ggplot(rf_importances, aes(x = reorder(Feature,
                                       MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_point(size = 1) +
  labs(x = "Feature", y = "Mean Decrease Accuracy") +
  theme_classic(base_size = 25) +
  theme(axis.text.x = element_text(size= 15, angle =90, hjust = 1, vjust = 0.5))

ggsave(filename = paste0("rf_feat_imp.png"), 
       plot = feat_imp_plot, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 15,       # Width in inches (adjust as needed)
       height = 10,       # Height in inches (adjust as needed)
       units = "in")
```

### Option 2: Use Both SB and ENV to predict ENV
Run prediction separatedly on each analytical dataset and only use significant features from GBFE and pairwise test asinput for combined data

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_combined_SB_ENV_shared_cols_ntr_clean,
  hplc               = hplc_combined_SB_ENV_shared_cols_ntr_clean,
  icp                = icp_combined_SB_ENV_shared_cols_ntr_clean,
  gc_hplc            = gc_hplc_combined_SB_ENV_shared_cols_ntr_clean,
  gc_icp             = gc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  hplc_icp           = hplc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  gc_hplc_icp        = gc_hplc_icp_combined_SB_ENV_shared_cols_ntr_clean
)

# Predefine the result list with names from data_combinations
run_rf_analysis_list_SB_and_ENV_train <- vector("list", length(data_combinations))
names(run_rf_analysis_list_SB_and_ENV_train) <- c("gc_SB_and_ENV_train",
                                                  "hplc_SB_and_ENV_train",
                                                  "icp_SB_and_ENV_train",
                                                  "gc_hplc_SB_and_ENV_train",
                                                  "gc_icp_SB_and_ENV_train",
                                                  "hplc_icp_SB_and_ENV_train",
                                                  "gc_hplc_icp_SB_and_ENV_train")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]] <- run_rf_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_source_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    group_for_significance = "Plastic_type",
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_rfe = FALSE,
    train_proportion = 0.8,
    do_impute_norm_screen = FALSE,
    seed = 123)
  
   excel_list <- list(
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$prob_matrix),
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$prob_matrix_pair),
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$top_importance_results$top_100$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$top_importance_results$top_50$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$top_importance_results$top_25$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$top_importance_results$top_10$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$prob_matrix_opt),
    as.data.frame(run_rf_analysis_list_SB_and_ENV_train[[names(run_rf_analysis_list_SB_and_ENV_train)[i]]]$prob_matrix_gbfe))
  write_xlsx(excel_list, path = paste0(names(run_rf_analysis_list_SB_and_ENV_train)[i], "_all_prob_matrices.xlsx"))
}
```

### Option 3: Use SB to predict SB

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_SB_clean,
  hplc               = hplc_SB_clean,
  icp                = icp_SB_clean ,
  gc_hplc            = gc_hplc_SB_clean,
  gc_icp             = gc_icp_SB_clean,
  hplc_icp           = hplc_icp_SB_clean,
  gc_hplc_icp        = gc_hplc_icp_SB_clean
)

# Predefine the result list with names from data_combinations
run_rf_analysis_list_SB_only <- vector("list", length(data_combinations))
names(run_rf_analysis_list_SB_only) <- c("gc_SB",
                                         "hplc_SB",
                                         "icp_SB",
                                         "gc_hplc_SB",
                                         "gc_icp_SB",
                                         "hplc_icp_SB",
                                         "gc_hplc_icp_SB")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]] <- run_rf_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_store_vs_environmental_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    group_for_significance = "Plastic_type", 
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_rfe = FALSE,
    do_impute_norm_screen = FALSE,
    seed = 123)
  
  excel_list <- list(
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$prob_matrix),
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$prob_matrix_pair),
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$top_importance_results$top_100$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$top_importance_results$top_50$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$top_importance_results$top_25$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$top_importance_results$top_10$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$prob_matrix_opt),
    as.data.frame(run_rf_analysis_list_SB_only[[names(run_rf_analysis_list_SB_only)[i]]]$prob_matrix_gbfe))
  write_xlsx(excel_list, path = paste0(names(run_rf_analysis_list_SB_only)[i], "_all_prob_matrices.xlsx"))
}
```

### Option 4: Use ENV to predict ENV

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_ENV_clean,
  hplc               = hplc_ENV_clean,
  icp                = icp_ENV_clean ,
  gc_hplc            = gc_hplc_ENV_clean,
  gc_icp             = gc_icp_ENV_clean,
  hplc_icp           = hplc_icp_ENV_clean,
  gc_hplc_icp        = gc_hplc_icp_ENV_clean
)

# Predefine the result list with names from data_combinations
run_rf_analysis_list_ENV_only <- vector("list", length(data_combinations))
names(run_rf_analysis_list_ENV_only) <- c("gc_ENV",
                                          "hplc_ENV",
                                          "icp_ENV",
                                          "gc_hplc_ENV",
                                          "gc_icp_ENV",
                                          "hplc_icp_ENV",
                                          "gc_hplc_icp_ENV")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]] <- run_rf_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_store_vs_environmental_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    group_for_significance = "Plastic_type", 
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_rfe = FALSE,
    do_impute_norm_screen = FALSE,
    seed = 123)
  
  excel_list <- list(
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$prob_matrix),
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$prob_matrix_pair),
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$top_importance_results$top_100$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$top_importance_results$top_50$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$top_importance_results$top_25$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$top_importance_results$top_10$prob_matrix_top),
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$prob_matrix_opt),
    as.data.frame(run_rf_analysis_list_ENV_only[[names(run_rf_analysis_list_ENV_only)[i]]]$prob_matrix_gbfe))
  write_xlsx(excel_list, path = paste0(names(run_rf_analysis_list_ENV_only)[i], "_all_prob_matrices.xlsx"))
}
```

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
#### Simplify the prediction heatmap
# prob_df <- as.data.frame(kayla_trainPristine_testTreated$prob_matrix) %>% rownames_to_column(., var="label")
prob_df <- as.data.frame(run_rf_analysis_list_ENV_only$hplc_ENV$prob_matrix) %>% 
  dplyr::mutate(Plastic_type = gsub(".", " ", as.character(Plastic_type), fixed = TRUE)) %>%
  rownames_to_column(., var="label") # as.data.frame(kayla_trainPristine_testTreated_7plastictypes$prob_matrix)

prob_df <- prob_df %>% mutate(label = gsub("_rep1", "", label))
prob_df <- prob_df %>% mutate(label = gsub("_rep2", "", label))
prob_df <- prob_df %>% mutate(label = gsub("_rep3", "", label))
prob_df <- prob_df %>% mutate(label = gsub("_rep4", "", label))
prob_df <- prob_df %>% mutate(label = gsub("_rep5", "", label))
prob_df_melted <- reshape2::melt(prob_df , id.vars = "label")

prob_df_mean <- prob_df_melted %>%
  dplyr::group_by(label, variable) %>%
  dplyr::summarise(mean_value = mean(value)) 

heatmap <- ggplot(prob_df_mean, aes(x = variable, y = label, fill = mean_value)) +
  geom_tile() +
  geom_text(aes(label = round(mean_value, 2)), color = "darkgray") +
  scale_fill_viridis_c(option = "viridis", limits = c(0, 1), breaks = c(0, 0.5, 1), name = "Prediction Probability") +
  theme_minimal() +
  labs(x = "Library Pristine", y = "Sample Treatment") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0, color = "black"), #, face = "bold"),
        axis.ticks.length.x = unit(0, "cm")) 

ggsave(filename = paste0("Kayla_heatmap_trainlibrary_test4Treatments_7plastictypes_roxana recommendation_simplified.png"), 
       plot = heatmap, 
       device = "png",
       dpi = 300,        # Set DPI to 600
       width = 20,       # Width in inches (adjust as needed)
       height = 10,       # Height in inches (adjust as needed)
       units = "in")
```

# SVM
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
run_linearSVM_analysis_manuscript1_e1071 <- function(data,
                                                      type_col,
                                                      remove_cols,
                                                      train_proportion        = 0.5,
                                                      use_source_split        = FALSE,
                                                      use_store_vs_environmental_split = FALSE,
                                                      use_polymer             = FALSE,
                                                      do_pairwise_test        = FALSE,
                                                      do_top_importance_selection = FALSE,
                                                      top_importance_counts   = c(100, 50, 25, 10),
                                                      data_name,              
                                                      seed                    = NULL,
                                                      svm_candidates          = c(0.01, 0.1, 1, 10, 100),
                                                      metric                  = "Accuracy",  # Note: tune.svm minimizes error
                                                      do_rfe                  = FALSE,
                                                      rfe_folds               = 5,
                                                      do_gbfe                 = FALSE,
                                                      do_impute_norm_screen   = TRUE,
                                                      group_for_significance  = "Subcategory") {
  
  library(dplyr)
  library(e1071)
  library(pheatmap)
  library(viridis)
  
  # 1) Prepare Target Variable and Remove Classes with a Single Sample
  if(!is.null(seed)) set.seed(seed)
  data[[type_col]] <- as.factor(data[[type_col]])
  levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique = TRUE)
  
  class_counts  <- table(data[[type_col]])
  keep_classes  <- names(class_counts[class_counts > 1])
  data          <- data[data[[type_col]] %in% keep_classes, ]
  data[[type_col]] <- droplevels(data[[type_col]])
  
  # 2) Handle Polymer Column if Required
  if(use_polymer) {
    remove_cols <- setdiff(remove_cols, "Polymer")
    if("Polymer" %in% colnames(data)){
      data$Polymer <- as.factor(data$Polymer)
    }
  }
  
  # 3) Imputation and Normalization on Clean Data
  best_imp  <- "No_imputation"
  best_norm <- "No_normalization"
  if(do_impute_norm_screen){
    cat("\n### Screening Best Imputation + Normalization Combo ###\n")
    
    # (Assuming find_best_impute_normalize and associated methods are defined)
    best_res <- find_best_impute_normalize(
      df = data,
      class_col = type_col,
      group_for_significance = group_for_significance,
      remove_cols = remove_cols
    )
    cat("\n*** Summary of Imputation/Normalization Combos ***\n")
    print(best_res$results_table)
    cat("\n*** Best Combo ***\n")
    print(best_res$best_combo)
    
    best_imp  <- best_res$best_combo$Imputation
    best_norm <- best_res$best_combo$Normalization
    
    numeric_cols <- names(which(sapply(data, is.numeric)))
    X_original <- data[, numeric_cols, drop = FALSE]
    X_imputed <- imputation_methods[[best_imp]](X_original)
    X_final <- normalization_methods[[best_norm]](X_imputed)
    data[, numeric_cols] <- X_final
  } else {
    cat("\n### Skipping Imputation + Normalization; Using Raw Data ###\n")
  }
  
  cat("NAs in original data:", sum(is.na(data)), "\n")
  cat("NAs after imputation+normalization:", sum(is.na(data)), "\n")
  
  # 4) Run Pairwise/ANOVA Tests on Cleaned Data (if selected)
  if(do_pairwise_test) {
    cat("\n### Running Pairwise Significance Tests on Cleaned Data ###\n")
    pairwise_res <- pairwise_significance_tests(input_df = data, 
                                                group_col = group_for_significance, 
                                                start_col_index = 6)
    pairwise_sig_features <- pairwise_res$corrected
    if(length(pairwise_sig_features) == 0) {
      cat("No significant features found from pairwise tests. Skipping pairwise feature selection.\n")
      do_pairwise_test <- FALSE
    }
  }
  
  # 5) Train/Test Split
  if(use_store_vs_environmental_split){
    cat("\n### Using Store-vs-Environmental Splitting ###\n")
    train_data <- data[data$Source == "Store-Bought", ]
    test_data  <- data[data$Source == "Environmental", ]
    if(nrow(train_data) < 1) stop("No 'Store-Bought' samples for training!")
    if(nrow(test_data) < 1) stop("No 'Environmental' samples for testing!")
  } else if(use_source_split){
    cat("\n### Using Source-based Splitting with Environmental 50:50 Split ###\n")
    store_bought_set <- data[data$Source == "Store-Bought", ]
    environmental_set <- data[data$Source == "Environmental", ]
    if(!is.null(seed)) {
      set.seed(seed)
    } else {
      set.seed(sample.int(999, 1))
    }
    env_train_index <- caret::createDataPartition(environmental_set[[type_col]], p = 0.5, list = FALSE)
    environmental_training_set <- environmental_set[env_train_index, ]
    environmental_hold_out_testing_set <- environmental_set[-env_train_index, ]
    train_data <- rbind(store_bought_set, environmental_training_set)
    test_data  <- environmental_hold_out_testing_set
    if(nrow(train_data) < 1) stop("Training data is empty!")
    if(nrow(test_data) < 1) stop("Test data is empty!")
  } else {
    cat("\n### Using Original Train/Test Split ###\n")
    y <- data[[type_col]]
    if(is.null(seed)) {
      set.seed(sample.int(999, 1))
    }
    train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
    train_data <- data[train_index, ]
    test_data  <- data[-train_index, ]
  }
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # 7) Setup Cross-Validation Fold Count
  min_class_count <- min(table(y_train))
  folds <- ifelse(min_class_count > 1, min_class_count, 2)
  
  # 8) Full-Feature Classification using e1071 -------------------------------
  cat("\n### Training Full-Feature Linear SVM Model using e1071 ###\n")
  
  # For e1071, it is important to scale predictors manually.
  scaled_train <- scale(X_train)
  X_train_scaled <- as.data.frame(scaled_train)
  center_vec <- attr(scaled_train, "scaled:center")
  scale_vec <- attr(scaled_train, "scaled:scale")
  
  # Combine predictors with response for formula-based training
  train_df <- cbind(X_train_scaled, target = y_train)
  
  # Use tune.svm for grid search over cost (note: tune minimizes error)
  tune_result <- tune(svm, target ~ ., data = train_df,
                      kernel = "linear",
                      cost = svm_candidates,
                      tunecontrol = tune.control(sampling = "cross", cross = folds),
                      probability = TRUE)
  
  best_model <- tune_result$best.model
  best_C <- best_model$cost
  best_cv_accuracy <- 1 - min(tune_result$performances$error)
  
  cat("Best Full-Feature Model CV Accuracy (approx) =", best_cv_accuracy,
      " with C =", best_C, "\n")
  
  # Scale test predictors using same scaling parameters
  X_test_scaled <- as.data.frame(scale(X_test, center = center_vec, scale = scale_vec))
  
  # Predict on test set and extract probabilities
  pred <- predict(best_model, newdata = X_test_scaled, probability = TRUE)
  pred_probs <- attr(pred, "probabilities")
  pred_class <- as.factor(pred)
  
  y_test_aligned <- factor(as.character(y_test), levels = levels(y_train))
  acc_full <- mean(pred_class == y_test_aligned, na.rm = TRUE)
  
  cat("Full-Feature Test Accuracy:", acc_full, "\n")
  
  # --- Generate Heatmap (Full Features) ---
  {
    # Create a data frame with predicted probabilities and true labels
    new_pred_df_test <- as.data.frame(pred_probs)
    new_pred_df_test[[type_col]] <- y_test
    row_names <- as.character(new_pred_df_test[[type_col]])
    new_row_names <- ave(row_names, row_names, FUN = function(x) {
      if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
    })
    rownames(new_pred_df_test) <- new_row_names
    new_pred_df_test[[type_col]] <- NULL
    prob_matrix <- as.matrix(new_pred_df_test)
    
    pheatmap::pheatmap(prob_matrix,
                       fontsize = 15,
                       cluster_rows = FALSE,
                       cluster_cols = FALSE,
                       color = viridis::viridis(100),
                       display_numbers = TRUE,
                       number_format = "%.2f",
                       main = paste("Classification Probability (All Features) -", data_name, 
                                    sprintf("| Test Accuracy: %.2f", acc_full)),
                       breaks = seq(0, 1, length.out = 101))
  }
  
  # --- Feature Importance ---
  # For a linear SVM with 2 classes, weights can be approximated as:
  if(length(levels(y_train)) == 2) {
    w <- t(best_model$coefs) %*% best_model$SV
    w <- as.vector(w)
    names(w) <- colnames(X_train)
    svm_imp_df <- data.frame(Feature = names(w), Overall = abs(w))
    svm_imp_df <- svm_imp_df[order(svm_imp_df$Overall, decreasing = TRUE), ]
    cat("\nTop Features by Importance (Full Features):\n")
    print(head(svm_imp_df))
  } else {
    cat("\nVariable importance extraction is not straightforward for multi-class SVM in e1071.\n")
    svm_imp_df <- NULL
  }
  
  # 9) Classification with Pairwise/ANOVA Selected Features (if requested) -------
  if(do_pairwise_test){
    pairwise_features <- intersect(colnames(X_train), pairwise_sig_features)
    
    if(length(pairwise_features) == 0){
      cat("No overlapping pairwise significant features found in predictors. Skipping pairwise model.\n")
    } else {
      X_train_pair <- X_train[, pairwise_features, drop = FALSE]
      X_test_pair  <- X_test[, pairwise_features, drop = FALSE]
      
      cat("\n### Running Classification on Pairwise Selected Features ###\n")
      cat("Using Pairwise Features:", paste(pairwise_features, collapse = ", "), "\n")
      
      # Scale pairwise data
      scaled_train_pair <- scale(X_train_pair)
      X_train_pair_scaled <- as.data.frame(scaled_train_pair)
      center_pair <- attr(scaled_train_pair, "scaled:center")
      scale_pair  <- attr(scaled_train_pair, "scaled:scale")
      
      train_pair_df <- cbind(X_train_pair_scaled, target = y_train)
      
      # Use the best C from full-feature model
      svm_pair_model <- svm(target ~ ., data = train_pair_df, kernel = "linear", cost = best_C, probability = TRUE)
      
      X_test_pair_scaled <- as.data.frame(scale(X_test_pair, center = center_pair, scale = scale_pair))
      pred_pair <- predict(svm_pair_model, newdata = X_test_pair_scaled, probability = TRUE)
      pred_probs_pair <- attr(pred_pair, "probabilities")
      pred_class_pair <- as.factor(pred_pair)
      
      acc_pair <- mean(pred_class_pair == y_test_aligned, na.rm=TRUE)
      cat("Pairwise Model Test Accuracy:", acc_pair, "\n")
      
      {
        df_heatmap_pair <- as.data.frame(pred_probs_pair)
        df_heatmap_pair[[type_col]] <- y_test
        row_names_pair <- as.character(df_heatmap_pair[[type_col]])
        new_row_names_pair <- ave(row_names_pair, row_names_pair, FUN = function(x) {
          if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
        })
        rownames(df_heatmap_pair) <- new_row_names_pair
        df_heatmap_pair[[type_col]] <- NULL
        prob_matrix_pair <- as.matrix(df_heatmap_pair)
        
        pheatmap::pheatmap(prob_matrix_pair,
                           fontsize = 15,
                           cluster_rows = FALSE,
                           cluster_cols = FALSE,
                           color = viridis::viridis(100),
                           display_numbers = TRUE,
                           number_format = "%.2f",
                           main = paste("Classification Probability (Pairwise Features) -", data_name, 
                                        sprintf("| Test Accuracy: %.2f", acc_pair)),
                           breaks = seq(0, 1, length.out = 101))
      }
    }
  }
  
  # 10) Classification with Top-N Features by Importance (if requested) -------
  if(do_top_importance_selection){
    cat("\n### Running Classification on Top-N Features by Importance ###\n")
    top_importance_results <- list()
    total_features <- if(!is.null(svm_imp_df)) nrow(svm_imp_df) else ncol(X_train)
    
    for(top_n in top_importance_counts){
      if(total_features < top_n){
        selected_features <- if(!is.null(svm_imp_df)) svm_imp_df$Feature else colnames(X_train)
        cat(sprintf("Only %d features available; using all.\n", total_features))
      } else {
        selected_features <- if(!is.null(svm_imp_df)) head(svm_imp_df$Feature, top_n) else head(colnames(X_train), top_n)
      }
      
      X_train_top <- X_train[, selected_features, drop = FALSE]
      X_test_top  <- X_test[, selected_features, drop = FALSE]
      
      # Scale top features
      scaled_train_top <- scale(X_train_top)
      X_train_top_scaled <- as.data.frame(scaled_train_top)
      center_top <- attr(scaled_train_top, "scaled:center")
      scale_top  <- attr(scaled_train_top, "scaled:scale")
      train_top_df <- cbind(X_train_top_scaled, target = y_train)
      
      svm_top_model <- svm(target ~ ., data = train_top_df, kernel = "linear", cost = best_C, probability = TRUE)
      
      X_test_top_scaled <- as.data.frame(scale(X_test_top, center = center_top, scale = scale_top))
      pred_top <- predict(svm_top_model, newdata = X_test_top_scaled, probability = TRUE)
      pred_probs_top <- attr(pred_top, "probabilities")
      pred_class_top <- as.factor(pred_top)
      
      acc_top <- mean(pred_class_top == y_test_aligned, na.rm = TRUE)
      cat(sprintf("Top-N (%d) Test Accuracy: %f\n", top_n, acc_top))
      
      {
        df_heatmap_top <- as.data.frame(pred_probs_top)
        df_heatmap_top[[type_col]] <- y_test
        row_names_top <- as.character(df_heatmap_top[[type_col]])
        new_row_names_top <- ave(row_names_top, row_names_top, FUN = function(x) {
          if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
        })
        rownames(df_heatmap_top) <- new_row_names_top
        df_heatmap_top[[type_col]] <- NULL
        prob_matrix_top <- as.matrix(df_heatmap_top)
        
        pheatmap::pheatmap(prob_matrix_top,
                           fontsize = 15,
                           cluster_rows = FALSE,
                           cluster_cols = FALSE,
                           color = viridis::viridis(100),
                           display_numbers = TRUE,
                           number_format = "%.2f",
                           main = paste("Classification Probability (Top", top_n, "Features) -", 
                                        data_name, sprintf("| Test Accuracy: %.2f", acc_top)),
                           breaks = seq(0, 1, length.out = 101))
      }
      
      top_importance_results[[paste0("top_", top_n)]] <- list(
        best_model = svm_top_model,
        best_cv_accuracy = NA,  # CV accuracy not re-computed here; using fixed best_C
        best_C = best_C,
        accuracy = acc_top,
        selected_features = selected_features
      )
    }
  }
  
  # 11) (Optional) RFE and 12) (Optional) GBFE -------------------------------
  # Implementing RFE or GBFE with e1071::svm would require writing a custom wrapper.
  # For brevity, these sections are not re-implemented here.
  
  # 13) Return Results
  result_list <- list(
    final_imp_norm_dat    = data,
    imp_norm_res_table    = if(exists("best_res")) best_res$results_table else NULL,
    best_imputatation     = best_imp,
    best_normalization    = best_norm,
    best_svm_model        = best_model,
    full_features_acc     = acc_full,
    svm_feature_importance = svm_imp_df
  )
  
  if(do_pairwise_test){
    result_list$pairwise_model    <- svm_pair_model
    result_list$pairwise_features <- pairwise_features
    result_list$pairwise_test_acc <- acc_pair
  }
  
  if(do_top_importance_selection){
    result_list$top_importance_results <- top_importance_results
  }
  
  return(result_list)
}

```

### Option 1: Use SB to predict ENV - Only select shared features between training and testing 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
###################################################################################
## Explanation of the parameter of run_rf_analysis()
# split environmental into 50:50 and combine with store_bought -> use_source_split = FALSE
# use store-bought as training and environmental as testing -> use_store_vs_environmental_split = TRUE
###################################################################################

data_combinations <- list(
  gc                 = gc_combined_SB_ENV_shared_cols_ntr_clean,
  hplc               = hplc_combined_SB_ENV_shared_cols_ntr_clean,
  icp                = icp_combined_SB_ENV_shared_cols_ntr_clean ,
  gc_hplc            = gc_hplc_combined_SB_ENV_shared_cols_ntr_clean,
  gc_icp             = gc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  hplc_icp           = hplc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  gc_hplc_icp        = gc_hplc_icp_combined_SB_ENV_shared_cols_ntr_clean
)

# Predefine the result list with names from data_combinations
run_svc_analysis_list_SB_train <- vector("list", length(data_combinations))
names(run_svc_analysis_list_SB_train) <- c("gc_USSB_train",
                                          "hplc_USSB_train",
                                          "icp_USSB_train",
                                          "gc_hplc_USSB_train",
                                          "gc_icp_USSB_train",
                                          "hplc_icp_USSB_train",
                                          "gc_hplc_icp_USSB_train")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_svc_analysis_list_SB_train[[names(run_svc_analysis_list_SB_train)[i]]] <- run_linearSVM_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_store_vs_environmental_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}


# Scatter plot features importance -------
# svc_importances <- as.data.frame(run_svc_analysis_list_SB_train$gc_USSB_train$final_svc_all_feats$importance)
# svc_importances$Feature <- rownames(svc_importances)
# svc_importances <- svc_importances[order(svc_importances$MeanDecreaseGini, decreasing = TRUE), ]
# 
# ggplot(svc_importances, aes(x = reorder(Feature, 
#                                        MeanDecreaseGini), y = MeanDecreaseGini)) +
#   geom_point() +
#   labs(x = "Feature", y = "Mean Decrease Gini",
#        title = "Scatter Plot: Features from Lowest to Highest Mean Decrease Gini") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Option 2: Use Both SB and ENV to predict ENV
Run prediction separatedly on each analytical dataset and only use significant features from GBFE and pairwise test asinput for combined data

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_combined_SB_ENV_shared_cols_ntr_clean,
  hplc               = hplc_combined_SB_ENV_shared_cols_ntr_clean,
  icp                = icp_combined_SB_ENV_shared_cols_ntr_clean ,
  gc_hplc            = gc_hplc_combined_SB_ENV_shared_cols_ntr_clean,
  gc_icp             = gc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  hplc_icp           = hplc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  gc_hplc_icp        = gc_hplc_icp_combined_SB_ENV_shared_cols_ntr_clean
)

# Predefine the result list with names from data_combinations
run_svc_analysis_list_SB_and_ENV_train <- vector("list", length(data_combinations))
names(run_svc_analysis_list_SB_and_ENV_train) <- c("gc_SB_and_ENV_train",
                                                  "hplc_SB_and_ENV_train",
                                                  "icp_SB_and_ENV_train",
                                                  "gc_hplc_SB_and_ENV_train",
                                                  "gc_icp_SB_and_ENV_train",
                                                  "hplc_icp_SB_and_ENV_train",
                                                  "gc_hplc_icp_SB_and_ENV_train")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_svc_analysis_list_SB_and_ENV_train[[names(run_svc_analysis_list_SB_and_ENV_train)[i]]] <- run_svm_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_source_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}
```

### Option 3: Use SB to predict SB

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_SB_clean,
  hplc               = hplc_SB_clean,
  icp                = icp_SB_clean ,
  gc_hplc            = gc_hplc_SB_clean,
  gc_icp             = gc_icp_SB_clean,
  hplc_icp           = hplc_icp_SB_clean,
  gc_hplc_icp        = gc_hplc_icp_SB_clean
)

# Predefine the result list with names from data_combinations
run_svc_analysis_list_SB_only <- vector("list", length(data_combinations))
names(run_svc_analysis_list_SB_only) <- c("gc_SB",
                                         "hplc_SB",
                                         "icp_SB",
                                         "gc_hplc_SB",
                                         "gc_icp_SB",
                                         "hplc_icp_SB",
                                         "gc_hplc_icp_SB")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_svc_analysis_list_SB_only[[names(run_svc_analysis_list_SB_only)[i]]] <- run_svm_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(run_svc_analysis_list_SB_only)[i], 
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}
```

### Option 4: Use ENV to predict ENV

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_ENV_clean,
  hplc               = hplc_ENV_clean,
  icp                = icp_ENV_clean ,
  gc_hplc            = gc_hplc_ENV_clean,
  gc_icp             = gc_icp_ENV_clean,
  hplc_icp           = hplc_icp_ENV_clean,
  gc_hplc_icp        = gc_hplc_icp_ENV_clean
)

# Predefine the result list with names from data_combinations
run_svc_analysis_list_ENV_only <- vector("list", length(data_combinations))
names(run_svc_analysis_list_ENV_only) <- c("gc_ENV",
                                          "hplc_ENV",
                                          "icp_ENV",
                                          "gc_hplc_ENV",
                                          "gc_icp_ENV",
                                          "hplc_icp_ENV",
                                          "gc_hplc_icp_ENV")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_svc_analysis_list_ENV_only[[names(run_svc_analysis_list_ENV_only)[i]]] <- run_svm_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(run_svc_analysis_list_ENV_only)[i], 
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}
```

# GBM
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
run_gbm_analysis_manuscript1 <- function(data,
                                         type_col,
                                         remove_cols,
                                         train_proportion        = 0.5,
                                         use_source_split        = FALSE,               # Option 1: environmental 50:50 split and combine with store-bought
                                         use_store_vs_environmental_split = FALSE,       # Option 2: use store-bought for training and environmental for testing
                                         use_polymer             = FALSE,               # Whether to include "Polymer" as a predictor
                                         do_pairwise_test        = FALSE,               # Run pairwise/ANOVA tests on imputed/normalized data
                                         do_top_importance_selection = FALSE,           # Run additional classification using top-N features by importance
                                         top_importance_counts   = c(100, 50, 25, 10),   # Which top features to try
                                         data_name,              # Name of the input data combination for figure titles
                                         seed                    = NULL,
                                         ntree_candidates        = c(100, 500, 1000, 2500), # Used for gbm's n.trees parameter
                                         metric                  = "Accuracy",
                                         do_rfe                  = FALSE,
                                         rfe_folds               = 5,
                                         do_gbfe                 = FALSE,
                                         do_impute_norm_screen   = TRUE,
                                         group_for_significance  = "Subcategory") {
  
  # 1) Prepare Target Variable and Remove Classes with a Single Sample
  if(!is.null(seed)) set.seed(seed)
  data[[type_col]] <- as.factor(data[[type_col]])
  levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique = TRUE)
  
  class_counts  <- table(data[[type_col]])
  keep_classes  <- names(class_counts[class_counts > 1])
  data          <- data[data[[type_col]] %in% keep_classes, ]
  data[[type_col]] <- droplevels(data[[type_col]])
  
  # 2) Handle Polymer Column if Required
  if(use_polymer) {
    remove_cols <- setdiff(remove_cols, "Polymer")
    if("Polymer" %in% colnames(data)){
      data$Polymer <- as.factor(data$Polymer)
    }
  }
  
  # 3) Imputation and Normalization on Clean Data
  best_imp  <- "No_imputation"
  best_norm <- "No_normalization"
  if(do_impute_norm_screen){
    cat("\n### Screening Best Imputation + Normalization Combo ###\n")
    
    # Identify numeric columns (excluding remove_cols and target)
    numeric_data <- data[!apply(data %>% dplyr::select(-all_of(remove_cols), -all_of(type_col)), 1, function(x) all(is.na(x))), ]
    numeric_cols <- names(which(sapply(numeric_data, is.numeric)))
    
    best_res <- find_best_impute_normalize(
      df = data,
      class_col = type_col,
      group_for_significance = group_for_significance,
      remove_cols = remove_cols
    )
    cat("\n*** Summary of Imputation/Normalization Combos ***\n")
    print(best_res$results_table)
    cat("\n*** Best Combo ***\n")
    print(best_res$best_combo)
    
    best_imp  <- best_res$best_combo$Imputation
    best_norm <- best_res$best_combo$Normalization
    
    X_original <- data[, numeric_cols, drop = FALSE]
    X_imputed <- imputation_methods[[best_imp]](X_original)
    X_final <- normalization_methods[[best_norm]](X_imputed)
    data[, numeric_cols] <- X_final
    
  } else {
    cat("\n### Skipping Imputation + Normalization; Using Raw Data ###\n")
  }
  
  cat("NAs in original data:", sum(is.na(data)), "\n")
  cat("NAs after imputation+normalization:", sum(is.na(data)), "\n")
  
  # 4) Run Pairwise/ANOVA Tests on Cleaned Data (if selected)
  if(do_pairwise_test) {
    cat("\n### Running Pairwise Significance Tests on Cleaned Data ###\n")
    pairwise_res <- pairwise_significance_tests(input_df = data, 
                                                group_col = group_for_significance, 
                                                start_col_index = 6)
    pairwise_sig_features <- pairwise_res$corrected
    if(length(pairwise_sig_features) == 0) {
      cat("No significant features found from pairwise tests. Skipping pairwise feature selection.\n")
      do_pairwise_test <- FALSE
    }
  }
  
  # 5) Train/Test Split
  if(use_store_vs_environmental_split){
    cat("\n### Using Store-vs-Environmental Splitting ###\n")
    train_data <- data[data$Source == "Store-Bought", ]
    test_data  <- data[data$Source == "Environmental", ]
    if(nrow(train_data) < 1) stop("No 'Store-Bought' samples for training!")
    if(nrow(test_data) < 1) stop("No 'Environmental' samples for testing!")
  } else if(use_source_split){
    cat("\n### Using Source-based Splitting with Environmental 50:50 Split ###\n")
    store_bought_set <- data[data$Source == "Store-Bought", ]
    environmental_set <- data[data$Source == "Environmental", ]
    if(!is.null(seed)) {
      set.seed(seed)
    } else {
      set.seed(sample.int(999, 1))
    }
    env_train_index <- caret::createDataPartition(environmental_set[[type_col]], p = 0.5, list = FALSE)
    environmental_training_set <- environmental_set[env_train_index, ]
    environmental_hold_out_testing_set <- environmental_set[-env_train_index, ]
    train_data <- rbind(store_bought_set, environmental_training_set)
    test_data  <- environmental_hold_out_testing_set
    if(nrow(train_data) < 1) stop("Training data is empty!")
    if(nrow(test_data) < 1) stop("Test data is empty!")
  } else {
    cat("\n### Using Original Train/Test Split ###\n")
    X <- data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
    y <- data[[type_col]]
    if(is.null(seed)) {
      set.seed(sample.int(999, 1))
    }
    train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
    train_data <- data[train_index, ]
    test_data  <- data[-train_index, ]
  }
  
  # Drop unused factor levels after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  # 6) Prepare Predictors and Response for Full-Feature Classification
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # 7) Setup Cross-Validation Control
  min_class_count <- min(table(y_train))
  folds <- ifelse(min_class_count > 1, min_class_count, 2)
  cv_ctrl <- caret::trainControl(method = "cv", number = folds, classProbs = TRUE)
  
  # 8) Full-Feature Classification: Grid Search over (n.trees, interaction.depth)
  best_model <- NULL
  best_metric_val <- -Inf
  best_n_trees <- NA
  best_interaction_depth <- NA
  best_n_minobsinnode <- NA
  best_shrinkage <- NA
  dist_param <- if(nlevels(y_train) == 2) "bernoulli" else "multinomial"
  
  for(n_trees in ntree_candidates){
    set.seed(sample.int(999, 1))
    gbm_grid <- expand.grid(n.trees = n_trees,
                            shrinkage = c(0.01, 0.1),
                            n.minobsinnode = c(1,2),
                            interaction.depth = c(1,2,3))
    gbm_fit <- caret::train(x = X_train,
                            y = as.factor(y_train),
                            method = "gbm",
                            trControl = cv_ctrl,
                            tuneGrid = gbm_grid,
                            metric = metric,
                            distribution = dist_param,
                            bag.fraction = 1, # use full sample instead of subsampling
                            verbose = FALSE)
    current_best <- max(gbm_fit$results[[metric]])
    if(current_best > best_metric_val){
      best_metric_val <- current_best
      best_model <- gbm_fit
      best_n_trees <- n_trees
      best_interaction_depth <- gbm_fit$bestTune$interaction.depth
      best_n_minobsinnode <- gbm_fit$bestTune$n.minobsinnode
      best_shrinkage <- gbm_fit$bestTune$shrinkage
    }
  }
  
  cat("Best Full-Feature Model", metric, "on Training =", best_metric_val,
      " with n.trees =", best_n_trees,
      " and interaction.depth =", best_interaction_depth, "\n")
  
  final_gbm <- best_model$finalModel
  
  pred_prob_test <- predict(best_model, newdata = X_test, type = "prob")
  new_pred_df_test <- as.data.frame(pred_prob_test)
  new_pred_df_test[[type_col]] <- y_test
  
  y_test_aligned <- factor(as.character(y_test), levels = levels(y_train))
  pred_class_test <- predict(best_model, newdata = X_test, type = "raw")
  pred_class_test_aligned <- factor(as.character(pred_class_test), levels = levels(y_train))
  
  acc_full <- compute_balanced_accuracy(true = y_test_aligned, pred = pred_class_test_aligned)
  
  # --- Generate Heatmap (Full Features) ---
  {
    df_heatmap <- new_pred_df_test
    row_names <- as.character(df_heatmap[[type_col]])
    new_row_names <- ave(row_names, row_names, FUN = function(x) {
      if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
    })
    rownames(df_heatmap) <- new_row_names
    df_heatmap[[type_col]] <- NULL
    prob_matrix <- as.matrix(df_heatmap)
    
    pheatmap::pheatmap(prob_matrix,
                       fontsize = 15,
                       cluster_rows = FALSE,
                       cluster_cols = FALSE,
                       color = viridis::viridis(100),
                       display_numbers = TRUE,
                       number_format = "%.2f",
                       main = paste("Classification Probability (All Features) -", data_name, 
                                    sprintf("| Balanced Test Accuracy: %.2f", acc_full)),
                       breaks = seq(0, 1, length.out = 101))
  }
  
  # Obtain feature importance via relative influence
  gbm_importances <- summary(best_model$finalModel, plotit = FALSE)
  gbm_importances <- gbm_importances[order(gbm_importances$rel.inf, decreasing = TRUE), ]
  cat("\nTop Features by Relative Influence (Full Features):\n")
  print(head(gbm_importances))
  
  # 9) Classification with Pairwise/ANOVA Selected Features (if requested)
  if(do_pairwise_test){
    pairwise_features <- intersect(colnames(X_train), pairwise_sig_features)
    
    if(length(pairwise_features) == 0){
      cat("No overlapping pairwise significant features found in predictors. Skipping pairwise model.\n")
    } else {
      X_train_pair <- X_train[, pairwise_features, drop = FALSE]
      X_test_pair  <- X_test[, pairwise_features, drop = FALSE]
      
      cat("\n### Running Classification on Pairwise Selected Features ###\n")
      cat("Using Pairwise Features:", paste(pairwise_features, collapse = ", "), "\n")
      
      gbm_grid_pair <- expand.grid(n.trees = best_n_trees, 
                                   shrinkage = c(0.01, 0.1),
                                   n.minobsinnode = c(1,2),
                                   interaction.depth = c(1,2,3))
      
      best_model_pair <- caret::train(x = X_train_pair,
                                      y = as.factor(y_train),
                                      method = "gbm",
                                      trControl = cv_ctrl,
                                      tuneGrid = gbm_grid_pair,
                                      metric = metric,
                                      distribution = dist_param,
                                      bag.fraction = 1, # use full sample instead of subsampling
                                      verbose = FALSE)
      cat("Pairwise Model", metric, "on Training =", max(best_model_pair$results[[metric]]), "\n")
      
      final_gbm_pair <- best_model_pair$finalModel
      
      pred_prob_test_pair <- predict(best_model_pair, newdata = X_test_pair, type = "prob")
      new_pred_df_test_pair <- as.data.frame(pred_prob_test_pair)
      new_pred_df_test_pair[[type_col]] <- y_test
      
      y_test_aligned <- factor(as.character(y_test), levels = levels(y_train))
      pred_class_pair <- predict(best_model_pair, newdata = X_test_pair, type = "raw")
      pred_class_pair_aligned <- factor(as.character(pred_class_pair), levels = levels(y_train))
      
      acc_pair <- compute_balanced_accuracy(true = y_test_aligned, pred = pred_class_pair_aligned)
      
      {
        df_heatmap_pair <- new_pred_df_test_pair
        row_names_pair <- as.character(df_heatmap_pair[[type_col]])
        new_row_names_pair <- ave(row_names_pair, row_names_pair, FUN = function(x) {
          if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
        })
        rownames(df_heatmap_pair) <- new_row_names_pair
        df_heatmap_pair[[type_col]] <- NULL
        prob_matrix_pair <- as.matrix(df_heatmap_pair)
        
        pheatmap::pheatmap(prob_matrix_pair,
                           fontsize = 15,
                           cluster_rows = FALSE,
                           cluster_cols = FALSE,
                           color = viridis::viridis(100),
                           display_numbers = TRUE,
                           number_format = "%.2f",
                           main = paste("Classification Probability (Pairwise Features) -", data_name, 
                                        sprintf("| Balanced Test Accuracy: %.2f", acc_pair)),
                           breaks = seq(0, 1, length.out = 101))
      }
    }
  }
  
  # 10) Classification with Top-N Features by Importance (if requested)
  if(do_top_importance_selection){
    cat("\n### Running Classification on Top-N Features by Importance ###\n")
    top_importance_results <- list()
    total_features <- nrow(gbm_importances)
    
    for(top_n in top_importance_counts){
      if(total_features < top_n){
        selected_features <- gbm_importances$var
        cat(sprintf("Only %d features available; using all.\n", total_features))
      } else {
        selected_features <- head(gbm_importances$var, top_n)
      }
      
      X_train_top <- X_train[, selected_features, drop = FALSE]
      X_test_top  <- X_test[, selected_features, drop = FALSE]
      
      gbm_grid_top <- expand.grid(n.trees = best_n_trees, 
                                  shrinkage = c(0.01, 0.1),
                                  n.minobsinnode = c(1,2),
                                  interaction.depth = c(1,2,3))
      
      best_model_top <- caret::train(
        x = X_train_top,
        y = as.factor(y_train),
        method = "gbm",
        trControl = cv_ctrl,
        tuneGrid = gbm_grid_top,
        metric = metric,
        distribution = dist_param,
        bag.fraction = 1,
        verbose = FALSE)
      
      final_gbm_top <- best_model_top$finalModel
      
      pred_prob_test_top <- predict(best_model_top, newdata = X_test_top, type = "prob")
      new_pred_df_test_top <- as.data.frame(pred_prob_test_top)
      new_pred_df_test_top[[type_col]] <- y_test
      
      y_test_aligned <- factor(as.character(y_test), levels = levels(y_train))
      pred_class_top <- predict(best_model_top, newdata = X_test_top, type = "raw")
      pred_class_top_aligned <- factor(as.character(pred_class_top), levels = levels(y_train))
      
      acc_top <- compute_balanced_accuracy(true = y_test_aligned, pred = pred_class_top_aligned)
      
      cat(sprintf("Top-N (%d) Balanced Test Accuracy: %f\n", top_n, acc_top))
      
      {
        df_heatmap_top <- new_pred_df_test_top
        row_names_top <- as.character(df_heatmap_top[[type_col]])
        new_row_names_top <- ave(row_names_top, row_names_top, FUN = function(x) {
          if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
        })
        rownames(df_heatmap_top) <- new_row_names_top
        df_heatmap_top[[type_col]] <- NULL
        prob_matrix_top <- as.matrix(df_heatmap_top)
        pheatmap::pheatmap(prob_matrix_top,
                           fontsize = 15,
                           cluster_rows = FALSE,
                           cluster_cols = FALSE,
                           color = viridis::viridis(100),
                           display_numbers = TRUE,
                           number_format = "%.2f",
                           main = paste("Classification Probability (Top", top_n, "Features) -", 
                                        data_name, sprintf("| Balanced Test Accuracy: %.2f", acc_top)),
                           breaks = seq(0, 1, length.out = 101))
      }
      
      top_importance_results[[paste0("top_", top_n)]] <- list(
        best_model = best_model_top,
        best_metric_val = max(best_model_top$results[[metric]]),
        best_n_trees = best_n_trees,
        accuracy = acc_top,
        selected_features = selected_features
      )
    }
  }
  
  # 11) (Optional) RFE with Fixed Hyperparameters
  final_gbm_opt <- NULL
  rfe_results_fixed <- NULL
  rfe_selected_feats <- NULL
  
  if(do_rfe){
    my_gbmFuncs <- caret::gbmFuncs
    my_gbmFuncs$fit <- function(x, y, first, last, ...) {
      caret::train(x = x, y = y,
                   method = "gbm",
                   trControl = cv_ctrl,
                   tuneGrid = expand.grid(n.trees = best_n_trees,
                                          shrinkage = c(0.01, 0.1),
                                          n.minobsinnode = c(1,2),
                                          interaction.depth = c(1,2,3)),
                   metric = metric,
                   distribution = dist_param,
                   bag.fraction = 1,
                   verbose = FALSE)
    }
    rfe_ctrl_fixed <- caret::rfeControl(functions = my_gbmFuncs,
                                        method = "cv",
                                        number = rfe_folds,
                                        repeats = 1,
                                        saveDetails = TRUE,
                                        returnResamp = "final")
    subset_sizes <- seq_len(ncol(X_train))
    set.seed(sample.int(999, 1))
    rfe_results_fixed <- caret::rfe(x = X_train, y = y_train, sizes = subset_sizes,
                                    rfeControl = rfe_ctrl_fixed)
    cat("Best Subset Size =", rfe_results_fixed$bestSubset, "\n")
    cat("Selected Features:\n")
    print(rfe_results_fixed$optVariables)
    rfe_selected_feats <- rfe_results_fixed$optVariables
    
    X_train_opt <- X_train[, rfe_selected_feats, drop = FALSE]
    
    best_model_opt <- caret::train(x = X_train_opt, y = as.factor(y_train),
                                   method = "gbm",
                                   trControl = cv_ctrl,
                                   tuneGrid = expand.grid(n.trees = best_n_trees, 
                                                          shrinkage = c(0.01, 0.1),
                                                          n.minobsinnode = c(1,2),
                                                          interaction.depth = c(1,2,3)),
                                   metric = metric,
                                   distribution = dist_param,
                                   bag.fraction = 1,
                                   verbose = FALSE)
    
    final_gbm_opt <- best_model_opt$finalModel
    X_test_opt <- X_test[, rfe_selected_feats, drop = FALSE]
    pred_prob_test_opt <- predict(best_model_opt, newdata = X_test_opt, type = "prob")
    new_pred_df_test_opt <- as.data.frame(pred_prob_test_opt)
    new_pred_df_test_opt[[type_col]] <- y_test
    
    y_test_aligned <- factor(as.character(y_test), levels = levels(y_train))
    pred_class_opt <- predict(best_model_opt, newdata = X_test_opt, type = "raw")
    pred_class_opt_aligned <- factor(as.character(pred_class_opt), levels = levels(y_train))
    
    acc_opt <- compute_balanced_accuracy(true = y_test_aligned, pred = pred_class_opt_aligned)
    
    {
      df_heatmap_opt <- new_pred_df_test_opt
      row_names_opt <- as.character(df_heatmap_opt[[type_col]])
      new_row_names_opt <- ave(row_names_opt, row_names_opt, FUN = function(x) {
        if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
      })
      rownames(df_heatmap_opt) <- new_row_names_opt
      df_heatmap_opt[[type_col]] <- NULL
      prob_matrix_opt <- as.matrix(df_heatmap_opt)
      pheatmap::pheatmap(prob_matrix_opt,
                         fontsize = 15,
                         cluster_rows = FALSE,
                         cluster_cols = FALSE,
                         color = viridis::viridis(100),
                         display_numbers = TRUE,
                         number_format = "%.2f",
                         main = paste("Classification Probability (RFE Features) -", data_name, 
                                      sprintf("| Balanced Test Accuracy: %.2f", acc_opt)),
                         breaks = seq(0, 1, length.out = 101))
    }
    
  }
  
  # 12) (Optional) Greedy Backward Feature Elimination (GBFE)
  gbfe_selected_feats <- colnames(X_train)
  final_gbm_gbfe <- NULL
  
  if(do_gbfe){
    cat("\n========== BEGIN GREEDY BACKWARD FEATURE ELIMINATION (GBFE) ==========\n")
    baseline_test_acc <- acc_full
    current_features <- colnames(X_train)
    improved <- TRUE
    
    while(improved && length(current_features) > 1){
      improved <- FALSE
      for(feature_to_remove in current_features){
        trial_features <- setdiff(current_features, feature_to_remove)
        set.seed(sample.int(999, 1))
        trial_model <- caret::train(x = X_train[, trial_features, drop = FALSE],
                                    y = as.factor(y_train),
                                    method = "gbm",
                                    trControl = cv_ctrl,
                                    tuneGrid = expand.grid(n.trees = best_n_trees,
                                                           interaction.depth = min(best_interaction_depth, length(trial_features)),
                                                           shrinkage = c(0.01, 0.1),
                                                           n.minobsinnode = c(1,2,3)),
                                    metric = metric,
                                    distribution = dist_param,
                                    bag.fraction = 1,
                                    verbose = FALSE)
        pred_trial <- predict(trial_model, newdata = X_test[, trial_features, drop = FALSE], type = "raw")
        pred_trial_aligned <- factor(as.character(pred_trial), levels = levels(y_train))
        trial_acc <- compute_balanced_accuracy(true = y_test_aligned, pred = pred_trial_aligned)
        if(!is.na(trial_acc) && trial_acc < baseline_test_acc){
          cat(sprintf("Removing feature '%s' (Balanced test acc: %.3f < baseline: %.3f)\n",
                      feature_to_remove, trial_acc, baseline_test_acc))
          current_features <- trial_features
          improved <- TRUE
          baseline_test_acc <- trial_acc
          break
        }
      }
    }
    
    gbfe_selected_feats <- current_features
    cat("\nFinal GBFE Selected Features:\n")
    print(gbfe_selected_feats)
    
    best_model_gbfe <- caret::train(x = X_train[, gbfe_selected_feats, drop = FALSE],
                                    y = as.factor(y_train),
                                    method = "gbm",
                                    trControl = cv_ctrl,
                                    tuneGrid = expand.grid(n.trees = best_n_trees,
                                                           interaction.depth = min(best_interaction_depth, ncol(X_train[, gbfe_selected_feats, drop = FALSE])),
                                                           shrinkage = c(0.01, 0.1),
                                                           n.minobsinnode = c(1,2,3)),
                                    metric = metric,
                                    distribution = dist_param,
                                    bag.fraction = 1,
                                    verbose = FALSE)
    
    final_gbm_gbfe <- best_model_gbfe$finalModel
    X_test_gbfe <- X_test[, gbfe_selected_feats, drop = FALSE]
    pred_prob_test_gbfe <- predict(best_model_gbfe, newdata = X_test_gbfe, type = "prob")
    new_pred_df_test_gbfe <- as.data.frame(pred_prob_test_gbfe)
    new_pred_df_test_gbfe[[type_col]] <- y_test
    
    y_test_aligned <- factor(as.character(y_test), levels = levels(y_train))
    pred_class_gbfe <- predict(best_model_gbfe, newdata = X_test_gbfe, type = "raw")
    pred_class_gbfe_aligned <- factor(as.character(pred_class_gbfe), levels = levels(y_train))
    acc_gbfe <- compute_balanced_accuracy(true = y_test_aligned, pred = pred_class_gbfe_aligned)
    
    {
      df_heatmap_gbfe <- new_pred_df_test_gbfe
      row_names_gbfe <- as.character(df_heatmap_gbfe[[type_col]])
      new_row_names_gbfe <- ave(row_names_gbfe, row_names_gbfe, FUN = function(x) {
        if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
      })
      rownames(df_heatmap_gbfe) <- new_row_names_gbfe
      df_heatmap_gbfe[[type_col]] <- NULL
      prob_matrix_gbfe <- as.matrix(df_heatmap_gbfe)
      pheatmap::pheatmap(prob_matrix_gbfe,
                         fontsize = 15,
                         cluster_rows = FALSE,
                         cluster_cols = FALSE,
                         color = viridis::viridis(100),
                         display_numbers = TRUE,
                         number_format = "%.2f",
                         main = paste("Classification Probability (GBFE Features) -", data_name, 
                                      sprintf("| Balanced Test Accuracy: %.2f", acc_gbfe)),
                         breaks = seq(0, 1, length.out = 101))
    }
    
  }
  
  cat("\nFull-Feature Balanced Test Accuracy:", acc_full, "\n")
  if(do_rfe) cat("\nRFE-Optimized Balanced Test Accuracy:", acc_opt, "\n")
  if(do_gbfe) cat("\nGBFE-Selected Balanced Test Accuracy:", acc_gbfe, "\n")
  if(do_pairwise_test) cat("\nPairwise Selected Features Balanced Test Accuracy:", acc_pair, "\n")
  
  result_list <- list(
    final_imp_norm_dat    = data,
    imp_norm_res_table    = best_res$results_table,
    best_imputatation     = best_imp,
    best_normalization    = best_norm,
    best_caret_model      = best_model,
    final_gbm_all_feats   = final_gbm,
    full_features_acc     = acc_full,
    rfe_model             = rfe_results_fixed,
    final_gbm_opt         = final_gbm_opt,
    rfe_selected_features = rfe_selected_feats,
    gbfe_model            = final_gbm_gbfe,
    gbfe_selected_feats   = gbfe_selected_feats
  )
  
  if(do_pairwise_test){
    result_list$pairwise_model    <- best_model_pair
    result_list$final_gbm_pair    <- final_gbm_pair
    result_list$pairwise_features <- pairwise_features
    result_list$pairwise_test_acc <- acc_pair
  }
  
  if(do_top_importance_selection){
    result_list$top_importance_results <- top_importance_results
  }
  
  return(result_list)
}
```

### Option 1: Use SB to predict ENV - Only select shared features between training and testing 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
###################################################################################
## Explanation of the parameter of run_rf_analysis()
# split environmental into 50:50 and combine with store_bought -> use_source_split = FALSE
# use store-bought as training and environmental as testing -> use_store_vs_environmental_split = TRUE
###################################################################################

data_combinations <- list(
  gc                 = gc_combined_SB_ENV_shared_cols_ntr_clean,
  hplc               = hplc_combined_SB_ENV_shared_cols_ntr_clean,
  icp                = icp_combined_SB_ENV_shared_cols_ntr_clean ,
  gc_hplc            = gc_hplc_combined_SB_ENV_shared_cols_ntr_clean,
  gc_icp             = gc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  hplc_icp           = hplc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  gc_hplc_icp        = gc_hplc_icp_combined_SB_ENV_shared_cols_ntr_clean
)

# Predefine the result list with names from data_combinations
run_gbm_analysis_list_SB_train <- vector("list", length(data_combinations))
names(run_gbm_analysis_list_SB_train) <- c("gc_USSB_train",
                                          "hplc_USSB_train",
                                          "icp_USSB_train",
                                          "gc_hplc_USSB_train",
                                          "gc_icp_USSB_train",
                                          "hplc_icp_USSB_train",
                                          "gc_hplc_icp_USSB_train")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_gbm_analysis_list_SB_train[[names(run_gbm_analysis_list_SB_train)[i]]] <- run_gbm_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_store_vs_environmental_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}


# Scatter plot features importance -------
# gbm_importances <- as.data.frame(run_gbm_analysis_list_SB_train$gc_USSB_train$final_gbm_all_feats$importance)
# gbm_importances$Feature <- rownames(gbm_importances)
# gbm_importances <- gbm_importances[order(gbm_importances$MeanDecreaseGini, decreasing = TRUE), ]
# 
# ggplot(gbm_importances, aes(x = reorder(Feature, 
#                                        MeanDecreaseGini), y = MeanDecreaseGini)) +
#   geom_point() +
#   labs(x = "Feature", y = "Mean Decrease Gini",
#        title = "Scatter Plot: Features from Lowest to Highest Mean Decrease Gini") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Option 2: Use Both SB and ENV to predict ENV
Run prediction separatedly on each analytical dataset and only use significant features from GBFE and pairwise test asinput for combined data

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_combined_SB_ENV_shared_cols_ntr_clean,
  hplc               = hplc_combined_SB_ENV_shared_cols_ntr_clean,
  icp                = icp_combined_SB_ENV_shared_cols_ntr_clean ,
  gc_hplc            = gc_hplc_combined_SB_ENV_shared_cols_ntr_clean,
  gc_icp             = gc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  hplc_icp           = hplc_icp_combined_SB_ENV_shared_cols_ntr_clean,
  gc_hplc_icp        = gc_hplc_icp_combined_SB_ENV_shared_cols_ntr_clean
)

# Predefine the result list with names from data_combinations
run_gbm_analysis_list_SB_and_ENV_train <- vector("list", length(data_combinations))
names(run_gbm_analysis_list_SB_and_ENV_train) <- c("gc_SB_and_ENV_train",
                                                  "hplc_SB_and_ENV_train",
                                                  "icp_SB_and_ENV_train",
                                                  "gc_hplc_SB_and_ENV_train",
                                                  "gc_icp_SB_and_ENV_train",
                                                  "hplc_icp_SB_and_ENV_train",
                                                  "gc_hplc_icp_SB_and_ENV_train")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_gbm_analysis_list_SB_and_ENV_train[[names(run_gbm_analysis_list_SB_and_ENV_train)[i]]] <- run_gbm_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(data_combinations)[i], 
    use_source_split = TRUE,
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}
```

### Option 3: Use SB to predict SB

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_SB_clean,
  hplc               = hplc_SB_clean,
  icp                = icp_SB_clean ,
  gc_hplc            = gc_hplc_SB_clean,
  gc_icp             = gc_icp_SB_clean,
  hplc_icp           = hplc_icp_SB_clean,
  gc_hplc_icp        = gc_hplc_icp_SB_clean
)

# Predefine the result list with names from data_combinations
run_gbm_analysis_list_SB_only <- vector("list", length(data_combinations))
names(run_gbm_analysis_list_SB_only) <- c("gc_SB",
                                         "hplc_SB",
                                         "icp_SB",
                                         "gc_hplc_SB",
                                         "gc_icp_SB",
                                         "hplc_icp_SB",
                                         "gc_hplc_icp_SB")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_gbm_analysis_list_SB_only[[names(run_gbm_analysis_list_SB_only)[i]]] <- run_gbm_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(run_gbm_analysis_list_SB_only)[i], 
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}
```

### Option 4: Use ENV to predict ENV

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
data_combinations <- list(
  gc                 = gc_ENV_clean,
  hplc               = hplc_ENV_clean,
  icp                = icp_ENV_clean ,
  gc_hplc            = gc_hplc_ENV_clean,
  gc_icp             = gc_icp_ENV_clean,
  hplc_icp           = hplc_icp_ENV_clean,
  gc_hplc_icp        = gc_hplc_icp_ENV_clean
)

# Predefine the result list with names from data_combinations
run_gbm_analysis_list_ENV_only <- vector("list", length(data_combinations))
names(run_gbm_analysis_list_ENV_only) <- c("gc_ENV",
                                          "hplc_ENV",
                                          "icp_ENV",
                                          "gc_hplc_ENV",
                                          "gc_icp_ENV",
                                          "hplc_icp_ENV",
                                          "gc_hplc_icp_ENV")

# Iterate through each data combination by index
for(i in seq_along(data_combinations)) {
  cat("Processing dataset:", names(data_combinations)[i], "\n")
  
  # Run the analysis using the corresponding data and pass the custom name for figure titles
  run_gbm_analysis_list_ENV_only[[names(run_gbm_analysis_list_ENV_only)[i]]] <- run_gbm_analysis_manuscript1(
    data = data_combinations[[i]],
    data_name = names(run_gbm_analysis_list_ENV_only)[i], 
    type_col                = "Plastic_type", 
    remove_cols             = c("Source", "Polymer", "technique", "Subcategory"),
    do_pairwise_test = TRUE,             # Adjust parameters as needed
    do_top_importance_selection = TRUE,
    do_gbfe = TRUE,
    seed = 123)
}
```


## Elastic Net
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
run_enet_analysis <- function(data,
                              type_col         = "Subcategory",
                              remove_cols      = c("Source", "Polymer", "technique"),
                              train_proportion = 0.5,
                              use_source_split = FALSE,               # Option 1: environmental 50:50 split and combine with store-bought
                              use_store_vs_environmental_split = FALSE, # Option 2: use store-bought for training and environmental for testing
                              seed             = NULL,
                              metric           = "Accuracy",
                              do_rfe           = FALSE,
                              rfe_folds        = 5,
                              do_gbfe          = TRUE,
                              do_impute_norm_screen = TRUE,
                              group_for_significance = "Subcategory") {
  
  # 0) Optional set seed
  if(!is.null(seed)) set.seed(seed)
  
  # 1) Factorize the target column and remove classes with only one sample
  data[[type_col]] <- as.factor(data[[type_col]])
  levels(data[[type_col]]) <- make.names(levels(data[[type_col]]), unique = TRUE)
  
  # Remove single-member classes
  class_counts  <- table(data[[type_col]])
  keep_classes  <- names(class_counts[class_counts > 1])
  data          <- data[data[[type_col]] %in% keep_classes, ]
  data[[type_col]] <- droplevels(data[[type_col]])
  
  # 2) Imputation and normalization (if requested)
  best_imp  <- "No_imputation"
  best_norm <- "No_normalization"
  if(do_impute_norm_screen){
    cat("\n### Screening best Imputation + Normalization combo ###\n")
    best_res <- find_best_impute_normalize(
      df = data,
      class_col = type_col,
      group_for_significance = group_for_significance,
      remove_cols = setdiff(remove_cols, type_col)
    )
    cat("\n*** Summary of all combos ***\n")
    print(best_res$results_table)
    
    cat("\n*** Best combo ***\n")
    print(best_res$best_combo)
    
    best_imp  <- best_res$best_combo$Imputation
    best_norm <- best_res$best_combo$Normalization
    
    numeric_cols <- colnames(data %>% dplyr::select(-all_of(remove_cols), -all_of(type_col)))
    X_original   <- data[, numeric_cols, drop = FALSE]
    X_imputed    <- imputation_methods[[best_imp]](X_original)
    X_final      <- normalization_methods[[best_norm]](X_imputed)
    data[, numeric_cols] <- X_final
  } else {
    cat("\n### Skipping impute+norm screen; using raw data ###\n")
  }
  
  cat("NAs in original data:", sum(is.na(data)), "\n")
  cat("NAs after final imputation+normalization:", sum(is.na(data)), "\n")
  
  # --- Conditional Train/Test Split ---
  if(use_store_vs_environmental_split){
    cat("\n### Using Store-vs-Environmental splitting ###\n")
    # Training set: only "Store-Bought"; Test set: only "Environmental"
    train_data <- data[data$Source == "Store-Bought", ]
    test_data  <- data[data$Source == "Environmental", ]
    
    if(nrow(train_data) < 1) stop("No 'Store-Bought' samples found for training!")
    if(nrow(test_data) < 1) stop("No 'Environmental' samples found for testing!")
    
  } else if(use_source_split){
    cat("\n### Using Source-based splitting with environmental 50:50 split ###\n")
    store_bought_set <- data[data$Source == "Store-Bought", ]
    environmental_set <- data[data$Source == "Environmental", ]
    
    if(!is.null(seed)) {
      set.seed(seed)
    } else {
      set.seed(sample.int(999, 1))
    }
    env_train_index <- caret::createDataPartition(environmental_set[[type_col]], p = 0.5, list = FALSE)
    environmental_training_set <- environmental_set[env_train_index, ]
    environmental_hold_out_testing_set <- environmental_set[-env_train_index, ]
    
    train_data <- rbind(store_bought_set, environmental_training_set)
    test_data  <- environmental_hold_out_testing_set
    
    if(nrow(train_data) < 1) stop("Training data is empty after source-based splitting!")
    if(nrow(test_data) < 1) stop("Test data is empty after source-based splitting!")
    
  } else {
    cat("\n### Using original train/test split ###\n")
    X <- data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
    y <- data[[type_col]]
    if(is.null(seed)) {
      set.seed(sample.int(999, 1))
    }
    train_index <- caret::createDataPartition(y, p = train_proportion, list = FALSE)
    train_data <- data[train_index, ]
    test_data  <- data[-train_index, ]
  }
  
  # Drop unused factor levels in the outcome variable after splitting
  train_data[[type_col]] <- droplevels(train_data[[type_col]])
  test_data[[type_col]]  <- droplevels(test_data[[type_col]])
  
  X_train <- train_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_train <- train_data[[type_col]]
  X_test  <- test_data %>% dplyr::select(-c(all_of(remove_cols), all_of(type_col)))
  y_test  <- test_data[[type_col]]
  
  # ========= Continue with model training =========
  
  # 4) Cross-validation control
  min_class_count <- min(table(y_train))
  folds <- ifelse(min_class_count > 1, min_class_count, 2)
  cv_ctrl <- caret::trainControl(
    method = "cv",
    number = folds,
    classProbs = TRUE
  )
  
  # 5) Elastic Net grid search.
  # Here we fix alpha = 0.5 and tune over a range of lambda values.
  enet_grid <- expand.grid(alpha = 0.5, lambda = 10^seq(-4, 1, length = 50))
  set.seed(sample.int(999, 1))
  enet_fit <- caret::train(
    x = X_train,
    y = y_train,
    method = "glmnet",
    trControl = cv_ctrl,
    tuneGrid = enet_grid,
    metric = metric
  )
  best_model <- enet_fit
  best_metric_val <- max(best_model$results[[metric]])
  cat("Best Model", metric, "on training =", best_metric_val,
      "with bestTune: alpha =", best_model$bestTune$alpha,
      "and lambda =", best_model$bestTune$lambda, "\n")
  
  # 6) Final model predictions on the test set (Full Feature Set)
  pred_prob_test <- predict(best_model, newdata = X_test, type = "prob")
  new_pred_df_test <- as.data.frame(pred_prob_test)
  new_pred_df_test$Subcategory <- y_test
  
  # Enforce common factor levels: re-factor using training levels.
  y_test_aligned <- factor(as.character(y_test), levels = levels(y_train))
  pred_class_test <- predict(best_model, newdata = X_test, type = "raw")
  pred_class_test_aligned <- factor(as.character(pred_class_test), levels = levels(y_train))
  
  # Compute accuracy, treating any NA as incorrect.
  acc_full <- sum(pred_class_test_aligned == y_test_aligned, na.rm = TRUE) / length(y_test_aligned)
  
  # 7) Feature Importances (using coefficient magnitudes)
  enet_coefs <- as.matrix(coef(best_model$finalModel, s = best_model$bestTune$lambda))
  enet_coefs <- enet_coefs[-1, , drop = FALSE]  # Remove intercept
  enet_importances <- data.frame(Feature = rownames(enet_coefs), Coefficient = as.vector(enet_coefs))
  enet_importances <- enet_importances[order(abs(enet_importances$Coefficient), decreasing = TRUE), ]
  cat("\nTop Features by Coefficient Magnitude:\n")
  print(head(enet_importances))
  
  # 8) (Optional) RFE with fixed hyperparameters for Elastic Net
  final_enet_opt <- NULL
  rfe_results_fixed <- NULL
  rfe_selected_feats <- NULL
  
  if(do_rfe){
    # Define custom functions for Elastic Net for use with caret::rfe
    enetFuncs <- list(
      fit = function(x, y, first, last, ...){
        caret::train(
          x = x,
          y = y,
          method = "glmnet",
          trControl = trainControl(method = "cv", number = rfe_folds, classProbs = TRUE),
          tuneGrid = expand.grid(alpha = best_model$bestTune$alpha, lambda = best_model$bestTune$lambda),
          metric = metric
        )
      },
      pred = function(object, x) predict(object, newdata = x, type = "raw"),
      rank = function(object, x, y) {
        coefs <- as.matrix(coef(object$finalModel, s = object$bestTune$lambda))
        coefs <- coefs[-1, , drop = FALSE]  # drop intercept
        out <- abs(coefs)
        out <- as.vector(out)
        names(out) <- rownames(coefs)
        out
      },
      selectSize = function(object, metric, ...) {
        ncol(object$trainingData) - 1
      },
      selectVar = function(object, size) {
        var_names <- rownames(as.matrix(coef(object$finalModel, s = object$bestTune$lambda)))[-1]
        var_names
      }
    )
    rfe_ctrl_fixed <- rfeControl(
      functions = enetFuncs,
      method = "cv",
      number = rfe_folds,
      repeats = 1,
      saveDetails = TRUE,
      returnResamp = "final"
    )
    subset_sizes <- seq_len(ncol(X_train))
    set.seed(123)
    rfe_results_fixed <- rfe(
      x = X_train,
      y = y_train,
      sizes = subset_sizes,
      rfeControl = rfe_ctrl_fixed
    )
    cat("Best Subset Size =", rfe_results_fixed$bestSubset, "\n")
    cat("Selected Features:\n")
    print(rfe_results_fixed$optVariables)
    rfe_selected_feats <- rfe_results_fixed$optVariables
    
    X_train_opt <- X_train[, rfe_selected_feats, drop = FALSE]
    final_enet_opt <- caret::train(
      x = X_train_opt,
      y = y_train,
      method = "glmnet",
      trControl = cv_ctrl,
      tuneGrid = expand.grid(alpha = best_model$bestTune$alpha, lambda = best_model$bestTune$lambda),
      metric = metric
    )
    
    X_test_opt <- X_test[, rfe_selected_feats, drop = FALSE]
    pred_class_opt <- predict(final_enet_opt, newdata = X_test_opt, type = "raw")
    pred_class_opt_aligned <- factor(as.character(pred_class_opt), levels = levels(y_train))
    acc_opt <- sum(pred_class_opt_aligned == y_test_aligned, na.rm = TRUE) / length(y_test_aligned)
    
    pred_prob_test_opt <- predict(final_enet_opt, newdata = X_test_opt, type = "prob")
    new_pred_df_test_opt <- as.data.frame(pred_prob_test_opt)
    new_pred_df_test_opt$Subcategory <- y_test
    
    {
      df_heatmap_opt <- new_pred_df_test_opt
      row_names_opt <- as.character(df_heatmap_opt$Subcategory)
      new_row_names_opt <- ave(row_names_opt, row_names_opt, FUN = function(x) {
        if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
      })
      rownames(df_heatmap_opt) <- new_row_names_opt
      df_heatmap_opt$Subcategory <- NULL
      prob_matrix_opt <- as.matrix(df_heatmap_opt)
      pheatmap(
        prob_matrix_opt,
        fontsize = 15,
        cluster_rows = FALSE,
        cluster_cols = FALSE,
        color = viridis(100),
        display_numbers = TRUE,
        number_format = "%.2f",
        main = "Classification Probability (RFE Features - Elastic Net)"
      )
    }
  }
  
  # 9) (Optional) Greedy Backward Feature Elimination (GBFE) for Elastic Net
  gbfe_selected_feats <- colnames(X_train)
  final_enet_gbfe <- NULL
  
  if(do_gbfe){
    cat("\n========== BEGIN GREEDY BACKWARD FEATURE ELIMINATION (GBFE) ==========\n")
    baseline_test_acc <- acc_full
    current_features <- colnames(X_train)
    improved <- TRUE
    
    while(improved && length(current_features) > 1){
      improved <- FALSE
      
      for(feature_to_remove in current_features){
        trial_features <- setdiff(current_features, feature_to_remove)
        set.seed(1000)
        trial_model <- caret::train(
          x = X_train[, trial_features, drop = FALSE],
          y = y_train,
          method = "glmnet",
          trControl = cv_ctrl,
          metric = metric,
          tuneGrid = expand.grid(alpha = best_model$bestTune$alpha, lambda = best_model$bestTune$lambda)
        )
        pred_trial <- predict(trial_model, newdata = X_test[, trial_features, drop = FALSE], type = "raw")
        pred_trial_aligned <- factor(as.character(pred_trial), levels = levels(y_train))
        trial_acc <- sum(pred_trial_aligned == y_test_aligned, na.rm = TRUE) / length(y_test_aligned)
        
        if(!is.na(trial_acc) && trial_acc >= baseline_test_acc){
          cat(sprintf("Removing feature '%s' (test acc: %.3f >= baseline: %.3f)\n",
                      feature_to_remove, trial_acc, baseline_test_acc))
          current_features <- trial_features
          improved <- TRUE
          break
        }
      }
    }
    
    gbfe_selected_feats <- current_features
    cat("\nFinal GBFE Selected Features:\n")
    print(gbfe_selected_feats)
    
    final_enet_gbfe <- caret::train(
      x = X_train[, gbfe_selected_feats, drop = FALSE],
      y = y_train,
      method = "glmnet",
      trControl = cv_ctrl,
      tuneGrid = expand.grid(alpha = best_model$bestTune$alpha, lambda = best_model$bestTune$lambda),
      metric = metric
    )
    
    X_test_gbfe <- X_test[, gbfe_selected_feats, drop = FALSE]
    pred_class_gbfe <- predict(final_enet_gbfe, newdata = X_test_gbfe, type = "raw")
    pred_class_gbfe_aligned <- factor(as.character(pred_class_gbfe), levels = levels(y_train))
    acc_gbfe <- sum(pred_class_gbfe_aligned == y_test_aligned, na.rm = TRUE) / length(y_test_aligned)
    
    # Probability predictions on the test set
    pred_prob_test_gbfe <- predict(final_enet_gbfe, newdata = X_test_gbfe, type = "prob")
    new_pred_df_test_gbfe <- as.data.frame(pred_prob_test_gbfe)
    new_pred_df_test_gbfe$Subcategory <- y_test
    
    {
      df_heatmap_gbfe <- new_pred_df_test_gbfe
      row_names_gbfe <- as.character(df_heatmap_gbfe$Subcategory)
      new_row_names_gbfe <- ave(row_names_gbfe, row_names_gbfe, FUN = function(x) {
        if (length(x) > 1) paste0(x, "_rep", seq_along(x)) else x
      })
      rownames(df_heatmap_gbfe) <- new_row_names_gbfe
      df_heatmap_gbfe$Subcategory <- NULL
      prob_matrix_gbfe <- as.matrix(df_heatmap_gbfe)
      pheatmap(
        prob_matrix_gbfe,
        fontsize = 15,
        cluster_rows = FALSE,
        cluster_cols = FALSE,
        color = viridis(100),
        display_numbers = TRUE,
        number_format = "%.2f",
        main = "Classification Probability (GBFE Features - Elastic Net)"
      )
    }
  }
  
  cat("\nFull-Feature Test Accuracy:", acc_full, "\n")
  if(do_rfe) cat("\nRFE-Optimized Test Accuracy:", acc_opt, "\n")
  if(do_gbfe) cat("\nGBFE-Selected Test Accuracy:", acc_gbfe, "\n")
  
  return(list(
    final_imp_norm_dat    = data,
    best_imputation       = best_imp,
    best_normalization    = best_norm,
    best_caret_model      = best_model,
    full_features_acc     = acc_full,
    top_feature_importances = enet_importances,
    rfe_model             = rfe_results_fixed,
    rfe_selected_features = rfe_selected_feats,
    gbfe_model            = final_enet_gbfe,
    gbfe_selected_feats   = gbfe_selected_feats
  ))
}

```

## Loading reference databases
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Read in suspect screening result from Eric // PlasticMAP // PlastChem database
sus_scr_eric_fries <- readxl::read_xlsx(path = paste0(getwd(), "/data/PMT Suspect Screening Plastics Results EF.xlsx"), skip = 1)
plasticmap <- readxl::read_xlsx(path = paste0(getwd(), "/data/PlasticMAP_S1_210611.xlsx"), sheet = "S8 - PlasticMAP")
plastchem <- readxl::read_xlsx(path = paste0(getwd(), "/data/plastchem_db_v1.0.xlsx"), sheet = "Full database")

# Remove all legacy empty rows from Eric's Excel file 
sus_scr_eric_fries <- sus_scr_eric_fries[rowSums(is.na(sus_scr_eric_fries)) != (ncol(sus_scr_eric_fries) - 1), ]

# Massbank of North America (MoNA-LC-MS-Spectra)
# library(jsonlite)
# mona_lcms <- jsonlite::read_json(path = paste0(getwd(), "/data/MoNA-export-LC-MS_Spectra.json"), 
#                                  flatten = TRUE, 
#                                  simplifyVector = TRUE)
# 
# NORMAN-SLE
# norman_sle <- read_xlsx(path = paste0(getwd(), "/data/norman_sle_susdat_2024-01-12-082351_ed.xlsx"))

```

# Matching ML Classifier's Top contributions

## Get unique RT and m/z of the top contributions
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
all_unique <- data.frame(comp = run_rf_analysis_list_SB_and_ENV_train$hplc_SB_and_ENV_train$top_importance_results$top_100$selected_features)

# Adding RT and molecular ions info of significant compounds:
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc$Feature == all_unique[row,])
  
  mean_mz <- c(mean_mz, round(mean(hplc[idx,]$m.z), 4)) 
}

all_unique$mean_mz <- mean_mz
```

## A. EF's Suspect Screening 

### Find in all_unique the compounds that match with suspect hit list in all_unique with molecular ions in the range of +- 0.0005
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
df_list <- list()
i <- 1

for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the feature fall into the windows +-0.0005 of suspect, then fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  # If not found any match in Suspect screening list then next
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
View(all_matching_sus_scr)
```

### Stacked bar plot of compound match with EF's Suspect Screening 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Replace all the compounds in plot_df with suspect screening matches 
joined <- left_join(run_rf_analysis_list_SB_and_ENV_train$hplc_SB_and_ENV_train$final_imp_norm_dat %>%
                      pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Values") %>%
                      dplyr::select(Plastic_type, Feature, Values), 
                    unique(all_matching_sus_scr %>%
                             dplyr::select(Feature, `Suspect Name`)), by = "Feature") %>%
  dplyr::filter(!is.na(`Suspect Name`)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, `Suspect Name`, Plastic_type) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  dplyr::mutate(Plastic_type = gsub(x=Plastic_type, pattern = ".", replacement=" ", fixed = TRUE))

ggplot(data = joined, aes(x = Plastic_type, y = Values, fill = `Suspect Name`)) +
  geom_bar(stat = "identity") +
  labs(title = "", x = "Plastic product types", y = "Peak Height",
       fill = "Top hits with database") +
  theme_minimal(base_size = 25) +
  theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
        axis.ticks.length.x = unit(0, "cm"),
        legend.position = 'bottom',
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) + 
  scale_y_continuous(expand = c(0,0)) +
  guides(fill=guide_legend(ncol=2))
```

# B. PlasticMap (Wiesinger et. al.) 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
df_list <- list()
i <- 1

for (mz in unique(na.omit(plasticmap$Mass))) {
  # if the of the feature fall into the windows +-0.0005 of suspect in database, then fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (mz + 0.001) & all_unique$mean_mz >= (mz - 0.001))
  # If not found any match in Suspect screening list then next
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- plasticmap[which(plasticmap$Mass == mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr_plasticmap <- bind_rows(df_list)
View(all_matching_sus_scr_plasticmap[, c(2,3,23,25)])
```

### Stacked bar plot of compound match with PlasticMap 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Replace all the compounds in plot_df with suspect screening matches 
joined <- left_join(df_percentage %>% 
                      pivot_longer(cols = 2:ncol(.), names_to = "Feature", values_to = "Values") %>%
                      dplyr::select(Subcategory, Feature, Values), 
                    all_matching_sus_scr_plasticmap %>%
                      dplyr::select(Feature, `Substance Name`), by = "Feature") %>%
  dplyr::filter(!is.na(`Substance Name`)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, `Substance Name`, Subcategory) %>%
  dplyr::summarise(across(Values, base::mean))

ggplot(data = joined, aes(x = Subcategory, y = Values, fill = `Substance Name`)) +
  geom_bar(stat = "identity") +
  labs(title = "", x = "Plastic product types", y = "Normalized Peak Height (%)",
       fill = "Top hits with database") +
  theme_minimal(base_size = 15) +
  theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
        axis.ticks.length.x = unit(0, "cm"),
        # legend.text = element_text(size = 18),
        # legend.title = element_text(size = 20),
        legend.position = "bottom",
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) + 
  scale_y_continuous(expand = c(0,0)) +
  guides(fill=guide_legend(ncol=2))
```

### Extract information about Functions and Industrial sectors in which plastics including this chemical suspects might be used
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
writexl::write_xlsx(all_matching_sus_scr_plasticmap[, c(3,23,25)], "plasticmap_suspect_function_industrialsector.xlsx")
```


# C. PlastChem (Hans Peter Arp et. al.) 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
df_list <- list()
i <- 1

for (mz in unique(na.omit(plastchem$monoisotopic_mass))) {
  # if the of the feature fall into the windows +-0.0005 of suspect in database, then fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (mz + 0.001) & all_unique$mean_mz >= (mz - 0.001))
  # If not found any match in Suspect screening list then next
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- plastchem[which(plastchem$monoisotopic_mass == mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr_plastchem <- bind_rows(df_list)
View(all_matching_sus_scr_plastchem[, c(6,212,217, 234)])
```

### Extract information about Functions and Industrial sectors in which plastics including this chemical suspects might be used
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
writexl::write_xlsx(all_matching_sus_scr_plastchem[, c(6,212,217)], "plastchem_suspect_function_industrialsector.xlsx")
```

### Stacked bar plot of compound match with PlastChem 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Replace all the compounds in plot_df with suspect screening matches 
joined <- left_join(df_percentage %>% 
                      pivot_longer(cols = 2:ncol(.), names_to = "Feature", values_to = "Values") %>%
                      dplyr::select(Subcategory, Feature, Values), 
                    unique(all_matching_sus_scr_plastchem %>%
                      dplyr::select(Feature, pubchem_name)), by = "Feature") %>%
  dplyr::filter(!is.na(pubchem_name)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, pubchem_name, Subcategory) %>%
  dplyr::summarise(across(Values, base::mean))

ggplot(data = joined, aes(x = Subcategory, y = Values, fill = pubchem_name)) +
  geom_bar(stat = "identity") +
  labs(title = "", x = "Plastic product types", y = "Normalized Peak Height (%)",
       fill = "Top hits with database") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
        axis.ticks.length.x = unit(0, "cm"),
        # legend.text = element_text(size = 18),
        # legend.title = element_text(size = 20),
        legend.position = "bottom",
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) + 
  scale_y_continuous(expand = c(0,0)) +
  guides(fill=guide_legend(ncol=2))
```

# Statistical fingerprinting

## Result from ML Classifiers 
### Probability Result table
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Import samples to list
import_samples_to_list <- function(path) {
  setwd(paste0("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic Manuscript 1/ML results_11Dec2024/prob_df", path))
  ml_prob_df_list <- list.files(pattern = '*.xlsx')
  ml_prob_df <- dplyr::bind_rows(purrr::map(ml_prob_df_list, read_xlsx))
  return(ml_prob_df)
}

#GC
gc_ml_prob_df <- import_samples_to_list(path = "/GC") %>% mutate(technique = "GC")
#HPLC
hplc_ml_prob_df <- import_samples_to_list(path = "/HPLC") %>% mutate(technique = "HPLC")
#ICP
icp_ml_prob_df <- import_samples_to_list(path = "/ICP") %>% mutate(technique = "ICP")
#GC_ICP
gc_icp_ml_prob_df <- import_samples_to_list(path = "/GC_ICP")%>% mutate(technique = "GC_ICP")
#HPLC_ICP
hplc_icp_ml_prob_df <- import_samples_to_list(path = "/HPLC_ICP")%>% mutate(technique = "HPLC_ICP")
#GC_HPLC
gc_hplc_ml_prob_df <- import_samples_to_list(path = "/GC_HPLC")%>% mutate(technique = "GC_HPLC")
#GC_HPLC_ICP
gc_hplc_icp_ml_prob_df <- import_samples_to_list(path = "/GC_HPLC_ICP")%>% mutate(technique = "GC_HPLC_ICP")
```

### Balanced accuracy scores from ML models
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Import samples to list
import_accuracy_score <- function(path) {
  setwd(paste0("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic Manuscript 1/ML results_11Dec2024/accuracy_score/", path))
  ml_accuracy_list <- list.files(pattern = '\\.xlsx$')
  ml_accuracy <- ml_accuracy_list %>%
  map_dfr(~ {
    data <- read_xlsx(.x)
    data$File <- .x  # Add a new column with the filename
    data
  }) %>% 
    mutate(technique = path) %>% 
    mutate(algo = ifelse(str_detect(File, "LGBM"), "LGBM",
                         ifelse(str_detect(File, "RF"), "RF",
                                ifelse(str_detect(File, "MLR"), "MLR",
                                       ifelse(str_detect(File, "SVM"), "SVM",
                                              ifelse(str_detect(File, "XGB"), "XGB", 
                                                     ifelse(str_detect(File, "CNB"), "CNB", 
                                                            ifelse(str_detect(File, "KNN"), "KNN", "GBM")))))))) %>%
    dplyr::select(-File)
  return(ml_accuracy)
}

#GC
gc_ml_accuracy <- import_accuracy_score(path = "GC")
#HPLC
hplc_ml_accuracy <- import_accuracy_score(path = "HPLC")
#ICP
icp_ml_accuracy <- import_accuracy_score(path = "ICP")
#GC_ICP
gc_icp_ml_accuracy <- import_accuracy_score(path = "GC_ICP") %>% mutate(technique = "GC + ICP")
#HPLC_ICP
hplc_icp_ml_accuracy <- import_accuracy_score(path = "HPLC_ICP") %>% mutate(technique = "HPLC + ICP")
#GC_HPLC
gc_hplc_ml_accuracy <- import_accuracy_score(path = "GC_HPLC") %>% mutate(technique = "GC + HPLC")
#GC_HPLC_ICP
gc_hplc_icp_ml_accuracy <- import_accuracy_score(path = "GC_HPLC_ICP") %>% mutate(technique = "GC + HPLC + ICP")

# Combine all df
all_accuracy <- rbind(gc_ml_accuracy, hplc_ml_accuracy, icp_ml_accuracy, 
                      gc_icp_ml_accuracy, hplc_icp_ml_accuracy, 
                      gc_hplc_ml_accuracy, gc_hplc_icp_ml_accuracy) %>% 
  group_by(algo, technique) %>% 
  summarize(mean_accuracy = mean(`Balanced Accuracy`),
            accuracy_std_dev = sd(`Balanced Accuracy`))

all_accuracy$algo <- factor(all_accuracy$algo, levels = c("SVM", "LGBM", "XGB", "MLR", "GBM", "RF"))

# Plot
ggplot(data = all_accuracy, aes(x = algo, y = mean_accuracy, fill = technique)) +
  geom_bar(position = position_dodge(width = 0.9), stat = "identity") +
  geom_errorbar(data = all_accuracy, aes(x = algo,
                                         ymin = mean_accuracy - accuracy_std_dev,
                                         ymax = mean_accuracy + accuracy_std_dev, 
                                         group = technique), position = "dodge", 
                linewidth = 0.15, alpha = 1, size = 1.5) +
  scale_fill_manual(values = c("#a6cee3",
                               "#1f78b4",
                               "#b2df8a",
                               "#33a02c", 
                               "#fb9a99",
                               "#e31a1c",
                               "#fdbf6f")) +
  labs(title = "", x = "Algorithms", y = "Classification accuracy",
       fill = "Datasets") +
  theme_classic(base_size = 30) +
  theme(axis.text.x = element_text(angle = 90, vjust =0.5, color = "black", face = "bold"),
        axis.ticks.length.x = unit(0, "cm")) +
  scale_y_continuous(breaks = seq(from = 0, to = 1, by =.1), expand = c(0,0))
```

### Top 10 important features from supervised ML
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Import samples to list
import_accuracy_score <- function(path) {
  setwd(paste0("C:/Users/huyng/OneDrive - Toronto Metropolitan University (TMU)/Microplastic/Microplastic Manuscript 1/microplastic manuscript 1 - ML results 13thNov2024/feature_importance/", path))
  ml_feature_imp_list <- list.files(pattern = '\\.xlsx$')
  ml_feature_imp <- ml_feature_imp_list %>%
  map_dfr(~ {
    data <- read_xlsx(.x)
    data$File <- .x  # Add a new column with the filename
    data
  }) %>% 
    mutate(technique = path) %>% 
    mutate(algo = ifelse(str_detect(File, "LGBM"), "LGBM",
                         ifelse(str_detect(File, "RF"), "RF",
                                ifelse(str_detect(File, "MLR"), "MLR",
                                       ifelse(str_detect(File, "SVM"), "SVM",
                                              ifelse(str_detect(File, "XGB"), "XGB", "GBM")))))) %>%
    dplyr::select(-File)
  return(ml_accuracy)
}


```

### Bar plot with error bar of Standard deviation from 10 iterations
```{r , echo=FALSE, warning = FALSE, message=FALSE}
df <- readxl::read_xlsx(path = "ML classifier performance.xlsx") 

df <- df %>%
  pivot_longer(cols = 3:ncol(.), names_to = "rep", values_to = "BA") %>%
  group_by(Technique, 
           Classifier) %>%
  summarise(BA_mean = mean(BA),
            BA_sd = sd(BA))

df$Classifier <- factor(df$Classifier, levels = c("SVC", "MLR", "LGBM", "XGB", "GBM", "RF"))

ggplot(data = df, aes(x = Classifier, y = BA_mean, fill = Technique)) +
  geom_bar(position = position_dodge(width = 0.9), stat = "identity") +
  geom_errorbar(aes(ymin = BA_mean - BA_sd,
                    ymax = BA_mean + BA_sd), position = 'dodge', linewidth = 0.15, alpha = 1, size = 1.5) +
  labs(title = "", x = "Machine Learning Classifiers", y = "Balanced Accuracy (%)",
       fill = "Data combination") +
  scale_fill_manual(values = c("#a6cee3",
                               "#1f78b4",
                               "#b2df8a",
                               "#33a02c", 
                               "#fb9a99",
                               "#e31a1c",
                               "#fdbf6f")) +
  theme_classic(base_size = 30) +
  theme(axis.text.x = element_text(angle = 45, hjust =1.25, color = "black", face = "bold"),
        axis.ticks.length.x = unit(0, "cm")) +
  scale_y_continuous(expand = c(0,0))
```

# ================================================== 
# Unused codes
# ==================================================

# Exploratory data analysis
# What type of products/product categories have most / highest amounts of compounds?
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Product categories with highest number of compounds (non-NA values)
# Row count for each row and then for each plastic categories, calculate mean of occurence and rank between categories
summary_table <- copy(merged_df)
summary_table$non_na_count <- apply(summary_table[, 3:ncol(summary_table)], 1, function(x) round(sum(!is.na(x)), 0))
summary_df <- summary_table %>%
  group_by(Subcategory) %>% 
  summarize(mean_non_na = mean(non_na_count, na.rm = TRUE)) %>% 
  arrange(desc(mean_non_na))

View(summary_df)

# Bar plot to show which product has highest number of compounds
df <- summary_table %>%
  group_by(technique, Subcategory) %>%
  summarise(non_na_count_mean = round(mean(non_na_count),0),
            non_na_count_sd = round(sd(non_na_count), 0))

ggplot(data = df, aes(x = Subcategory, y = non_na_count_mean, fill = technique)) +
  geom_bar(position = position_dodge(width = 0.9), stat = "identity") +
  geom_errorbar(aes(ymin = non_na_count_mean - non_na_count_sd,
                    ymax = non_na_count_mean + non_na_count_sd), position = 'dodge', 
                linewidth = 0.15, alpha = 1, size = 1.5) +
  labs(title = "", x = "Plastic categories", y = "Number of compounds",
       fill = "Analytical technique") +
  theme_classic(base_size = 25) +
  theme(axis.text.x = element_text(angle = 90, hjust =1, vjust = 0.5, color = "black", face = "bold"),
        axis.ticks.length.x = unit(0, "cm")) +
  scale_y_continuous(expand = c(0,0))
```

# Compound that are most commonly found across all samples?
#### For GC data
```{r , echo=FALSE, warning = FALSE, message=FALSE}
summary_df <- merged_df %>%
  dplyr::select(-c(1,2)) %>%  # Exclude the label column
  summarise_all(list(
    non_na_non_zero_sum = ~ sum(!is.na(.) & . != 0),  # Count non-NA and non-zero values
    non_na_non_zero_avg = ~ mean(.[!is.na(.) & . != 0], na.rm = TRUE)  # Average of non-NA and non-zero values
  )) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("feature", ".value"),
    names_sep = "_non_na_non_zero_"
  ) %>% 
  arrange(desc(sum), desc(avg))

top_gc_plot_df <- gc %>% dplyr::filter(Feature %in% summary_df[1:10,]$feature)

summary_mz_rt <- top_gc_plot_df %>% 
  group_by(Feature) %>%
  summarize(mean_mz = round(mean(m.z, na.rm=TRUE), 3), mean_rt = round(mean(RT, na.rm = TRUE), 4))

top_gc_plot_df <- full_join(top_gc_plot_df, summary_mz_rt, by = "Feature")
top_gc_plot_df <- top_gc_plot_df %>% mutate(New_feature_name = paste0(Feature, "_m/z: ", mean_mz, "_RT: ", mean_rt))

# top_gc_plot_df <- top_gc_plot_df %>% 
#   group_by(Feature) %>% 
#   summarize(mean_mz = mean(m.z, na.rm=TRUE))
# 
# for (comp in unique(top_gc_plot_df$Feature)) {
#   idx <- which(top_gc_plot_df$mean_mz <= (mz + 0.005) & top_gc_plot_df$mean_mz >= (mz - 0.005))
#   print(plastchem[which(plastchem$monoisotopic_mass == mz),])
# }

ggplot(data = top_gc_plot_df, aes(x = Subcategory, y = Values, fill = New_feature_name)) +
  geom_bar(position = "dodge", stat = "identity") +
  labs(title = "ATD-GC-MS data", x = "Plastic product types", y = "Signal Intensity",
       fill = "Top ATD-GC-MS compounds") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

####For HPLC data
```{r , echo=FALSE, warning = FALSE, message=FALSE}
summary_df <- merged_df %>%
  dplyr::select(-c(1,2)) %>%  # Exclude the label column
  summarise_all(list(
    non_na_non_zero_sum = ~ sum(!is.na(.) & . != 0),  # Count non-NA and non-zero values
    non_na_non_zero_avg = ~ mean(.[!is.na(.) & . != 0], na.rm = TRUE)  # Average of non-NA and non-zero values
  )) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("feature", ".value"),
    names_sep = "_non_na_non_zero_"
  ) %>% 
  arrange(desc(sum), desc(avg))

top_hplc_plot_df <- hplc %>% dplyr::filter(Feature %in% summary_df[1:10,]$feature)

summary_mz_rt <- top_hplc_plot_df %>% 
  group_by(Feature) %>%
  summarize(mean_mz = round(mean(m.z, na.rm=TRUE), 3), mean_rt = round(mean(RT, na.rm = TRUE), 4))

top_hplc_plot_df <- full_join(top_hplc_plot_df, summary_mz_rt, by = "Feature")
top_hplc_plot_df <- top_hplc_plot_df %>% mutate(New_feature_name = paste0(Feature, "_m/z: ", mean_mz, "_RT: ", mean_rt))

# top_gc_plot_df <- top_gc_plot_df %>% 
#   group_by(Feature) %>% 
#   summarize(mean_mz = mean(m.z, na.rm=TRUE))
# 
# for (comp in unique(top_gc_plot_df$Feature)) {
#   idx <- which(top_gc_plot_df$mean_mz <= (mz + 0.005) & top_gc_plot_df$mean_mz >= (mz - 0.005))
#   print(plastchem[which(plastchem$monoisotopic_mass == mz),])
# }

ggplot(data = top_hplc_plot_df, aes(x = Subcategory, y = Values, fill = New_feature_name)) +
  geom_bar(position = "dodge", stat = "identity") +
  labs(title = "HPLC-QToF-MS data", x = "Plastic product types", y = "Signal Intensity",
       fill = "Top HPLC-QToF-MS compounds") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# mz <- unique(top_hplc_plot_df[which(top_hplc_plot_df$Feature == "Compound_RT1_20612.HPLCTOFMS"),]$mean_mz)
# which(plastchem$monoisotopic_mass <= (mz + 0.005) & plastchem$monoisotopic_mass >= (mz - 0.005))

```

####For ICP data
```{r , echo=FALSE, warning = FALSE, message=FALSE}
summary_df <- merged_df %>%
  dplyr::select(-c(1,2)) %>%  # Exclude the label column
  summarise_all(list(
    non_na_non_zero_sum = ~ sum(!is.na(.) & . != 0),  # Count non-NA and non-zero values
    non_na_non_zero_avg = ~ mean(.[!is.na(.) & . != 0], na.rm = TRUE)  # Average of non-NA and non-zero values
  )) %>%
  pivot_longer(
    cols = everything(),
    names_to = c("feature", ".value"),
    names_sep = "_non_na_non_zero_"
  ) %>% 
  arrange(desc(sum), desc(avg))

top_metal_plot_df <- icp %>% dplyr::filter(Feature %in% summary_df[1:10,]$feature)

# Remove everything after the square bracket
top_metal_plot_df$New_feature_name <- str_extract(top_metal_plot_df$Feature, "^[^\\[]+")

# Trim any trailing whitespace
top_metal_plot_df$New_feature_name <- str_trim(top_metal_plot_df$New_feature_name)

# Use a regular expression to remove the numerical parts
# Explanation of Regular Expression:
#
# \\d+ matches one or more digits.
# *(-> *\\d+)? * optionally matches the pattern -> digits with any number of spaces around it.
# gsub function is used to find and replace the matching parts in the string with an empty string ("")
top_metal_plot_df$New_feature_name <- gsub("\\d+ *(-> *\\d+)? *", "", top_metal_plot_df$New_feature_name)

ggplot(data = top_metal_plot_df, aes(x = Subcategory, y = Values, fill = New_feature_name)) +
  geom_bar(position = "dodge", stat = "identity") +
  labs(title = "ICP-MS Trace metal data", x = "Plastic product types", y = "Signal Intensity",
       fill = "Top trace metals") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

# What are the main features in USE and USSB? 
### Top 10 features with highest average signal intensity
```{r , echo=FALSE, warning = FALSE, message=FALSE}
label_column <- "Source"

df <- df_percentage %>% dplyr::select(-c("Subcategory"))

# Compute the top 10 features for each label
top_features <- lapply(unique(df[[label_column]]), function(label) {
  # Filter data for the current label
  label_data <- df[df[[label_column]] == label, ]
  
  # Compute mean signal intensity for each feature, ignoring NA
  mean_signals <- colMeans(label_data[, -which(names(df) == label_column)], na.rm = TRUE)
  
  # Sort features by mean signal intensity in descending order
  sorted_features <- sort(mean_signals, decreasing = TRUE)
  
  # Select the top 10 features
  top_10 <- head(sorted_features, 10)
  
  # Return as a dataframe with label information
  data.frame(Label = label, Feature = names(top_10), Average_Normalized_Signal_Intensity = top_10)
})

# Combine results into a single dataframe
top10_feature_signal_intensity <- do.call(rbind, top_features)
rownames(top10_feature_signal_intensity) <- NULL
# Print the results
View(top10_feature_signal_intensity)
```

### Top 10 features with highest frequency of occurence in USE vs USSB
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Count occurrences of non-zero values for each feature by label
top_features_highest_occurence_source <- df_percentage %>%
  dplyr::select(-Subcategory) %>%
  group_by(Source) %>%
  summarise(across(where(is.numeric), ~ sum(.x != 0)/nrow(df_percentage)*100)) %>%
  pivot_longer(-Source, names_to = "Feature", values_to = "Percentage_of_Occurence") %>%
  arrange(Source, desc(Percentage_of_Occurence)) %>%
  group_by(Source) %>%
  slice_head(n = 10) # Select top 10 features for each label

# View the result
print(top_features_highest_occurence_source)
```

### Boxplot of plastic products
```{r , echo=FALSE, warning = FALSE, message=FALSE}
plotdat <- hplc_icp_subcat[[1]] %>%
  pivot_longer(-c(Subcategory, technique), names_to = "Compounds", values_to = "Values")
ggplot(data = plotdat) + 
  stat_boxplot(aes(x = Subcategory, 
                   y = Values)) +
  labs(x = "Plastic products", 
       y = "Normalized peak area") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

### Boxplot of Top 10 compounds with highest Concen. in each sample?
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# For each row, Extract Top 10 compound with highest concen.
top <- c()
dat <- hplc_subcat[[3]][,-2] %>%
# For each product type, calculate average of all compounds
  group_by(Subcategory) %>%
  summarise_all(.funs = mean, na.rm = TRUE)

# Select top 10 compound with highest abundance in each plastic product
top <- list()
for (subcat in unique(dat$Subcategory)) {
  top[[subcat]] <- colnames(dat)[order(as.numeric(dat %>% filter(., Subcategory %in% subcat)), decreasing = TRUE)[1:10]]
}

dat2 <- dat %>% pivot_longer(cols = 2:ncol(.), names_to = "Feature", values_to = "Values")

top_list <- list()
i <- 1
for (subcat in unique(dat2$Subcategory)) {
  top_list[[i]] <- dat2 %>% filter(., Subcategory %in% subcat) %>% filter(., Feature %in% top[[i]])
  i <- i + 1
}

plotdat <- bind_rows(top_list)

ggplot(data = plotdat) +
  geom_boxplot(aes(x = Subcategory, y = Values, fill = Feature)) +
  # geom_errorbar(data = data_summary, aes(x = NewFile, ymin = mean - sd, 
  #                                        ymax = mean + sd, group = Metal_component), 
  #               position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "", x = "Plastic product types", y = "Normalized Abundance (%)",
       fill = "Top features") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### What are the Top 10 metals with highest Concen. in each sample?

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
newmetal <- icp_list[[1]] %>% 
  tidyr::pivot_wider(names_from = Feature, values_from = Values) %>% 
  column_to_rownames(., var = "File")

# Replace all NA in data frame with 0
newmetal[is.na(newmetal)] <- 0

# For each sample, Extract Top 10 metal with highest concen.
top_metal <- list()
for (row in 1:nrow(newmetal)) {
  top_metal[[row]] <- colnames(newmetal)[order(as.numeric(newmetal[row,]), decreasing = TRUE)[1:10]]
}

topmetal_list <- list()
i <- 1
for (file in unique(icp_list[[1]]$File)) {
  topmetal_list[[i]] <- icp_list[[1]] %>% filter(., File %in% file) %>% filter(., Feature %in% top_metal[[i]])
  i <- i + 1
}

top_metal_plot_df <- bind_rows(topmetal_list) %>%
   mutate(Category = ifelse(str_detect(File, "USE-01"), "Polystyrene food packaging", 
                                  ifelse(str_detect(File, "USE-02"), "Mixed plastic waste",
                                         ifelse(str_detect(File, "USE-03"), "Plastic drinking straws", 
                                                ifelse(str_detect(File, "USE-05"), "Cigarettes tips", 
                                                       ifelse(str_detect(File, "USE-06"), "Face masks", 
                                                              ifelse(str_detect(File, "USE-07"), "Food containers", 
                                                                     ifelse(str_detect(File, "USE-09"), "Food wrappers",
                                                                            ifelse(str_detect(File, "USE-11"), "Plastic toy balls", 
                                                                                   ifelse(str_detect(File, "USE-12"), "Ziploc bags", 
                                                                                          ifelse(str_detect(File, "USE-13"), "Food packaging",
                                                                                                 ifelse(str_detect(File, "USE-14"), "Bottle caps", "Fishing bait trays"))))))))))))
                                                              
### Stacked bar plot of values of  Top 10 trace metal for each metal data sample (x-axis: sample name; y-axis: metal values) --------------------------------------

# data_summary <- top10_plot %>%
#   group_by(NewFile, Metal_component) %>%
#   summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = top_metal_plot_df, aes(x = Category, y = Values, fill = Feature)) +
  geom_bar(stat = "identity") +
  # geom_errorbar(data = data_summary, aes(x = NewFile, ymin = mean - sd, 
  #                                        ymax = mean + sd, group = Metal_component), 
  #               position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "ICP-MS Trace metal data", x = "Plastic product types", y = "Normalized Abundance (%)",
       fill = "Top trace metals") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### For each plastic type, What are compounds that only appear in USSB and not USE?

```{r, echo=FALSE, warning = FALSE}
# compdiff <- list()
for (plt in unique(adjusted_df$plastic_type)) {
  # compdiff[[plt]] <- 
  print(plt)
  print(base::setdiff(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

#### For each plastic product, What are the compounds that consistently appear in both USSB & USE?

```{r, echo=FALSE, warning = FALSE}
for (plt in unique(adjusted_df$plastic_type)) {
  print(plt)
  print(base::intersect(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

### Retention time and mass-to-charge of top 10 most significant features from ATD-GC-MS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
top10 <- unique((gc_subcat_wilcox[[6]] %>% arrange(adjusted_pvalue))[1:10,]$comp)

# Plot bar plot of top 10 most significant features
# plotdat <- gc_subcat[[2]] %>%
#   select(c(Subcategory, top10)) %>%
#   pivot_longer(cols = 2:ncol(.), names_to = "Features", values_to = "Values")
# 
# data_summary <- plotdat %>%
#   group_by(Subcategory, Features) %>%
#   summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')
# 
# ggplot(data = data_summary, aes(x = Subcategory, y = mean, fill = Features)) +
#   geom_bar(stat = "identity", position = 'dodge') +
#   geom_errorbar(aes(ymin = mean - sd,
#                     ymax = mean + sd),
#                 position = position_dodge(width = 0.9), width = 0.25) +
#   labs(x = "Plastic product group", y = "Normalized Concentration") +
#   theme_classic(base_size = 30) +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Scatter plot of Retention time and mass-to-charge of top 10 most significant features
rtmz <- gc_list[[1]] %>%
  filter(Feature %in% colnames(gc_subcat[[1]][, 3:ncol(gc_subcat[[1]])])) %>%
  select(Feature, RT, m.z) %>% 
  group_by(Feature) %>% 
  summarise_all(.funs = mean, na.rm = TRUE)

# Merge rtmz with pvalues output
plotdat <- right_join(x = rtmz,
                     y = gc_subcat_wilcox[[6]] %>% filter(., adjusted_pvalue < 0.05), 
                     by = 'Feature')

ggplot(plotdat, aes(x = RT, y = m.z, colour = comparison_pair)) +
  geom_point(size = 2.5) +
  labs(x = "Retention Time", y = "Mass-to-charge (m/z)", colour = "Comparison pair") +
  theme_minimal(base_size = 30) +
  guides(colour = guide_legend(override.aes = list(size = 10))) # Adjust size of the points representing each label on the legend
```

### Retention time and mass-to-charge of Top 10 GC//HPLC features that most important in RandomForest and Gradient Boosting machine

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
top10_rf_gbm <- read.csv(paste0(getwd(), "/toprf_gbm.csv"))

plotdat <- gc_list[[1]] %>% # hplc_list[[1]]
  # filter(Feature %in% colnames(hplc_subcat[[1]][, 3:ncol(hplc_subcat[[1]])])) %>%
  filter(., Feature %in% top10_rf_gbm$Feature) %>%
  select(Feature, RT, m.z) %>% 
  group_by(Feature) %>% 
  summarise_all(.funs = mean, na.rm = TRUE)

ggplot(plotdat, aes(x = RT, y = m.z, colour = Feature)) +
  geom_point(size = 5) +
  labs(x = "Retention Time", y = "Mass-to-charge (m/z)", colour = "Comparison pair") +
  theme_minimal(base_size = 30) +
  guides(colour = guide_legend(override.aes = list(size = 10))) # Adjust size of the points representing each label on the legend

# Plot bar plot of top 10 most significant features
plotdat <- df_percentage %>%
  select(c(Subcategory, top10_rf_gbm$Feature)) %>%
  pivot_longer(cols = 2:ncol(.), names_to = "Features", values_to = "Values")

data_summary <- plotdat %>%
  group_by(Subcategory, Features) %>%
  summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = data_summary, aes(x = Subcategory, y = mean, fill = Features)) +
  geom_bar(stat = "identity", position = 'dodge') +
  geom_errorbar(aes(ymin = mean - sd,
                    ymax = mean + sd),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Plastic product group", y = "Normalized Concentration") +
  theme_classic(base_size = 30) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Matching HPLC Wilcoxon significant compounds from HPLC suspect screening hits (EF)

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sus_scr <- readxl::read_xlsx(path = paste0(getwd(), "/data/PMT Suspect Screening Plastics Results EF.xlsx"), skip = 1)

# Remove all legacy empty rows from Eric's Excel file 
sus_scr <- sus_scr[rowSums(is.na(sus_scr)) != (ncol(sus_scr) - 1), ]
```

### 1. With HCA

(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from HCA clustering of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - Zeroimputed: AMong 2737 significant compounds -> we found 13 compounds that matches with EF's Suspect Screening
 [1] "4-ethenyl-cyclohexene"                             "4-Aminophenol"                                     "1,2,3,4-tetrahydro-naphthalene"                   
 [4] "1,1'-oxybis[2-methoxy-ethane]"                     "Benzothiazole"                                     "Acetaminophen (Paracetemol)"                      
 [7] "1-methyl-1-phenylethylhydroperoxide"               "Hexahydro-1,3-isobenzofurandione"                  "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"         
[10] "2-amino-5-methyl-benzenesulfonic acid"             "Dibutyl ester phosphoric acid"                     "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[13] "2-(2H-benzotriazol-2-yl)-4-methyl-phenol"  


```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO130-159_18Dec2023.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO151-162_06Dec2023.xlsx")
p3 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO161-172_06Dec2023.xlsx")

sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Inspect which subdf has maximum number of significant compounds
lapply(sus_pol_combined1, dim)
# Get the no of unique compounds in that subdf
length(unique(sus_pol_combined1[[]]$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unique(sus_pol_combined1[[71]]$comp))

all_unique <- data.frame(comp=unique(c(unique1))) %>%
  # get rid off all GC compounds if HPLC is in combination with other data sources.
  filter(., str_detect(comp,"HPLCTOFMS"))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  # print(idx)
  if (base::identical(idx, integer(0))) {
    next
  } else {
    # print(sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),])
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Polymer_Type_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

### 2. Without HCA
#### 2.1 Polymer Type level
[Update 17 Jan 2024:]
- For HPLC only data, we found the same 6 Suspects as with the "Category" level (Section 2.2 below)
[1] "4-Aminophenol"                       "2,6-dimethyl-benzenamine"           
[3] "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide"
[5] "TEP / Triethyl phosphate"            "N,N'-diphenyl-guanidine" 

- For GC + HPLC data, we found 4 suspects, all 4 of them are nested in the HPLC-only case ==> GC+HPLC found less suspect hit than HPLC-only data:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"   
[4] "N,N'-diphenyl-guanidine"

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_polymer_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Polymer_Type_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

#### 2.2 Category level
(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from "Category" of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - MO method: Among 24 significant compounds -> we found 3 compounds that matches with EF's Suspect Screening
 "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"
 
2. HPLC only - Zeroimputed: Among 664 significant compounds -> we found 11 compounds that matches with EF's Suspect Screening
[1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
 [4] "1,2,3,4-tetrahydro-naphthalene"                    "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "TEP / Triethyl phosphate"                          "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"         
[10] "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]" "2-(2H-benzotriazol-2-yl)-4-methyl-phenol"

3. HPLC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"      

4. HPLC + ICP - zeroimputed method:
 [1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
 [4] "1,2,3,4-tetrahydro-naphthalene"                    "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"          "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[10] "2-(2H-benzotriazol-2-yl)-4-methyl-phenol" 

5. HPLC + GC - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"   

6. HPLC + GC - Zeroimputed method:
 [1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1H-Benzotriazole"                                 
 [4] "1,3,5-Triazine-2,4,6-triamine"                     "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "TEP / Triethyl phosphate"                          "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[10] "Oxybenzone (Benzophenone-3)" 

7. HPLC + GC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"    

8.HPLC + GC + ICP - zeroimputed method:
[1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
[4] "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"               "Hexahydro-1,3-isobenzofurandione"                 
[7] "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"

[Update 17 Jan 2024:]
- For HPLC only data, we found also 6 suspect hits that match with Eric Fries's suspect screening excel file:
[1] "4-Aminophenol"                       "2,6-dimethyl-benzenamine"           
[3] "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide"
[5] "TEP / Triethyl phosphate"            "N,N'-diphenyl-guanidine" 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Zeroimputed_HPLCbenchmarksremoved/Wilcoxon_test_COMBINED GC_ICP_by_Subcategory_NA imputed_zeroimputed_18Jan2024.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))
lapply(sus_pol_combined1, dim)

# Unlisting and then finding unique values across all data frames
unique1 <- unique(sus_pol_combined1[[1]]$comp)

all_unique <- data.frame(comp=unique(c(unique1))) %>%
   filter(., str_detect(comp,"HPLCTOFMS"))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,]) # Here we use just hplc_list because when we merge hplc data with other data, the compounds stay the same anyway
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Category_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

#### 2.3 Subcategory level
(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from "Subcategory" of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - MO method: Among 24 total significant compounds -> we found 5 compounds that matches with EF's Suspect Screening
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine"

2. HPLC only - Zeroimputed: There is ZERO significant compound

3. HPLC + ICP - MO method: 
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine" 

4. HPLC + ICP - zeroimputed method: There is ZERO significant compound

5. HPLC + GC - MO method:
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine" 

6. HPLC + GC - Zeroimputed method: There is ZERO significant compound

7. HPLC + GC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine"  

8. HPLC + Gc + ICP - zeroimputed method: There is ZERO significant compound

[UPDATE 17 Jan 2024:] 
- For HPLC only resulted in ZERO significant compounds from Wilcoxon tests so we don't include it in this summary
- For GC + HPLC data, there are 4 compounds but they all ATDGCMS so we also don't include it in this summary 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Zeroimputed_HPLCbenchmarksremoved/Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Subcategory_Missing-observation_zeroimputedbatch_18Jan2024.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))
lapply(sus_pol_combined1, dim)

# Unlisting and then finding unique values across all data frames
unique1 <- unique(sus_pol_combined1[[7]]$comp)

all_unique <- data.frame(comp=unique(c(unique1)))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Category_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```


### ANOVA
(Update 11 April 2024):
- GC (with USE only) - without feature removal - Imputation with Zeros = > not significant
Df    Sum Sq   Mean Sq              F value Pr(>F)
Subcategory 17 1.304e-29 7.672e-31    0.66  0.794
Residuals   14 1.627e-29 1.162e-30 

- HPLC (with USE only)- without feature reduction -> - Imputation with Zeros  => too many variables -> Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restart

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros => not significant
            Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory 11 2.100e-30 1.913e-31   0.471  0.916
Residuals   84 3.411e-29 4.061e-31 

- ICP (with USE only) - without feature reduction -> not significant
            Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory 11 3.567e-30 3.243e-31    1.34  0.266
Residuals   23 5.567e-30 2.420e-31 

- GC + HPLC(with USE only) - without feature reduction -> imputation with Zeros => too many variables ==> Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restart

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros =>  Significant!!
              Df    Sum Sq   Mean Sq F value   Pr(>F)    
Subcategory  18 1.404e-28 7.798e-30   2.745 0.000658 ***
Residuals   109 3.097e-28 2.841e-30                     

Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

- GC + ICP (with USE only) 
*-** without feature reduction - imputation with Zeros => Significant!!
            Df    Sum Sq   Mean Sq F value Pr(>F)  
Subcategory 18 3.097e-30 1.721e-31   2.045  0.025 *
Residuals   48 4.039e-30 8.415e-32                 

Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

*-** WITH feature reduction - imputation with Zeros => NO SIGNIFICANT!
Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory 18 8.440e-31 4.689e-32    0.72  0.774
Residuals   48 3.125e-30 6.510e-32 

- HPLC + ICP (with USE only) 
*-** without feature reduction - imputation with Zeros => too many variables => Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restar

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros =>  NOT Significant
Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory  11 1.949e-29 1.772e-30   1.587  0.111
Residuals   119 1.329e-28 1.117e-30 

- GC+ HPLC + ICP (with USE only) 
*-** without feature reduction - imputation with Zeros => too many variables => Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restar

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros => SIGNIFICANT !!
Df    Sum Sq   Mean Sq F value   Pr(>F)    
Subcategory  18 4.557e-28 2.532e-29   3.804 2.97e-06 ***
Residuals   144 9.584e-28 6.656e-30                     

Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1


```{r , echo=FALSE, warning = FALSE, message=FALSE}
summary(aov(as.formula(paste(paste(setdiff(names(df_percentage), "Subcategory"), collapse = "+"), " ~ Subcategory")), data = df_percentage))
```

## Muli-way ANOVA

(Update 25 Mar 2024): The multiway ANOVA has a problem => Error in if (ssr < 1e-10 * mss) warning("ANOVA F-tests on an essentially perfect fit are unreliable") : 
  missing value where TRUE/FALSE needed
  
``**This warning suggests that the model might be fitting the data too well, to the extent that it's potentially overfitting or encountering numerical precision issues. This can happen when the model fits the data so perfectly that it's essentially a perfect fit, which can lead to unreliable statistical tests.

Then, I tried implement ridge or lasso regression with glmnet() but we have the error:
Error in UseMethod("anova") : 
  no applicable method for 'anova' applied to an object of class "c('elnet', 'glmnet')"
  
So in the end, I gotta find another method outside of ANOVA to statistically test multiple variables. ==> Tried permutation test

```{r, echo=FALSE, warning = FALSE, message=FALSE}
dat <- gc_subcat[[3]]
# remove any Subcategory that have less than 3 sample
dat <- dat[!(dat$Subcategory %in% names(table(dat$Subcategory)[table(dat$Subcategory) < 3])),] %>%
  mutate(Subcategory = factor(Subcategory)) %>%
  select(-c(technique))

testlm <- lm(formula_string, data = dat)
anova(testlm)

explanatory_vars <- colnames(dat)[!colnames(dat) %in% "Subcategory"]

# Create the formula string
formula_string <- paste("Subcategory ~", paste(explanatory_vars, collapse = " + "))
```
# Extract top Normalized Height compounds for each sample from HPLC only data
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
hplc <- df_percentage

# For each sample, Extract Top x compounds with highest concen.
top_hplc <- list()
for (row in 1:nrow(hplc)) {
  # go through each subcategory and Extract Top x compounds with highest concen.
  top_hplc[[hplc[row,]$Subcategory]] <- colnames(hplc)[order(as.numeric(hplc[row,]), decreasing = TRUE)[1:10]]
}

dat <- df_percentage %>% 
  pivot_longer(cols = 2:ncol(.), names_to = "Feature", values_to = "Values")

top_list <- list()
for (i in 1:length(top_hplc)) {
  top_list[[i]] <- dat %>% 
    filter(., Subcategory %in% names(top_hplc[i])) %>% 
    filter(., Feature %in% top_hplc[[i]])
}

top_hplc_plot_df <- bind_rows(top_list)
```

# Matching top Normalized Height compounds with
## A. EF's Suspect Screening 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# comp_of_interest <- c("Compound_RT1_1142.HPLCTOFMS", "Compound_RT1_1215.HPLCTOFMS","Compound_RT1_17262.HPLCTOFMS", "Compound_RT2_12155.HPLCTOFMS", "Compound_RT2_12334.HPLCTOFMS", "Compound_RT2_12348.HPLCTOFMS", "Compound_RT2_3031.HPLCTOFMS", "Compound_RT2_3668.HPLCTOFMS", "Compound_RT2_4159.HPLCTOFMS", "Compound_RT2_6892.HPLCTOFMS")
all_unique <- data.frame(comp = unique(top_hplc_plot_df$Feature))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz
```

### Find in all_unique the compounds that match with suspect hit list in all_unique with molecular ions in the range of +- 0.0005
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
df_list <- list()
i <- 1

for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the feature fall into the windows +-0.0005 of suspect, then fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  # If not found any match in Suspect screening list then next
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
View(all_matching_sus_scr)
```

### Stacked bar plot of values of top Normalized Height compound match with EF's Suspect Screening 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Replace all the compounds in plot_df with suspect screening matches 
joined <- left_join(top_hplc_plot_df %>%
                      dplyr::select(Subcategory, Feature, Values), 
                    all_matching_sus_scr %>%
                      dplyr::select(Feature, `Suspect Name`), by = "Feature") %>%
  dplyr::filter(!is.na(`Suspect Name`)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, `Suspect Name`, Subcategory) %>%
  dplyr::summarise(across(Values, base::mean))

ggplot(data = joined, aes(x = Subcategory, y = Values, fill = `Suspect Name`)) +
  geom_bar(stat = "identity") +
  labs(title = "", x = "Plastic product types", y = "Normalized Peak Height (%)",
       fill = "Top hits with database") +
  theme_minimal(base_size = 25) +
  theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
        axis.ticks.length.x = unit(0, "cm"),
        legend.position = 'bottom',
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) + 
  scale_y_continuous(expand = c(0,0)) +
  guides(fill=guide_legend(ncol=2))

# Beyond- Eric's PMT Suspect Screening
# Filter all the top HPLC compounds that "beyond" the PMT suspect screening list
beyond_pmt <- top_hplc_plot_df %>%
  filter(Feature %notin% unique(joined$Feature)) %>%
  select(Feature, RT, m.z) %>%
  group_by(Feature) %>%
  summarise(across(c(RT, m.z), base::mean))

# Export "beyond-PMT-Suspect Screening" list to excel
writexl::write_xlsx(x=beyond_pmt, path = "Top10_HPLC_data_beyond_pmt_suspect Screening.xlsx")
```

# B. PlasticMap (Wiesinger et. al.) 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
df_list <- list()
i <- 1

for (mz in unique(na.omit(plasticmap$Mass))) {
  # if the of the feature fall into the windows +-0.0005 of suspect in database, then fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (mz + 0.005) & all_unique$mean_mz >= (mz - 0.005))
  # If not found any match in Suspect screening list then next
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- plasticmap[which(plasticmap$Mass == mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr_plasticmap <- bind_rows(df_list)
View(all_matching_sus_scr_plasticmap)
```

### Stacked bar plot of values of top Normalized Height compound match with PlasticMap 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Replace all the compounds in plot_df with suspect screening matches 
joined <- left_join(top_hplc_plot_df %>%
                      dplyr::select(Subcategory, Feature, Values), 
                    all_matching_sus_scr_plasticmap %>%
                      dplyr::select(Feature, `Substance Name`), by = "Feature") %>%
  dplyr::filter(!is.na(`Substance Name`)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, `Substance Name`, Subcategory) %>%
  dplyr::summarise(across(Values, base::mean))

ggplot(data = joined, aes(x = Subcategory, y = Values, fill = `Substance Name`)) +
  geom_bar(stat = "identity") +
  labs(title = "", x = "Plastic product types", y = "Normalized Peak Height (%)",
       fill = "Top hits with database") +
  theme_minimal(base_size = 25) +
  theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
        axis.ticks.length.x = unit(0, "cm"),
        # legend.text = element_text(size = 18),
        # legend.title = element_text(size = 20),
        legend.position = "bottom",
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) + 
  scale_y_continuous(expand = c(0,0)) +
  guides(fill=guide_legend(ncol=2))
```

# C. PlastChem (Hans Peter Arp et. al.) 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
df_list <- list()
i <- 1

for (mz in unique(na.omit(plastchem$monoisotopic_mass))) {
  # if the of the feature fall into the windows +-0.0005 of suspect in database, then fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (mz + 0.005) & all_unique$mean_mz >= (mz - 0.005))
  # If not found any match in Suspect screening list then next
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- plastchem[which(plastchem$monoisotopic_mass == mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr_plastchem <- bind_rows(df_list)
View(all_matching_sus_scr_plastchem)
```

### Stacked bar plot of values of top Normalized Height compound match with PlastChem 
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Replace all the compounds in plot_df with suspect screening matches 
joined <- left_join(top_hplc_plot_df %>%
                      dplyr::select(Subcategory, Feature, Values), 
                    all_matching_sus_scr_plastchem %>%
                      dplyr::select(Feature, pubchem_name), by = "Feature") %>%
  dplyr::filter(!is.na(pubchem_name)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, pubchem_name, Subcategory) %>%
  dplyr::summarise(across(Values, base::mean))

ggplot(data = joined, aes(x = Subcategory, y = Values, fill = pubchem_name)) +
  geom_bar(stat = "identity") +
  labs(title = "", x = "Plastic product types", y = "Normalized Peak Height (%)",
       fill = "Top hits with database") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust =0.5, vjust = 0.25, color = "black", face = "bold"), 
        axis.ticks.length.x = unit(0, "cm"),
        # legend.text = element_text(size = 18),
        # legend.title = element_text(size = 20),
        legend.position = "bottom",
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) + 
  scale_y_continuous(expand = c(0,0)) +
  guides(fill=guide_legend(ncol=2))
```

## STEP 6: Missing value imputation

#### Option 1: Zero
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Go through each row and replace all missing values in that row by zeros
zero_df <- copy(gc_SB_clean)
for (r in 1:nrow(zero_df)) { 
  zero_df[r, which(base::is.na(zero_df[r,]))] <- 0  #0.0001
}
```

#### Option 2: Half-Minimum 

```{r , echo=FALSE, warning = FALSE, message=FALSE}
min_df <- copy(regular_80_rule)
# Go through each row and replace all missing values in that row by global minimum

min_df <- min_df %>% 
  tibble::rownames_to_column(., var="File") %>%
  pivot_longer(cols = 6:ncol(.), names_to = "Feature", values_to = "Values")

# Fill in GC MVs with global GC min (GC-HPLC, GC-ICP, GC-HPLC-ICP)
min_df[which(is.na(min_df$Values) & min_df$technique == "GC"),]$Values <- min(gc$Values)/2

# Fill in HPLC MVs with global HPLC min (GC-HPLC, HPLC-ICP, GC-HPLC-ICP)
min_df[which(is.na(min_df$Values) & min_df$technique == "HPLC"),]$Values <- min(hplc$Values)/2

# Fill in ICP MVs with global ICP min (GC-ICP, HPLC-ICP, GC-HPLC-ICP))
min_df[which(is.na(min_df$Values) & min_df$technique == "ICP"),]$Values <-  min(icp$Values)/2

# Pivot_wider again for normalization
test2 <- min_df %>%
  dplyr::select(File,
                Subcategory,
                Polymer,
                Source,
                Feature,
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Subcategory,
    Polymer,
    Source,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var="File") %>%
  relocate(Subcategory, Source, Polymer, .before = 1)

```

#### Option 3: LOD imputation

* Firstly, if the compound that don't appear at all in Blanks OR if for the compound that only have 1 Blank values -> replace with minimum values of the whole dataset.
* Secondly, if  the compound that have more than 2 blank values -> replace missing values in "Sample" = average of blank data points + 3*sd(blanks). 

```{r , echo=FALSE, warning = FALSE, message=FALSE}
lod_df <- copy(merged_df)
# ATDGCMS RELATED datasets ---------
# A. Replace missing values in compounds that don't appear in Blank at all.

for (i in 1:length(gcms_col1)) {
  # If the feature was removed during removal of >90% NAs, then next
  if (sum(colnames(lod_df) == names(gcms_col1[i])) == 0) {
    next
  }
  # In case the feature occur in blanks and have no missing values, then next
  else if (identical(which(is.na(lod_df[, names(gcms_col1[i])])), integer(0))) {
    next
  } else {
    lod_df[which(is.na(lod_df[, names(gcms_col1[i])])), names(gcms_col1[i])] <- gcms_col1[[i]]
  }
}


# B. Replace missing values in compounds that appear in >= 2 Blanks.

for (i in 1:length(gcms_col2)) {
  # If the feature was removed during removal of >90% NAs, then next
  if (sum(colnames(lod_df) == names(gcms_col2[i])) == 0) {
    next
  }
  # In case the feature occur in blanks and have no missing values, then next
  else if (identical(which(is.na(lod_df[, names(gcms_col2[i])])), integer(0))) {
    next
  } else {
    lod_df[which(is.na(lod_df[, names(gcms_col2[i])])), names(gcms_col2[i])] <- gcms_col2[[i]]
  }
}

# ================================================================================================================================
# HPLCTOFMS RELATED datasets ----------
# A. Replace missing values in compounds that don't appear in Blank at all.

  for (i in 1:length(hplctofms_col1)) {
    # If the feature was removed during removal of >90% NAs, then next
    if (sum(colnames(lod_df) == names(hplctofms_col1[i])) == 0) {
      next
    }
    # In case the feature occur in blanks and have no missing values, then next
    else if (identical(which(is.na(lod_df[, names(hplctofms_col1[i])])), integer(0))) {
      next
    } else {
      lod_df[which(is.na(lod_df[,names(hplctofms_col1[i])])), names(hplctofms_col1[i])] <- hplctofms_col1[[i]]
    }
  }


# B. Replace missing values in compounds that appear in >= 2 Blanks.

  for (i in 1:length(hplctofms_col2)) {
    # If the feature was removed during removal of >90% NAs, then next
    if (sum(colnames(lod_df) == names(hplctofms_col2[i])) == 0) {
      next
    }
    # In case the feature have no missing values, then next
    else if (identical(which(is.na(lod_df[, names(hplctofms_col2[i])])), integer(0))) {
      next
    } else {
      lod_df[which(is.na(lod_df[, names(hplctofms_col2[i])])), names(hplctofms_col2[i])] <- hplctofms_col2[[i]]
    }
  }

```

#### Option 4: missForest Iterative imputation
```{r , echo=FALSE, warning = FALSE, message=FALSE}
random_forest_imputation <- function(df) {
  library(missForest)
  
  # Perform random forest imputation
  imputed_data <- missForest(df)$ximp
  return(imputed_data)
}

missforest <- cbind(merged_df[, c(1,2,3,4)], random_forest_imputation(merged_df[, -c(1,2,3,4)])) %>% 
  relocate(Subcategory, Source, Polymer, .before =1)
```

## STEP 7: Data normalization
### Z-score normalization
```{r , echo=FALSE, warning = FALSE, message=FALSE}
z_score_normalize <- function(x) {
  (x - mean(x)) / sd(x)
}

# Apply to each column
data_normalized_custom <- as.data.frame(lapply(zero_df[, -c(1,2,3,4,5)], z_score_normalize))
data_normalized_zscore <- cbind(data_normalized_custom, zero_df[, c(1,2,3,4,5)]) %>% relocate(Subcategory, Source, Polymer, technique, Plastic_type, .before =1)
```

### Min-max scaling
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Min-max scaling function
min_max_scale <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# Apply to each column
data_minmaxscaled <- as.data.frame(lapply(zero_df[, -c(1,2,3,4,5)], min_max_scale))
data_minmaxscaled <- cbind(data_minmaxscaled, zero_df[, c(1,2,3,4,5)]) %>% relocate(Subcategory, Source, Polymer, technique, Plastic_type, .before =1)
```

### TSN
```{r , echo=FALSE, warning = FALSE, message=FALSE}
input <- zero_df
df_0.001_TSN <- as.data.frame(t(apply(input[, 6:ncol(input)],
                                       MARGIN = 1, 
                                       function(row) {row/sum(row, na.rm = TRUE)}))) 
print(sum(is.na(df_0.001_TSN)))

df_0.001_TSN <- df_0.001_TSN %>%
  mutate(Subcategory = input$Subcategory) %>%
  mutate(technique = input$technique) %>%
  mutate(Source = input$Source) %>%
  mutate(Polymer = input$Polymer) %>%
  mutate(Plastic_type = input$Plastic_type) %>%
  relocate(Subcategory, Source, Polymer, technique, Plastic_type, .before = 1)
```

### Log normalization
```{r, echo = FALSE, message = FALSE, warning = FALSE}
input <- zero_df
log_normalized_df <- as.data.frame(t(apply(input[, 6:ncol(input)], 1, function(x) log(x))))

log_normalized_df <- log_normalized_df %>% 
  mutate(Subcategory = input$Subcategory) %>%
  mutate(technique = input$technique) %>%
  mutate(Source = input$Source) %>%
  mutate(Polymer = input$Polymer) %>%
  mutate(Plastic_type = input$Plastic_type) %>%
  relocate(Subcategory, Source, Polymer, technique, Plastic_type, .before = 1)
```


### Check Correlation between predictors aka. multicollinearity to choose correct ML algorithms
```{r , echo=FALSE, warning = FALSE, message=FALSE}
cor_matrix <- stats::cor(data)

# Determine classifier type based on correlation
# High correlations (absolute value > 0.8) may suggest multicollinearity, favoring non-linear models.
high_corr <- any(abs(cor_matrix[upper.tri(cor_matrix)]) > 0.8)
if (high_corr) {
  cat("Consider using a non-linear machine learning classifier.\n")
} else {
  cat("Linear models may suffice based on correlation analysis.\n")
}
```

## K-means Clustering

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# Input numeric matrix data : sample as rows, collapsed_compounds as columns
df <- gc_hplc_list[[1]] %>%
  dplyr::select(NewFile,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- stats::runif(length(which(base::is.na(df[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

tot.withinss <- list()
for (n_clus in 1:nrow(df)) {
  set.seed(round(runif(2:20, 0, 999)))
  km.out <- stats::kmeans(df, n_clus, nstart = 50)
  tot.withinss[[n_clus]] <- km.out$tot.withinss
  print(paste0("The number of centers is ", n_clus, " and total within-cluster sum of square is ", km.out$tot.withinss))
}

plot(df, col = (stats::kmeans(df, 171, nstart = 50)$cluster + 1) ,
     main = "K- Means Clustering Results with K = 2",
     xlab = "", ylab = "", pch = 20, cex = 2)


```

## t-SNE
```{r, echo=FALSE, warning = FALSE, message=FALSE}
# REFERENCES VISUALIZATION: 
# https://plotly.com/r/t-sne-and-umap-projections/
# https://distill.pub/2016/misread-tsne/

library(tsne)
library(plotly)

dat <- df_percentage # icp_subcat[[3]]
# remove any Subcategory that have less than 3 sample
dat <- dat[!(dat$Subcategory %in% names(table(dat$Subcategory)[table(dat$Subcategory) < 3])),]
features <- subset(dat, select = -c(Subcategory, technique))

tsne <- tsne(features,
             initial_dims = 3, 
             k = 3, 
             perplexity = 20, # Hyperparameter: perplexity (optimal number of neighbors) < number of samples
             max_iter = 5000
             )

pdb <- cbind(data.frame(tsne), dat$Subcategory)
options(warn = -1)
tsne_plot <- plot_ly(data = pdb ,x =  ~X1, y = ~X2, z = ~X3, 
               color = ~dat$Subcategory) %>% 
  add_markers(size = 8) %>%
  layout( 
    xaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'), 
    yaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'),
    scene =list(bgcolor = "#e5ecf6"))

tsne_plot
```

## UMAP clustering 

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(umap)

dat <- df_percentage # gc_subcat[[3]]
# removgc_e any Subcategory that have less than 3 sample
dat <- dat[!(dat$Subcategory %in% names(table(dat$Subcategory)[table(dat$Subcategory) < 3])),]

features <- subset(dat, select = -c(Subcategory,
                                    technique
                                    ))

umap <- umap(features, n_components = 3,
             method = 'naive', metric= "manhattan",
             alpha = 0.0001, gamma = 0.0001)

layout <- cbind(data.frame(umap[["layout"]]), dat$Subcategory)
umap_plot <- plot_ly(layout, x = ~X1, y = ~X2, z = ~X3, 
                color = ~dat$Subcategory) %>% 
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'x-axis'), 
                                   yaxis = list(title = 'y-axis'), 
                                   zaxis = list(title = 'z-axis'))) 
umap_plot
```

## Wilcoxon test

```{r, echo = FALSE, message = FALSE, warning = FALSE}
input <- test2 

df <- data.frame(Feature=character(), comparison_pair=character(), pval=integer())

for (feature_col in 4:ncol(input)) {      
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(input$Source), 2))) {  
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(input$Source), 2)[1,col]
    p_2 <- utils::combn(unique(input$Source), 2)[2,col]
    
    # print(paste0(p_1, "_", p_2))
    # calculating the p-value between each gas station pair 
    vec1 <- as.numeric(unlist(input[which(input$Source == p_1), feature_col]))
    vec2 <- as.numeric(unlist(input[which(input$Source == p_2), feature_col]))
    
    if (length(vec1) >= 3 & length(vec1) >= 3) { # requires at least 3 data points per comparison pair
      # If all number in either of the vector are just zeros
      if (all(vec1 == vec1[1]) || all(vec2 == vec2[1])) {
        next
      } else {
        if (shapiro.test(vec1)$p.value < 0.05 || shapiro.test(vec2)$p.value < 0.05) {
          pval <- try(stats::wilcox.test(vec1, vec2)$p.value)
        } else if (shapiro.test(vec1)$p.value < 0.05 & shapiro.test(vec2)$p.value < 0.05) {
          pval <- try(stats::t.test(vec1, vec2)$p.value)
        } else {
          pval <- try(stats::wilcox.test(vec1, vec2)$p.value)
        }
      }
    } else {
      next
    }
    
    # For cases when Error: not enough 'x' observations
    if (inherits(pval, "try-error")) {
      next  # Skip to the next iteration if an error occurs
    }
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(input)[feature_col], 
                           paste0(p_1, " & ", p_2), 
                           pval)
  }
}

sig_comp <- list()
uncorrected_raw_pair_test_results <- list()
j <- 1

pvaluecorrect <- c("bonferroni", "holm", "hochberg", "hommel", "BH", "BY")
for (m in pvaluecorrect) {
  temp <- copy(df)
  temp$adjusted_pvalue <- stats::p.adjust(temp$pval, method = m)
  sig_comp[[j]] <- temp %>%
    filter(., adjusted_pvalue < 0.05) %>%
    arrange(adjusted_pvalue)
  
  uncorrected_raw_pair_test_results[[j]] <- temp
  
  if (dim(sig_comp[[j]])[1] == 0) {
    print(paste0(m," does not resulted in any significant compound"))
    j <- j + 1
    next
  } else {
    print(sig_comp[[j]] %>%
            arrange(adjusted_pvalue))
    print(paste0(m, " has ", length(unique((sig_comp[[j]] %>%
            arrange(adjusted_pvalue))$Feature)), " significant features"))
    j <- j + 1
  }
}


```

## Step 1.4: Remove all feature that only appears in 1 replicate of samples and blanks

If a feature presented across all replicates, it was retained and linearly averaged, if not in all replicates ==> remove from the sample

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS
# Filter to retain rows that have more than one unique replicate
# atdgcms_grouped_step1.4 <- atdgcms_grouped %>%
#   group_by(Feature, simplified_file) %>%
#   mutate(unique_replicates = n_distinct(Replicate)) %>%
#   ungroup()
# 
# # Identify indices to retain and remove
# idx_to_retain <- which(atdgcms_grouped_step1.4$unique_replicates > 1)
# idx_to_remove <- which(atdgcms_grouped_step1.4$unique_replicates <= 1)
# 
# atdgcms_grouped_inallreplicates <- atdgcms_grouped_step1.4[idx_to_retain, ]

# HPLCTOFMS
# Retain rows that have more than one unique replicate
# hplc_batch_grouped_step1.4 <- hplc_batch_grouped %>%
#   group_by(Feature, simplified_file, Day, Batch_number) %>%
#   mutate(unique_replicates = n_distinct(Replicate)) %>%
#   ungroup()
# 
# # Identify indices to retain and remove
# idx_to_retain <- which(hplc_batch_grouped_step1.4$unique_replicates > 1)
# idx_to_remove <- which(hplc_batch_grouped_step1.4$unique_replicates <= 1)
# 
# hplc_grouped_inallreplicates <- hplc_batch_grouped_step1.4[idx_to_retain, ]
```