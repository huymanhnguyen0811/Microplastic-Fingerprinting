---
title: "Demo ML Microplastic"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = getwd())
knitr::opts_chunk$set(echo = FALSE)
```

## Documentation
This repo is accompanying the publication: "Computational fingerprinting workflow for environmental source tracking of microplastic based on representative additives"

Demo of each ML algorithms is shown below.

## Data processing

You can include R code in the document as follows:

```{r, echo = FALSE, message = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)

# Functions -------------------------------------------------------------------------------------------------------
# Filtering matched compound names
filtering <- function(df, filter_list) {
  clean_data <-  copy(df)
  for (ele in filter_list) {
    clean_data <- clean_data %>%
      filter(!grepl(ele, Compound))
  }
  return(clean_data)
}

# Filtering limit of observations
limit_obser <- function(df_list, file_list, cap) {
  df_list_filter_area <- list()
  df_list_removed_area <- list()
  for (i in 1:length(df_list)) {
    df_list_filter_area[[i]] <- df_list[[i]] %>%
      filter(., Area > cap)
      # mutate(sample_name = file_list[[i]])
      
    df_list_removed_area[[i]] <- df_list[[i]] %>%
      filter(., Area <= cap)
      # mutate(sample_name = file_list[[i]])
  }
  return(list(df_list_filter_area, df_list_removed_area))
}
  
# Notin function
`%notin%` <- Negate(`%in%`)

# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp_ver1 <- function(data, rtthres, mzthres) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rtthres: RT threshold window
  # mzthres: mz threshold window
  dat <- copy(data)
  
  # Initialize the compound column filled with NA values
  dat$collapsed_compound <- NA
  i <- 1
  
  for (row in 1:nrow(dat)) {
    # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    rt <- dat[row,]$RT
    mz <- dat[row,]$m.z
    
    idx <- which(dat$RT <= (rt + rtthres) & dat$RT >= (rt - rtthres) &
                   dat$m.z <= (mz + mzthres) & dat$m.z >= (mz - mzthres) &
                   is.na(dat$collapsed_compound))
    
    if (identical(idx, integer(0))) {
      next
    }
    else {
      dat[idx, "collapsed_compound"] <- paste0("Compound_", i, ".")
      i <- i + 1
    }  
  }
  
  return(dat)
}

# Filtering similar and unique compound 
# compound appear in at least 2 samples

comp_filter_ver1 <- function(data, n) {
  all_similar_compounds_idx <- c()
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()
  
  for (comp_grp in unique(data$collapsed_compound)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$collapsed_compound, fixed = TRUE))
    
    if (length(unique(data[idx,]$File)) > (n - 1)) {
      all_similar_compounds_idx <- c(all_similar_compounds_idx, idx)
    }
    else if (length(unique(data[idx,]$File)) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    }
    else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_similar_compounds_idx, all_other_compounds_idx, all_unique_compounds_idx))
}

# compound appear in at least 2 plastic types

comp_filter_ver2 <- function(data, n) {
  all_similar_compounds_idx <- c()
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()
  
  for (comp_grp in unique(data$collapsed_compound)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$collapsed_compound, fixed = TRUE))
    
    if (length(unique(data[idx,]$plastic_type)) > (n - 1)) {
      all_similar_compounds_idx <- c(all_similar_compounds_idx, idx)
    }
    else if (length(unique(data[idx,]$plastic_type)) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    }
    else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_similar_compounds_idx, all_other_compounds_idx, all_unique_compounds_idx))
}

# TSN - Percent-based normalization 
data_normalization <- function(data) {
  temp_list <- list()
  i <- 1
  # Normalize Peak Area for each sample 
  for (sample in unique(data$File)) {
    df <- data[which(data$File == sample),] %>%
      mutate(Percent_Area = Area/sum(.$Area)) %>%
      mutate(Percent_Height = Height/sum(.$Height))
    temp_list[[i]] <- df
    i <- i + 1
  }
  # Then combine data again to 1 grand data frame
  newdata <- dplyr::bind_rows(temp_list)
  return(newdata)
}


# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

# STEP 1.1: Data import --------------------------------------------
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data")

file_list <- list.files(pattern = '*.csv') %>%
  .[!str_detect(., "Blank")]
  # .[!str_detect(., "_USSB")] # exclude environmental samples

# Blank samples 
blank_list <- list.files(pattern = '*.csv') %>%
  .[str_detect(., "Blank")]

# Import samples to list
df_list_step1.1 <- purrr::map(file_list, read.csv)

df_list_blank <- purrr::map(blank_list, read.csv)

df_blank <- dplyr::bind_rows(df_list_blank)
# summary(df_step1.1)

sampleinfo <- readxl::read_excel(paste0(getwd(), '/SampleInfo.xlsx'))
colnames(sampleinfo)[1] <- 'File'

# sampleinfo <- sampleinfo %>%
#   filter(., !str_detect(File, "_USSB")) # exclude environmental samples

sampleinfo$`Collection Date (YYYY-MM-DD)` <- as.Date(as.numeric(sampleinfo$`Collection Date (YYYY-MM-DD)`), origin = "1899-12-30")

# STEP 1.2B Filtering out limit of observations---------------------------- 
# Area  = 100000 seems to be the right cutoff after inspection with quality control A and B

list_remaining_area <- limit_obser(df_list_step1.1, file_list, cap = 100000)[[1]]
list_removed_area <- limit_obser(df_list_step1.1, file_list, cap = 100000)[[2]]

# STEP 1.3: Grouping compounds based on Retention time and molecular ions  -----------------------------------------------------------------------
# STEP 1.3A: Generate 1 grand data frame of all 31 IL samples

df_step1.3 <- bind_rows(list_remaining_area) %>%
  select(-c("Start", "End", "Width", "Base.Peak")) %>%
  mutate(plastic_type = ifelse(str_detect(File, "Balloons"), "Balloons", 
                            ifelse(str_detect(File, "FPW_"), "Food_Packaging_Waste",
                                   ifelse(str_detect(File, "MPW_"), "Mixed_Plastic_Waste", 
                                          ifelse(str_detect(File, "PBBC_"), "Plastic_Bottles_and_Bottle_Caps",
                                                 ifelse(str_detect(File, "PC_Sample"),"Plastic_Cups",
                                                        ifelse(str_detect(File, "PDS_Sample"),"Plastic_Drinking_Straws", "Other")))))))

df_blank <- df_blank %>%
  select(-c("Start", "End", "Width", "Base.Peak")) %>%
  mutate(plastic_type = "Blanks")

combined_df <- rbind(df_step1.3, df_blank) %>% arrange(RT)


# STEP 1.3B: Collapsing compounds based on RT1, RT2, Ion1 threshold

combined_df_grouped <- grouping_comp_ver1(combined_df,
                                          rtthres = 0.05,
                                          mzthres = 0.05)



# STEP 2: Normalizing data accordingly to different data frames of interest --------------------------------------------------

comp_normalized <- data_normalization(combined_df_grouped)

# Step 3: Readjust compound RA (sample) by average blank RA ======================
# Create list to store temp dfs
temp_list <- list()
i <- 1
# Iterate through each collapsed_compound
for (comp in unique(comp_normalized$collapsed_compound)) {
  temp <- comp_normalized[which(comp_normalized$collapsed_compound == comp),]
  # if compound does not exist in blanks then skip the compounds
  if (identical(which(temp$plastic_type == "Blanks"), integer(0))) {
    temp_list[[i]] <- temp
    i <- i + 1
    next
  }
  else {
    # Calculate avg_blank for that compound across all blanks
    avg_blank <- mean(temp[which(temp$plastic_type == "Blanks"),]$Percent_Area)
    temp <- temp[which(temp$plastic_type != "Blanks"),]
    # iterate through each sample
    for (sample in unique(temp$File)) {
      # Adjust RA for each compound of each sample = RA (sample) - avg_blank
      temp[which(temp$File == sample),]$Percent_Area <- temp[which(temp$File == sample),]$Percent_Area - avg_blank
    } 
  }
  # Append current temp df to temp_list
  temp_list[[i]] <- temp
  i <- i + 1
}

adjusted_df <- bind_rows(temp_list)

# Step 4: Replace negative adjusted RA values with LOD =============================
adjusted_df$Percent_Area[adjusted_df$Percent_Area < 0] <- runif(length(adjusted_df$Percent_Area[adjusted_df$Percent_Area < 0]),
                                                                min = sort(adjusted_df$Percent_Area[adjusted_df$Percent_Area > 0])[1],
                                                                max = sort(adjusted_df$Percent_Area[adjusted_df$Percent_Area > 0])[2])

# STEP 5: Identify shared and unique compound groups across samples ------------------------------------------------
# at least in 2 samples
idx_list_filter_samples <- comp_filter_ver1(adjusted_df, 
                                            length(file_list))

# at least in 2 plastic types
idx_list_filter_plastic_types <- comp_filter_ver2(adjusted_df, 
                                                  length(unique(adjusted_df$plastic_type)))

# Combine compounds that occur in at least 2 samples
shared_comp_sample <- adjusted_df[c(idx_list_filter_samples[[1]], idx_list_filter_samples[[2]]),]

# Combine compounds that occur in at least 2 plastic types 
shared_comp_plastic_type <- adjusted_df[c(idx_list_filter_plastic_types[[1]],
                                          idx_list_filter_plastic_types[[2]]
                                          )
                                        ,]

# Merging Sample info with shared df ===========================================

merge_df <- dplyr::full_join(x = sampleinfo,  y = shared_comp_plastic_type, by = 'File') %>%
  select(-"plastic_type") %>%
  filter(., !is.na(collapsed_compound))

```

## Demo PCA

```{r , echo=FALSE, warning = FALSE}
library(PCAtools)
library(stats)

# PCA -----------------
# prep input
input_df <- function(data, pkg) {
  # create sample df
  if (pkg == "PCAtools") {
    df_X_rq1 <- data %>%
    dplyr::select(File, collapsed_compound, Percent_Area) %>%
    mutate(File = factor(File, levels = unique(File))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, collapsed_compound) %>%
    summarise(across(Percent_Area, mean)) %>%
    pivot_wider(names_from = File, values_from = Percent_Area) %>%
    column_to_rownames(., var = "collapsed_compound")
  
  }
  else {
      df_X_rq1 <- data %>%
    dplyr::select(File, collapsed_compound, Percent_Area) %>%
    mutate(File = factor(File, levels = unique(File))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, collapsed_compound) %>%
    summarise(across(Percent_Area, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Percent_Area) %>%
    column_to_rownames(., var = "File")
  }
  for (r in 1:nrow(df_X_rq1)) {
    df_X_rq1[r, which(base::is.na(df_X_rq1[r,]))] <- runif(length(which(base::is.na(df_X_rq1[r,]))),
                                                           min = sort(data$Percent_Area)[1],
                                                           max = sort(data$Percent_Area)[2])
  }

  # table for information (rows are sample IDs, columns are sample information) -----------------------
  metadata_X_rq1 <- data.frame(unique((data 
                                         # filter(., !str_detect(File, "_USSB"))
                                       )$File))
  colnames(metadata_X_rq1) <- c('File')
  material <- c()
  for (row in 1:nrow(metadata_X_rq1)) {
    material<- c(material, unique(data[which(data$File == metadata_X_rq1[row, 'File']),]$Material))
  }
  metadata_X_rq1$material <- material
  metadata_X_rq1 <- metadata_X_rq1 %>%
    mutate(plastic_type = ifelse(str_detect(File, "FPW_"), "Food_Packaging_Waste",
                                 ifelse(str_detect(File, "Balloons"), "Balloons",
                                        ifelse(str_detect(File, "MPW_"), "Mixed_Plastic_Waste",
                                               ifelse(str_detect(File, "PBBC_"), "Plastic_Bottles_and_Bottle_Caps",
                                                      ifelse(str_detect(File, "PC_Sample"),"Plastic_Cups",
                                                             ifelse(str_detect(File, "PDS_Sample"),"Plastic_Drinking_Straws", "Other"))))))) %>%
    column_to_rownames(., var = "File")

  return(list(df_X_rq1 ,metadata_X_rq1))
}

df_pca <- input_df(merge_df, pkg = "PCAtools")

# PCA with PCAtools::pca ===========
colnames(df_pca[[2]])[2] <- c("Plastic type")

# PCAtools::pca requires mat input (columns as sample name, rows as collapsed_compound)
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
                   )

# Retrieve PC and add as new variables to data frame 
PCAtools_mergePC <- p$rotated

# PCA with stats::prcomp ===========
# stats::prcomp requires input df (columns as collapsed_compound, rows as sample name) -> change function df_pca pivot_wider(names_from=..)
df_pca <- input_df(merge_df, pkg = "stats")
prcomp_res <- stats::prcomp(df_pca[[1]], center = FALSE)
stats::biplot(x = prcomp_res)

# Retrieve PC and add as new variables to data frame 
e1071_merge_PC <- as.data.frame(prcomp_res$x)

# A bi-plot =================
PCAtools::biplot(p,
                 lab = NULL, 
                 colby = "material", 
                 hline = 0, vline = 0,
                 legendPosition = 'right', labSize = 5,
                 sizeLoadingsNames = 5,
                 showLoadings = TRUE,
                 showLoadingsNames = FALSE,
                 ntopLoadings = 10,
                 pointSize = 4, 
                 legendLabSize = 15,
                 legendTitleSize = 16,
                 legendIconSize = 6)
```

## Demo RF + GBM

### With all samples

#### PCAtools

```{r, echo=FALSE, warning = FALSE}
library(randomForestSRC)

# PCA-based feature data

ptype_n_samp <- function(data, use) {
  newdata <- data %>%
    tibble::rownames_to_column(., var = "File") %>%
    dplyr::mutate(plastic_type = ifelse(str_detect(File, "Balloons"), "Balloons", 
                                 ifelse(str_detect(File, "FPW_"), "Food_Packaging_Waste",
                                        ifelse(str_detect(File, "MPW_"), "Mixed_Plastic_Waste", 
                                               ifelse(str_detect(File, "PBBC_"), "Plastic_Bottles_and_Bottle_Caps",
                                                      ifelse(str_detect(File, "PC_Sample"),"Plastic_Cups",
                                                             ifelse(str_detect(File, "PDS_Sample"),"Plastic_Drinking_Straws", "Other"))))))) %>%
    dplyr::mutate(Type = ifelse(str_detect(File, "USSB"), "Store-Bought", "Environmental")) %>%
    dplyr::mutate(plastic_type = factor(plastic_type, levels = unique(plastic_type))) %>%
    dplyr::mutate(Type = factor(Type, levels = unique(Type))) %>%
    dplyr::relocate(c(Type, plastic_type), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  if (use == "min") {
    # identify plastic type with minimum number of samples
    min_samp_size <- as.numeric(newdata %>% count(plastic_type) %>% summarise(min(n)))
    
    # select random number of rows for each plastic type == minimum number of samples
    newdf <- list()
    i <- 1
    for (plt in unique(newdata$plastic_type)) {
      tmpdf <- newdata[which(newdata$plastic_type == plt),]
      # random sampling within the levels of y when y is a factor to balance the class distributions within the splits.
      idx <- caret::createDataPartition(tmpdf$Type, 
                                        p = base::round(min_samp_size/nrow(tmpdf), 1), 
                                        list = F)
      newdf[[i]] <- tmpdf[idx, ]
      i <- i + 1
    }
    
    df <- bind_rows(newdf)
  }
  else {
    df <- newdata
  }
  return(df)
}
rfsrc.result <- function(dat, split.ratio){
  set.seed(1234)
  # Partitioning train&test sets / training / predict on test set
  plastic_idx <- caret::createDataPartition(dat$plastic_type, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]  
  
  # Train
  mergePC.rf <- rfsrc(plastic_type ~ ., 
                      ntree=20000, 
                      splitrule = "auc", 
                      nodesize = 1, #Minumum size of terminal node for classification (1)
                      # mtry = 21,
                      importance = "permute", 
                      samptype = "swr",
                      membership = TRUE,
                      perf.type="misclass",
                      block.size = 1, # cumulative error rate on every tree
                      data = plastic_trn
                      )
  print(mergePC.rf)
  oob_error_plot <- plot(mergePC.rf)
  
  imp_var <- vimp(mergePC.rf, importance = "permute")$importance
  print(get.auc(plastic_trn$plastic_type, mergePC.rf$predicted.oob))
  # Prediction results
  pred_res <- predict(mergePC.rf, newdata = plastic_tst, type = "prob")$predicted
  rownames(pred_res) <- rownames(plastic_tst)
  
  # selection of the best feature candidates
  # md.obj <- max.subtree(mergePC.rf)
  # best.feature <- md.obj$topvars # extracts the names of the variables in the object md.obj
  
  return(list(pred_res, oob_error_plot, imp_var))
}

PCAtools_alldf <- ptype_n_samp(PCAtools_mergePC, use = "all")
PCAtools_alldf.RFresult <- rfsrc.result(PCAtools_alldf, split.ratio = 0.6)
```

#### e1071

```{r, echo=FALSE, warning = FALSE}
e1071_alldf <- ptype_n_samp(e1071_merge_PC, use = "all")
e1071_alld.RFresult <- rfsrc.result(PCAtools_alldf, split.ratio = 0.6)
```

### With minimum sample size

#### PCAtools

```{r, echo=FALSE, warning = FALSE}
PCAtools_mindf <- ptype_n_samp(PCAtools_mergePC, use = "min")
PCAtools_mindf.RFresult <- rfsrc.result(PCAtools_alldf, split.ratio = 0.6)
```

#### e1071

```{r, echo=FALSE, warning = FALSE}
e1071_mindf <- ptype_n_samp(e1071_merge_PC, use = "min")
e1071_mindf.RFresult <- rfsrc.result(e1071_mindf, split.ratio = 0.6)
```

## Demo SVM
