---
title: "Microplastic fingerprinting"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data")
knitr::opts_chunk$set(echo = FALSE)
```

## Documentation

This repo is accompanying the publication: "Computational fingerprinting workflow for environmental source tracking of microplastic based on representative additives"

Demo of each ML algorithms is shown below.

## Data processing

You can include R code in the document as follows:

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(Hmisc)
library(writexl)
library(tidyr)

# Functions -------------------------------------------------------------------------------------------------------
# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp <- function(data, rtthres, mzthres, type) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rtthres: RT threshold window
  # mzthres: mz threshold window
  dat <- data[ , , drop = FALSE]  # Shallow copy to avoid modification of original data
  
  # Initialize the compound column filled with NA values
  dat$collapsed_compound <- NA
  i <- 1

  # Validate the 'type' parameter
  if (!(type %in% c("ATDGCMS", "HPLCTOFMS"))) {
    stop("Invalid type specified. Must be either 'ATDGCMS' or 'HPLCTOFMS'")
  }

  # Loop through each row to group compounds
  for (row in 1:nrow(dat)) {
    # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    rt <- base::as.numeric(dat[row, "RT"])
    mz <- base::as.numeric(dat[row, "m.z"])
    
    idx <- which(
      dat$RT <= (rt + rtthres) & dat$RT >= (rt - rtthres) &
      dat$m.z <= (mz + mzthres) & dat$m.z >= (mz - mzthres) &
      is.na(dat$collapsed_compound)
    )
    
    if (length(idx) > 0) {
      dat[idx, "collapsed_compound"] <- paste0("Compound_", i, ".", type)
      i <- i + 1
    }  
  }
  return(dat)
}


# Filtering similar and unique compound 

comp_filter <- function(data) {
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()

  for (comp_grp in unique(data$collapsed_compound)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$collapsed_compound, fixed = TRUE))
    
    if (length(idx) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    } else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_other_compounds_idx, all_unique_compounds_idx))
}
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# STEP 1.1: Data import --------------------------------------------

# ATDGCMS
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/ATDGCMS")

file_list1 <- list.files(pattern = '*.csv') %>%
  .[!str_detect(., "Blank")]
  # .[!str_detect(., "_USE")] # exclude environmental samples

# Blank samples 
blank_list1 <- list.files(pattern = '*.csv') %>%
  .[str_detect(., "Blank")]

# Import samples to list
df_list1_step1.1 <- purrr::map(file_list1, read.csv)

df_list1_blank <- purrr::map(blank_list1, read.csv)

df_blank1 <- dplyr::bind_rows(df_list1_blank)

# # Sample information ATDGCMS
# sampleinfo1 <- readxl::read_excel(paste0(getwd(), '/SampleInfo.xlsx'))
# colnames(sampleinfo1)[1] <- 'File'
# 
# # sampleinfo1 <- sampleinfo1 %>%
# #   filter(., !str_detect(File, "_USE")) # exclude environmental samples
# 
# sampleinfo1$`Collection Date (YYYY-MM-DD)` <- as.Date(as.numeric(sampleinfo1$`Collection Date (YYYY-MM-DD)`), origin = "1899-12-30")

# HPLCTOFMS
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/HPLCTOFMS")

file_list2 <- list.files(pattern = '*.xls') %>%
  .[!str_detect(., "Blank")] %>%
  .[!str_detect(., "Info")]

# Blank samples 
blank_list2 <- list.files(pattern = '*.xls') %>%
  .[str_detect(., "Blank")]

# Import samples to list
df_list2_step1.1 <- purrr::map(file_list2, read_xls, skip = 1)

df_list2_blank <- purrr::map(blank_list2, read_xls, skip = 1)

df_blank2 <- dplyr::bind_rows(df_list2_blank)

# # Sample information HPLCMS
# sampleinfo2 <- readxl::read_excel(paste0(getwd(), '/Plastic Product Info EF.xlsx'))
# colnames(sampleinfo2)[1] <- 'Sample_ID'
# sampleinfo2 <- sampleinfo2 %>%
#   filter(str_detect(Sample_ID, "USE"))

# STEP 1.2B Filtering out chemical noise (aka. setting limit of observations) of samples (NOT blanks) ---------------------------- 

# ATDGCMS

list1_remaining_area <- list()
list1_removed_area <- list()
for (i in 1:length(df_list1_step1.1)) {
  list1_remaining_area[[i]] <- df_list1_step1.1[[i]] %>%
    filter(., Area > 100000)
  
  list1_removed_area[[i]] <- df_list1_step1.1[[i]] %>%
    filter(., Area <= 100000)
}

# HPLCTOFMS
list2_remaining_area <- list()
list2_removed_area <- list()
for (i in 1:length(df_list2_step1.1)) {
  list2_remaining_area[[i]] <- df_list2_step1.1[[i]] %>%
    filter(., Height > 5000)
  
  list2_removed_area[[i]] <- df_list2_step1.1[[i]] %>%
    filter(., Height <= 5000)
}

# STEP 1.3: Grouping compounds based on Retention time and molecular ions  -----------------------------------------------------------------------
# STEP 1.3A: Generate 1 grand data frame

# ATDGCMS
df1 <- bind_rows(list1_remaining_area) %>%
  select(-c("Start", "End", "Width", "Base.Peak", "Cpd", "Label", "Height", "Ions")) %>%
  # Change all ATDGCMS file name with underscore-separated to hyphen-separated
  dplyr::mutate(File = gsub("_", "-", File)) %>%
  mutate(type = "Sample")

df_blank1 <- df_blank1 %>%
  select(-c("Start", "End", "Width", "Base.Peak", "Cpd", "Label", "Height", "Ions")) %>%
  mutate(type = "Blanks")

combined_df1 <- rbind(df1, df_blank1) %>% 
  arrange(RT)

# HPLCTOFMS
df2 <- bind_rows(list2_remaining_area) %>%
  select(c("m/z", "RT", "Height", "File")) %>%
  dplyr::mutate(File = gsub("_", "-", File)) %>%
  mutate(type = "Sample")

df_blank2 <- df_blank2 %>%
  select(c("m/z", "RT", "Height", "File")) %>%
  mutate(type = "Blanks")

combined_df2 <- rbind(df2, df_blank2) %>% 
  arrange(RT)

colnames(combined_df2)[[1]] <- "m.z"

# STEP 1.3B: Collapsing compounds based on RT1, RT2, Ion1 threshold

# Statistical Description for selecting rtthres and mzthres
# Hmisc::describe(combined_df1)

# ATDGCMS
combined_df1_grouped <- grouping_comp(combined_df1,
                                      rtthres = 0.05,
                                      mzthres = 0.05,
                                      type = "ATDGCMS")

# Statistical Description for selecting rtthres and mzthres
# Hmisc::describe(combined_df2)

# HPLCTOFMS
combined_df2_grouped <- grouping_comp(combined_df2,
                                      rtthres = 0.1,
                                      mzthres = 0.0005,
                                      type = "HPLCTOFMS")

# Step 2: Readjust compound RA (sample) by average blank RA ======================

# ATDGCMS

# Create list to store temp dfs
temp_list <- list()
i <- 1
# Iterate through each collapsed_compound
for (comp in unique(combined_df1_grouped$collapsed_compound)) {
  temp <- combined_df1_grouped[which(combined_df1_grouped$collapsed_compound == comp),]
  # if compound does not exist in blanks then skip the compounds
  if (identical(which(temp$type == "Blanks"), integer(0))) {
    temp_list[[i]] <- temp
    i <- i + 1
    next
  }
  else {
    # Calculate avg_blank for that compound across all blanks
    avg_blank <- mean(temp[which(temp$type == "Blanks"),]$Area)
    temp <- temp[which(temp$type != "Blanks"),]
    # iterate through each sample
    for (sample in unique(temp$File)) {
      # Adjust RA for each compound of each sample = RA (sample) - avg_blank
      temp[which(temp$File == sample),]$Area <- temp[which(temp$File == sample),]$Area - avg_blank
    } 
  }
  # Append current temp df to temp_list
  temp_list[[i]] <- temp
  i <- i + 1
}

adjusted_df1 <- bind_rows(temp_list)

# HPLCTOFMS

# Create list to store temp dfs
temp_list <- list()
i <- 1
# Iterate through each collapsed_compound
for (comp in unique(combined_df2_grouped$collapsed_compound)) {
  temp <- combined_df2_grouped[which(combined_df2_grouped$collapsed_compound == comp),]
  # if compound does not have blanks then append the compound to the list and move on to the next compounds in the loop
  if (identical(which(temp$type == "Blanks"), integer(0))) {
    temp_list[[i]] <- temp
    i <- i + 1
    next
  }
  else {
    # Calculate avg_blank for that compound across all blanks
    avg_blank <- mean(temp[which(temp$type == "Blanks"),]$Height)
    temp <- temp[which(temp$type != "Blanks"),]
    # iterate through each sample
    for (sample in unique(temp$File)) {
      # Adjust RA for each compound of each sample = RA (sample) - avg_blank
      temp[which(temp$File == sample),]$Height <- temp[which(temp$File == sample),]$Height - avg_blank
    } 
  }
  # Append current temp df to temp_list
  temp_list[[i]] <- temp
  i <- i + 1
}

adjusted_df2 <- bind_rows(temp_list)
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# STEP 3: Remove all detections with negative peak area and Normalizing data accordingly to different data frames of interest on with the one with positive Area values -----------------------------------

# ATDGCMS

# paste0("the number of compounds with negative Area values are: ", length(adjusted_df1$Area[adjusted_df1$Area < 0]))

temp_list <- list()
# neg_list <- list()
i <- 1
# Normalize Peak Area for each sample 
for (sample in unique(adjusted_df1$File)) {
  df <- adjusted_df1[which(adjusted_df1$File == sample),] %>%
    filter(Area > 0) %>%
    mutate(Percent_Area = Area/sum(.$Area))
  temp_list[[i]] <- df
  
  i <- i + 1
}

# For positive values => combine data again to 1 grand data frame
comp_normalized1 <- dplyr::bind_rows(temp_list)

transform_filename_atdgcms <- function(filename) {
  split_name <- base::strsplit(filename, "-")[[1]]

  # Remove 2nd and 3rd words and modify the 4th word (now 3rd after removal)
  split_name <- c(split_name[-c(2, 3, 6)], paste0("rep", split_name[6]))

  # Combine steps: Glue string together and add "ATDGCMS-" prefix
  return(paste0("ATDGCMS-", paste(split_name, collapse = "-")))
}


comp_normalized1 <- comp_normalized1 %>%
  group_by(File) %>%
  mutate(NewFile = transform_filename_atdgcms(File))

# Change name of Fishing Item to correct name
comp_normalized1[which(str_detect("ATDGCMS-Fishing-1-USE-1-rep15", comp_normalized1$NewFile)),]$NewFile <- "ATDGCMS-Fishing-Items-USE-15-rep1" 

# HPLCTOFMS

# paste0("the number of compounds with negative Area values are: ", length(adjusted_df2$Height[adjusted_df2$Height < 0]))

temp_list <- list()
# neg_list <- list()
i <- 1
# Normalize Peak Area for each sample 
for (sample in unique(adjusted_df2$File)) {
  df <- adjusted_df2[which(adjusted_df2$File == sample),] %>%
    dplyr::filter(Height > 0) %>%
    mutate(Percent_Height = Height/sum(.$Height))
  temp_list[[i]] <- df
  
  i <- i + 1
}

# Then combine data again to 1 grand data frame
comp_normalized2 <- dplyr::bind_rows(temp_list)

transform_filename_hplctofms <- function(filename) {
  # remove the last 3 elements of the splitted name
  split_name <- head(base::strsplit(filename, "-")[[1]], -3)

  # glue the remaining elements together
  match_phrase <- paste(split_name[c(1,2)], collapse = "-")
  
  # find the corresponding atdgcms file name that have the match first 6 letters -> add prefix HPLCTOFMS to filename -> Glue string together
  return(paste0("HPLCTOFMS-", paste(c(base::strsplit(unique(comp_normalized1$File)[str_detect(unique(comp_normalized1$File), match_phrase)], "-")[[1]][[1]], split_name), collapse = "-")))
}

comp_normalized2 <- comp_normalized2 %>%
  group_by(File) %>%
  mutate(NewFile = transform_filename_hplctofms(File))
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# STEP 4: Identify shared and unique compound groups across samples ------------------------------------------------
p <- "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/Table of product categorization_NewFileName.xlsx"

gc_list <- list()
hplc_list <- list()
gc_hplc_list <- list()

for (i in 1:length(excel_sheets(path = p))) {
  # Table of categorization
  sampinfo <- readxl::read_excel(path = p, 
                                 sheet = excel_sheets(path = p)[i])
  
    # at least in 2 samples
  # ATDGCMS
  # joining data frame with sampinfo df
  dat1 <- left_join(x = subset(comp_normalized1, select = -c(File)), 
                    y = subset(sampinfo, select = -c(File)), 
                    by = 'NewFile')
  
  idx_list_filter1 <- comp_filter(dat1) 
  
  shared_comp1<- dat1[idx_list_filter1[[1]],]
  
  
  # HPLCTOFMS
  # joining data frame with sampinfo df
  dat2 <- left_join(x = subset(comp_normalized2, select = -c(File)), 
                    y = subset(sampinfo, select = -c(File)),  
                    by = 'NewFile')
  
  idx_list_filter2 <- comp_filter(dat2)
  
  shared_comp2 <- dat2[idx_list_filter2[[1]],]
  
  # Step 5: Merging Sample info with shared df ===========================================
  # ATDGCMS
  
  atdgcms <- shared_comp1 %>%
    filter(., !is.na(collapsed_compound)) %>%
    select('NewFile', 'Suspected_Polymer', 'Category', 'Subcategory', 'collapsed_compound', 'm.z', 'RT', 'Percent_Area')
  
  colnames(atdgcms)[ncol(atdgcms)] <- 'Values'
  
  gc_list[[i]] <- atdgcms
  
  # HPLCTOFMS
  # unique(grep(pattern = paste(unique(merge_df2$Sample_ID), collapse = "|"), x = merge_df1$File, value = TRUE))
  
  hplctofms <- shared_comp2 %>%
    filter(., !is.na(collapsed_compound)) %>%
    select('NewFile', 'Suspected_Polymer', 'Category', 'Subcategory', 'collapsed_compound', 'm.z', 'RT', 'Percent_Height')
  
  colnames(hplctofms)[ncol(hplctofms)] <- 'Values'
  
  hplc_list[[i]] <- hplctofms
  
  ### COMBINATION OF HPLC AND ATDGC datasets
  gc_hplc <- rbind(atdgcms, hplctofms)
  
  gc_hplc_list[[i]] <- gc_hplc
}
```


### Export GC/HPLC/Combined df to excel as input for Python ML pipeline
```{r , echo=FALSE, warning = FALSE}
writexl::write_xlsx(x = gc_list, path = "gc_list_df.xlsx")
writexl::write_xlsx(x = hplc_list, path = "hplc_list_df.xlsx")
writexl::write_xlsx(x = gc_hplc_list, path = "gc_hplc_list_df.xlsx")
```

### ATDGCMS - Summary of Percent_Area in each sample of merge_df1 to know how much Percent_Area we lost when we force the compounds to appear in at least 2 product categories

```{r , echo=FALSE, warning = FALSE}
for (file in unique(merge_df1$File)) {
  print(paste0("The remaining Percent Area of ", file, " is ", sum(merge_df1[which(merge_df1$File == file),]$Percent_Area)))
}
```

### HPLCTOFMS - Summary of Percent_Area in each sample of merge_df1 to know how much Percent_Area we lost when we force the compounds to appear in at least 2 product categories

```{r , echo=FALSE, warning = FALSE}
for (file in unique(shared_comp_product_cat2$File)) {
  print(paste0("The remaining Percent Height of ", file, " is ", sum(shared_comp_product_cat2[which(shared_comp_product_cat2$File == file),]$Percent_Height)))
}
```

### For each plastic type, What are compounds that only appear in USSB and not USE?

```{r, echo=FALSE, warning = FALSE}
# compdiff <- list()
for (plt in unique(adjusted_df$plastic_type)) {
  # compdiff[[plt]] <- 
  print(plt)
  print(base::setdiff(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

### For each plastic product, What are the compounds that consistently appear in both USSB & USE?

```{r, echo=FALSE, warning = FALSE}
for (plt in unique(adjusted_df$plastic_type)) {
  print(plt)
  print(base::intersect(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

## Demo PCA

```{r , echo=FALSE, warning = FALSE, message=FALSE}
library(PCAtools)
library(stats)
library(FactoMineR)
library(factoextra)

```

Since our data is compositional data, combining PCA with the Aitchison dissimilarity index (inherited from HCA for compositional data), however, is not straightforward due to the nature of compositional data. Standard PCA assumes the data does not have the constant sum constraint, which is a fundamental property of compositional data. Applying PCA directly to raw compositional data can lead to misleading results because it does not account for the compositional nature of the data.

To perform PCA on compositional data while considering the Aitchison distance, you would typically follow these steps:

Log-Ratio Transformation: Transform the compositional data using an appropriate log-ratio transformation, such as the Centered Log-Ratio (CLR) transformation, the Additive Log-Ratio (ALR), or the Isometric Log-Ratio (ILR) transformation. These transformations allow the data to be analyzed in real Euclidean space, where standard statistical techniques are valid.

### columns as "sample name"

```{r , echo=TRUE, warning = FALSE, message=FALSE}
# columns as "sample name"
input_df <- function(data) {
  # create sample df
  df_X_rq1 <- data %>%
    dplyr::select(NewFile, collapsed_compound, Values) %>%
    # filter(., str_detect(NewFile, "USE")) %>%
    # filter(., str_detect(NewFile, "HPLC")) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    dplyr::group_by(NewFile, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    tidyr::pivot_wider(names_from = NewFile, values_from = Values) %>%
    tibble::column_to_rownames(., var = "collapsed_compound")
  
  for (r in 1:nrow(df_X_rq1)) {
    df_X_rq1[r, which(base::is.na(df_X_rq1[r,]))] <- stats::runif(length(which(base::is.na(df_X_rq1[r,]))),
                                                                  min = sort(data$Values)[1]/10000,
                                                                  max = sort(data$Values)[2]/10000)
  }

  # table for information (rows are sample IDs, columns are sample information)
  metadata_X_rq1 <- data.frame(NewFile = colnames(df_X_rq1))
  category <- c()
  for (NewFile in metadata_X_rq1$NewFile) {
    category <- c(category, unique(data[which(data$NewFile == NewFile),]$Category))
  }
  metadata_X_rq1$Category <- category
  metadata_X_rq1 <- metadata_X_rq1 %>% tibble::column_to_rownames(., var = "NewFile")

  return(list(df_X_rq1, metadata_X_rq1))
}
```

```{r , echo=FALSE, warning = FALSE, message=FALSE}
df_pca <- input_df(gc_hplc_list[[1]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
                   )

# A bi-plot =================
PCAtools::biplot(p,
                 lab = rownames(p$metadata),
                 colby = "Category",
                 hline = 0, vline = 0,
                 legendPosition = 'right', labSize = 5,
                 sizeLoadingsNames = 5,
                 showLoadings = TRUE,
                 showLoadingsNames = FALSE,
                 ntopLoadings = 10,
                 pointSize = 4, 
                 legendLabSize = 15,
                 legendTitleSize = 16,
                 legendIconSize = 6)

# Retrieve PC and add as new variables to data frame 
PCAtools_mergePC <- p$rotated
```

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# 3D PCa plot ===============================================================================
# FactoMineR_p <- FactoMineR::PCA(df_pca,
#                                 scale.unit = FALSE)
# 
# scores <- as.data.frame(FactoMineR_p$ind$coord[,1:3]) %>%
#   rownames_to_column(., var = "File")
# category <- sapply(scores$File, function(f) {
#   unique(gc_hplc_list[[1]]$Category[match(f, gc_hplc_list[[1]]$File)])
# })
# 
# # Add to scores data frame
# scores$Category <- as.factor(category)
# 
# rgl::plot3d(scores$Dim.1, scores$Dim.2, scores$Dim.3,
#        col = as.integer(scores$Category),
#        xlab = "PC1",
#        ylab = "PC2",
#        zlab = "PC3",
#        type = "p")
# 
# # Add legend
# rgl::legend3d("topright", legend = levels(scores$Category), col = 1:length(levels(scores$Category)), pch = 16)

# factoextra::fviz_screeplot(FactoMineR_p)
# factoextra::fviz_pca_biplot(FactoMineR_p,
#                             habillage = unique(gc_hplc_list[[1]]$File))
```


### columns as categorical type

```{r , echo=FALSE, warning = FALSE}
input_df <- function(data) {
  # create sample df
  df_X_rq1 <- data %>%
    dplyr::select(File, 
                  Suspected_Polymer, # Category, # Suspected_Polymer,  Subcategory
                  collapsed_compound, 
                  Values) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    dplyr::group_by(
      Suspected_Polymer, # Category, # Suspected_Polymer, 
      collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    tidyr::pivot_wider(names_from =  Suspected_Polymer, # Category, #Suspected_Polymer, Subcategory
                       values_from = Values) %>%
    tibble::column_to_rownames(., var = "collapsed_compound")
  
  for (r in 1:nrow(df_X_rq1)) {
    df_X_rq1[r, which(base::is.na(df_X_rq1[r,]))] <- stats::runif(length(which(base::is.na(df_X_rq1[r,]))),
                                                                  min = sort(data$Values)[1]/100000,
                                                                  max = sort(data$Values)[2]/100000)
  }

  # table for information (rows are sample IDs, columns are sample information)
  metadata_X_rq1 <- data.frame(Suspected_Polymer # Category # Suspected_Polymer Subcategory
                               = colnames(df_X_rq1))
  Suspected_Polymer <- metadata_X_rq1$Suspected_Polymer # Category  # Suspected_Polymer
  metadata_X_rq1$Subcategory2 <- Subcategory2

  metadata_X_rq1 <- metadata_X_rq1 %>% tibble::column_to_rownames(., var = "Subcategory" # "Category" # "Suspected_Polymer"
                                                                  )
  
  return(list(df_X_rq1 ,metadata_X_rq1))
}
```

##### "Suspected_Polymer"

Since Suspected Polymer is the same for all 4 different categorization "groupings" only 1 PCA-biplot for each GC/HPLC/Combined data was generated.

Also, here for polymer type, we remove all the "Type == Unknown", which are just a few samples in the data set and does not provide good information for plastic categorization in general.

```{r , echo=FALSE, warning = FALSE, message = FALSE}
library("FactoMineR")
library("factoextra")
```

```{r , echo=FALSE, warning = FALSE, message = FALSE}
df_pca <- gc_hplc_list[[1]] %>%
    dplyr::select(NewFile, 
                  Suspected_Polymer, # Category, # Suspected_Polymer,  Subcategory
                  collapsed_compound, 
                  Values) %>%
  filter(., Suspected_Polymer %notin% "Unknown") %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    dplyr::group_by(
      Suspected_Polymer, # Category, # Suspected_Polymer, 
      collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    tidyr::pivot_wider(names_from =  collapsed_compound, # Category, #Suspected_Polymer, Subcategory
                       values_from = Values) %>%
    tibble::column_to_rownames(., var = "Suspected_Polymer")
  
for (r in 1:nrow(df_pca)) {
  df_pca[r, which(base::is.na(df_pca[r,]))] <- stats::runif(length(which(base::is.na(df_pca[r,]))),
                                                                min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
                                                                max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

res.pca <- FactoMineR::PCA(df_pca,
                           scale.unit = FALSE,
                           graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "ind", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = "Polymer type")


# df_pca <- input_df(hplc_list[[1]]) # gc_hplc_list // gc_list
# 
# # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
# p <- PCAtools::pca(mat = df_pca[[1]], 
#                    metadata = df_pca[[2]], 
#                    # center = FALSE,
#                    scale = FALSE 
# )
# 
# # A bi-plot =================
# PCAtools::biplot(p,
#                  lab = NULL, 
#                  colby = "Suspected_Polymer2" ,
#                  hline = 0, vline = 0,
#                  legendPosition = 'right', labSize = 5,
#                  sizeLoadingsNames = 5,
#                  showLoadings = TRUE,
#                  showLoadingsNames = FALSE,
#                  ntopLoadings = 10,
#                  pointSize = 4, 
#                  legendLabSize = 15,
#                  legendTitleSize = 16,
#                  legendIconSize = 6)
```

##### "Subcategory"

Similarly to the case Suspected Polymer demonstrated in code above, Subcategory is the same for all 4 different categorization "groupings" only 1 PCA-biplot for each GC/HPLC/Combined data was generated.

```{r , echo=FALSE, warning = FALSE, message=FALSE}
df_pca <- gc_hplc_list[[1]] %>%
    dplyr::select(NewFile, 
                  Subcategory,
                  collapsed_compound, 
                  Values) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    dplyr::group_by(
      Subcategory,
      collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    tidyr::pivot_wider(names_from =  collapsed_compound,
                       values_from = Values) %>%
    tibble::column_to_rownames(., var = "Subcategory")
  
for (r in 1:nrow(df_pca)) {
  df_pca[r, which(base::is.na(df_pca[r,]))] <- stats::runif(length(which(base::is.na(df_pca[r,]))),
                                                                min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
                                                                max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

res.pca <- FactoMineR::PCA(df_pca, 
                           scale.unit = FALSE,
                           graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "ind", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = "Product Subcategory")

# df_pca <- input_df(gc_list[[1]])
# 
# # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
# p <- PCAtools::pca(mat = df_pca[[1]], 
#                    metadata = df_pca[[2]], 
#                    # center = FALSE,
#                    scale = FALSE 
# )
# 
# # A bi-plot =================
# PCAtools::biplot(p,
#                  lab = NULL, 
#                  colby = "Subcategory2" ,
#                  hline = 0, vline = 0,
#                  legendPosition = 'right', labSize = 5,
#                  sizeLoadingsNames = 5,
#                  showLoadings = TRUE,
#                  showLoadingsNames = FALSE,
#                  ntopLoadings = 10,
#                  pointSize = 4, 
#                  legendLabSize = 15,
#                  legendTitleSize = 16,
#                  legendIconSize = 6)
```

##### "Category"

```{r , echo=FALSE, warning = FALSE}
df_pca <- gc_hplc_list[[2]] %>%
  dplyr::select(NewFile, 
                Category,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    Category,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "Category")

for (r in 1:nrow(df_pca)) {
  df_pca[r, which(base::is.na(df_pca[r,]))] <- stats::runif(length(which(base::is.na(df_pca[r,]))),
                                                            min = sort(gc_hplc_list[[2]]$Values)[1]/100000,
                                                            max = sort(gc_hplc_list[[2]]$Values)[2]/100000)
}

res.pca <- FactoMineR::PCA(df_pca, 
                           scale.unit = FALSE,
                           graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "ind", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = paste0("Combined GC-HPLC samples by Category grouping ", 2))


# df_pca <- input_df(df)
  # 
  # # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  # p <- PCAtools::pca(mat = df_pca[[1]], 
  #                    metadata = df_pca[[2]], 
  #                    # center = FALSE,
  #                    scale = FALSE 
  # )
  # 
  # # A bi-plot =================
  # plt_list[[i]] <- PCAtools::biplot(p,
  #                                   lab = NULL, 
  #                                   colby = "Category2" ,
  #                                   hline = 0, vline = 0,
  #                                   legendPosition = 'right', labSize = 5,
  #                                   sizeLoadingsNames = 5,
  #                                   showLoadings = TRUE,
  #                                   showLoadingsNames = FALSE,
  #                                   ntopLoadings = 10,
  #                                   pointSize = 4, 
  #                                   legendLabSize = 15,
  #                                   legendTitleSize = 16,
  #                                   legendIconSize = 6)

```

## Demo Hierarchical Clustering Analysis

If we filter unique compounds by File (compounds appear in at least 2 files), then all File in df in gc_hplc_list are the same. That's why here we used gc_hplc_list[[1]]

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Data prep for HCA
hc_df <- gc_hplc_list[[1]] %>%
  dplyr::select(File,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
    # stats::runif(length(which(base::is.na(hc_df[r,]))),
    #                                                       min = sort(gc_hplc_list[[1]]$Values)[1]/10000,
    #                                                       max = sort(gc_hplc_list[[1]]$Values)[2]/10000)
}
```

### Clustering of samples
```{r echo=FALSE, warning=FALSE, message=FALSE}
## Dissimilarity Indices Calculated by vegan::vegdist()
hca_samp <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                         method = "robust.aitchison"))
plot(hca_samp,  # Since our data is continous -> canberra // manhattan
     # // aitchison // robust.aitchison
     # labels = FALSE, 
     hang = -1,
     main = "Combined GC - HPLC data_Samples")
```

### Clustering of compounds
```{r echo=FALSE, warning=FALSE, message=FALSE}
## Dissimilarity Indices Calculated by vegan::vegdist()
hca_comp <- hclust(vegan::vegdist(t(as.matrix(hc_df)), 
                           method = "robust.aitchison"))
plot(hca,  # Since our data is continous -> canberra // manhattan
     # // aitchison // robust.aitchison
     # labels = FALSE, 
     hang = -1,
     main = "Combined GC - HPLC data_Compounds")
```

A smaller Aitchison distance between two samples would indicate that their compositional structures are similar in terms of the ratios between components.


For selecting Dissimilarity Indices Calculated by vegan::vegdist(), since our data is continous, not normally distributed, and the data is sparse (meaning our data has lots of NA or NULL before data imputation, This basically because there are compounds that occur in some samples and not in the others). Manhattan dissimilarity index separating ATDGCMS and HPLCTOFMS into 2 big clusters, while Canberra//Aitchison//Robust.aitchison grouped ATDGCMS as a subgroup of a part of HPLCTOFMS data.

Since for each observations of each sample, the sum of values of collapsed compounds is sum up to 100%, Aitchison or Robust.Aitchison are currently the best option for dissimilarity index.

***Why Aitchison or Robust.Aitchison for OUR DATA, AKA. Compositional Data?***

Compositional data have particular characteristics and constraints that standard distance metrics like Euclidean don't handle well. Here are some reasons why Aitchison distance is more suitable:

**Closure Constraint:** Because the components in compositional data sum to a constant value (like 1 or 100%), the data is subject to a closure constraint. This constraint violates the assumption of independence between variables that many standard statistical techniques rely on. Aitchison distance accounts for this constraint.

*Implications of Closure Constraint* 
*Loss of Degrees of Freedom:* Because of the closure constraint, one variable is effectively 'dependent' on the others. For instance, in our example, knowing the proportions of Additive A and Additive B is enough to determine the proportion of Additive C.

*Spurious Correlations:* The closure constraint can lead to spurious or misleading correlations between variables. For example, an increase in the proportion of Additive A will necessarily result in a decrease in the sum of the proportions of Additives B and C, even if there's no biological or chemical reason for them to be inversely related.

*Non-Independence of Components:* In compositional data, the components are not independent of each other, violating the assumptions of independence that many statistical tests and measures (like the Euclidean distance) rely on.

*Comparisons are Relative:* In compositional data, what matters is the relative sizes of the components, not their absolute sizes. Any analysis should, therefore, focus on these relative proportions rather than absolute values.

For these reasons, specialized statistical methods like Aitchison distance are often used for compositional data. These methods take the closure constraint into account and focus on the relative proportions of the different components, providing a more accurate and interpretable analysis.

In this workflow on tracking microplastic additives in aquatic environments, understanding the closure constraint can be pivotal. If you're looking at the relative abundances of different types of plastic additives in water samples, those abundances are compositional data subject to the closure constraint. Using standard statistical techniques without accounting for this constraint could lead to misleading conclusions.

**Relative Importance:** In compositional data, what's generally important is the relative proportion between components, rather than their absolute values. The Aitchison distance is designed to capture the relationships between these relative values effectively.

**Log-Ratio Transformation:** The Aitchison distance is based on log-ratio transformations, which have been proven to be an effective way to analyze compositional data. The log-ratio removes the closure constraint, making the data easier to analyze statistically.

**Handling Zeros:** The robust variant of Aitchison distance often includes techniques for handling zeros in compositional data, which are common and challenging to deal with.

**Scale-Invariance:** Aitchison distance is scale-invariant, meaning that multiplying a component by a constant will not change the distance. This is crucial for compositional data, where only the relative relationships between components matter.

**Interpretable:** The distance measure reflects compositional difference in a way that's meaningful within the context of the data, making it easier to understand and interpret the results.

## Demo 2-way HCA dendrogram
```{r , echo=FALSE, warning = FALSE, message=FALSE}
library(pheatmap)
# Input matrix - sample as rows, features as columns
pheatmap(as.matrix(hc_df),
         cluster_rows = hclust(as.dist(vegan::vegdist(as.matrix(hc_df[55:75,50:100]), 
                                              method = "robust.aitchison"))),
         cluster_cols = hclust(dist(t(as.matrix(hc_df[55:75,50:100])))),
         clustering_method = "complete",
         color = colorRampPalette(c("navy", "white", "firebrick3"))(50),
         border_color = "grey20",
         cellwidth = 5,
         cellheight = 5,
         fontsize = 7,
         show_rownames = TRUE,
         show_colnames = TRUE)

```

(12th,Oct,2023) Current Problem with two-way HCA dendrogram is that we have too many samples (n = 172) and too many compounds (n = 13046). => Proposed solution: grouping the compounds together via the function cutree(hca_comp, h = 100) and combine all compounds that belongs to a group together by taking the mean/median across all compounds that belongs to group for each sample. The process is as demonstrated below.

```{r , echo=FALSE, warning = FALSE, message=FALSE}
cut_tree_comp <- data.frame(group = cutree(hca_comp, h = 130))
cut_tree_comp <- rownames_to_column(cut_tree_comp, var = "collapsed_compound")
joined <- full_join(cut_tree_comp, 
                    gc_hplc_list[[1]] %>%
                      select(File, collapsed_compound, Values) %>%
                      group_by(File, collapsed_compound) %>%
                      summarise(mean_value = mean(Values)) %>%
                      pivot_wider(names_from = File, values_from = mean_value), 
                    by = "collapsed_compound") %>% column_to_rownames(., var = "collapsed_compound")

# Iterating through each sample and combine values of compounds (FUN = mean) that belongs to the same grouping
tets <- joined %>%
  group_by(group) %>%
  summarise(across(everything(), \(x) mean(x, na.rm = TRUE))) %>% 
  column_to_rownames(., var = "group")  

for (r in 1:nrow(tets)) {
  tets[r, which(base::is.na(tets[r,]))] <- stats::runif(length(which(base::is.na(tets[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/10000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/10000)
}

hca_samp <- hclust(vegan::vegdist(t(as.matrix(tets)),
                           method = "robust.aitchison"))
plot(hca_samp, hang = -1)

```
So with the dimensional reduction of compounds from n = 13046 to 33, we still achieve a somewhat good clustering result in which sample of the same category tends to cluster together.

#### Here, we use a non-parametric test, Wilcoxon test, because our data violates the assumptions for other parametric test, such as normality and equality of variance between 2 samples, as demonstrate below:

```{r , echo=FALSE, warning = FALSE}
### Examining for normal distribution ----------------
## Histogram 
# Create empty list 
histlist <- list()
# For loop creating histograms of each plastic type
i <- 1
for (plastic in unique(transpose_df$plastic_type)) {
  grouped_plastic <- which(transpose_df$plastic_type == plastic)
  histlist[[i]] <- ggplot() +
    geom_histogram(mapping = aes(as.vector(t(transpose_df[grouped_plastic, 3:ncol(transpose_df)])))) + 
    labs(x = plastic, title = plastic)
  i <- i + 1
}
# Appending each histogram to the empty list in order to view all of them
gridExtra::grid.arrange(grobs = histlist)

## QQ Plot 
# Create empty list 
qqlist <- list()
# For loop creating QQ plots of each plastic type
i <- 1 
# Appending QQ plots together 
par(mfrow=c(3,3))
for (product in unique(transpose_df$product_cat)) {
  grouped_product <- which(transpose_df$product_cat == product)
  qqlist[[i]] <- qqnorm(as.vector(t(transpose_df[grouped_product, 3:ncol(transpose_df)])), main = product, xlab = product, col = 'steelblue')
  qqline(as.vector(t(transpose_df[grouped_product, 3:ncol(transpose_df)])), col = 'red')
  i <- i + 1
}

### Conclusion = they are not normally distributed 

### Examining for equality of variance -----------------------------
# Leveneâ€™s test-non-normally distributed data (car::leveneTest()), significant if p-value < 0.05 
library(car)

results <- list()
for (col in 1:ncol(utils::combn(unique(transpose_df$product_cat), 2))) {
  # extract the combinations of plastic type pairs
  p_1 <- utils::combn(unique(transpose_df$product_cat), 2)[,col][1]
  p_2 <- utils::combn(unique(transpose_df$product_cat), 2)[,col][2]
  idx1 <- which(transpose_df$product_cat == p_1)
  idx2 <- which(transpose_df$product_cat == p_2)
  V1 <- as.vector(t(transpose_df[idx1, 3:ncol(transpose_df)]))
  V2 <- as.vector(t(transpose_df[idx2, 3:ncol(transpose_df)]))
  datat <- c(V1, V2)
  grouped <- as.factor(c(rep(p_1, times = length(V1)), rep(p_2, times = length(V2))))
  non_norm <- data.frame(datat, grouped)
  results[[paste0(p_1, p_2)]] <- car::leveneTest(datat ~ grouped, data = non_norm)
}
# -> p-value is 0.4096 which is greater the significance level of 0.05 therefore can conclude there is no significant difference between the variances


# Fligner-Killeen's test (fligner.test()), significant if p-value < 0.05 
fligner.test(datat ~ grouped, data = non_norm)
# p-value less than significance level therefore there is significant difference between variances.
# Test to determine the homogeneity of group variances. 
```

When we use combine gc_hplc data, 96.9556% of the observation in the resulting stats data frame are NA values. It would be bad to fill in this large amount of observations with LOD values, aka. leading to the inflation of small values in the data set.

one way to overcome this problem is group_by(Suspected_Polymer) or Category or Subcategory instead of by File =\> this is stupid because then for each comp in each product_cat only have 1 values ==\> it is meaningless to do Wilcoxon test comparing just 1 obs against 1 obs. ==\> we can go through each columns (compounds) and check which product_cat has more than x values and then only do the wilcoxon/ks test with those product_cat for that specific compound. IN this case, we don't need to fill our the missing values because the default for na.action of wilcox.test function is \*\*"na.omit", meaning all the NA values of the numerical vectors resulted from slicing into each column (compound) and each product_cat

## Demo K-means Clustering

### With Combined GC HPLC
```{r, echo=FALSE, warning = FALSE, message=FALSE}
# Input numeric matrix data : sample as rows, collapsed_compounds as columns
df <- gc_hplc_list[[1]] %>%
  dplyr::select(NewFile,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- stats::runif(length(which(base::is.na(df[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

tot.withinss <- list()
for (n_clus in 1:nrow(df)) {
  set.seed(round(runif(2:20, 0, 999)))
  km.out <- stats::kmeans(df, n_clus, nstart = 50)
  tot.withinss[[n_clus]] <- km.out$tot.withinss
  print(paste0("The number of centers is ", n_clus, " and total within-cluster sum of square is ", km.out$tot.withinss))
}

plot(df, col = (stats::kmeans(df, 171, nstart = 50)$cluster + 1) ,
main = "K- Means Clustering Results with K = 2",
xlab = "", ylab = "", pch = 20, cex = 2)


```


## Demo Wilcoxon test

### Wilcoxon test with HCA result

#### With Combined GC HPLC

Since for HCA, we operate on sample level and for each 4 subdf of each gc_hplc/gc/hplc_list, they all have the same sample column. Therefore, just using one of the subdf is enough.

Here, we want to find what is the optimal cut-off in HCA-cluster-tree 's Height and min_obs, which will result in the maximum number of significant compounds.

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_hplc_list[[1]] %>%
  dplyr::select(NewFile,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- stats::runif(length(which(base::is.na(hc_df[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "NewFile")


# Initialize variables
sig_comp <- list()

# Main loop
for (ele in 1:length(hca$height)) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "NewFile")
  joined <- full_join(t, stats, by = "NewFile")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    # Loop through each collapsed_compound (as columns)
    
    for (c in 3:ncol(joined)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                        joined[which(joined$group == p_2), c])$p.value
        pval_ks_test <- ks.test(joined[which(joined$group == p_1), c],
                                joined[which(joined$group == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                     paste0(p_1, "&", p_2), 
                                     pval_wilcox_test, 
                                     pval_ks_test)
      }
    }
    
    
    sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
    
    sig_comp[[paste0("Cut height_", ele)]] <- sp_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sp_df %>%
            filter(., adjusted_pvalue_holm < 0.05) %>%
            arrange(adjusted_pvalue_holm))
  }
}

writexl::write_xlsx(x = sig_comp, path = "Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result.xlsx")
```

#### With just GC

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_list[[1]] %>%
  dplyr::select(NewFile,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- stats::runif(length(which(base::is.na(hc_df[r,]))),
                                                          min = sort(gc_list[[1]]$Values)[1]/100000,
                                                          max = sort(gc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "NewFile")


# Initialize variables
sig_comp <- list()

# Main loop
for (ele in 56:length(hca$height)) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "NewFile")
  joined <- full_join(t, stats, by = "NewFile")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_sp, 2)[1,col]
      p_2 <- utils::combn(satisfied_sp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      pval_ks_test <- ks.test(joined[which(joined$group == p_1), c],
                              joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test, 
                                   pval_ks_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

writexl::write_xlsx(x = sig_comp, path = "Wilcoxon_test_GC_by_Robust Aichitson_HCA clustering result.xlsx")
```

The code below run wilcoxcon test on NA imputed data frame and don't perform minimum observation method. It reports the number of significant compounds at each cutree Height.

After running the code with NA imputed data, it generates significant compounds (at best case for GCMS data => 31 significant compounds) as compared to Zero significant compounds if using Missing-Observation Method. Therefore, from now on, for Wilcoxon on HCA results, we impute both the HCA input and the Wilcoxon input in the same way.

#### With just HPLC

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- hplc_list[[1]] %>%
  dplyr::select(NewFile,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- stats::runif(length(which(base::is.na(hc_df[r,]))),
                                                          min = sort(hplc_list[[1]]$Values)[1]/100000,
                                                          max = sort(hplc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "NewFile")


# Initialize variables
sig_comp <- list()

# Main loop
for (ele in 1:length(hca$height)) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "NewFile")
  joined <- full_join(t, stats, by = "NewFile")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 2) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_sp, 2)[1,col]
      p_2 <- utils::combn(satisfied_sp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      pval_ks_test <- ks.test(joined[which(joined$group == p_1), c],
                              joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test, 
                                   pval_ks_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

writexl::write_xlsx(x = sig_comp, path = "Wilcoxon_test_HPLC_by_Robust Aichitson_HCA clustering result.xlsx")
```

### Wilcoxon test without HCA clustering precursor

Previously, when using "product_cat" as the grouping factor, even with the p-value threshold of 0.5, we only get 4 compounds that significant between product_cat ==\> This means that most of the values between product_cat are really subtle and cannot really be detect with just Wilcoxon test. To examine the number of significant compounds that can be achieve through gradual "branching" of different levels of sample grouping, we first group them by Suspected_Polymer, then by Category, then finally by Subcategory

First we examined the Wilcoxon test of Suspect_POlymer // Category // Subcategory individually (not using gradual "branching"):

#### By Suspect_Polymer

Here we get all significant compounds from Suspected_Polymer that has minimum 1 -\> 6 obs and see how the number of significant compounds change (from at least 2 obs -\> maximum number of obs in category in stats df)

##### COMBINED GC_HPLC

###### with Missing observation method
```{r, echo=FALSE, warning = FALSE, message=FALSE}
list_suspect_pol <- list()

stats <- gc_hplc_list[[1]] %>%
    select(NewFile, collapsed_compound, Suspected_Polymer, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(NewFile, Suspected_Polymer, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "NewFile")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sus_p in unique(stats$Suspected_Polymer)) {
      # get all Suspected_Polymer that has > n obs
      if (sum(!is.na(stats[which(stats$Suspected_Polymer == sus_p), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sus_p)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Suspected_Polymer == p_1), c], 
                                        stats[which(stats$Suspected_Polymer == p_2), c])$p.value
        pval_ks_test <- ks.test(stats[which(stats$Suspected_Polymer == p_1), c],
                                stats[which(stats$Suspected_Polymer == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test, 
                                     pval_ks_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_suspect_pol[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_suspect_pol[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_suspect_pol[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == list_suspect_pol[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 3))
  }
  
  list_suspect_pol[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_suspect_pol[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  print(length(unique(list_suspect_pol[[paste0("Min_obs_", min_obs)]]$comp)))
}

writexl::write_xlsx(x = list_suspect_pol, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_polymer_Missing-Observation.xlsx")
```

###### with NA imputation method

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- gc_hplc_list[[1]] %>%
  select(NewFile, collapsed_compound, Suspected_Polymer, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(NewFile, Suspected_Polymer, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "NewFile")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- stats::runif(length(which(base::is.na(stats[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/10000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/10000)
}

sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  
# create vector of satisfied category 
satisfied_sp <- c()
for (cat in unique(stats$Suspected_Polymer)) {
  satisfied_sp <- c(cat, satisfied_sp)
}

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(satisfied_sp, 2)[1,col]
    p_2 <- utils::combn(satisfied_sp, 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Suspected_Polymer == p_1), c], 
                                    stats[which(stats$Suspected_Polymer == p_2), c])$p.value
    pval_ks_test <- ks.test(stats[which(stats$Suspected_Polymer == p_1), c],
                            stats[which(stats$Suspected_Polymer == p_2), c])$p.value
    # assigning row information
    sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                   paste0(p_1, " & ", p_2), 
                                   pval_wilcox_test, 
                                   pval_ks_test)
    
  }
}

sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")

suspect_pol <- sp_df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(suspect_pol)[1] == 0) {
  next
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(suspect_pol)) {
    idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == suspect_pol[row,]$comp)
    mean_rt <- c(mean_rt, mean(gc_hplc_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(gc_hplc_list[[1]][idx, ]$m.z))
  }
  
  suspect_pol$mean_rt <- mean_rt
  suspect_pol$mean_mz <- mean_mz
  
  print(suspect_pol %>% arrange(comp))
}


writexl::write_xlsx(x = suspect_pol, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_NA imputed.xlsx")
```

##### ATDGCMS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_suspect_pol <- list()

for (min_obs in 1:10) {
  stats <- gc_list[[1]] %>%
    select(NewFile, collapsed_compound, Suspected_Polymer, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(NewFile, Suspected_Polymer, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "NewFile")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sus_p in unique(stats$Suspected_Polymer)) {
      # get all Suspected_Polymer that has > n obs
      if (sum(!is.na(stats[which(stats$Suspected_Polymer == sus_p), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sus_p)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Suspected_Polymer == p_1), c], 
                                        stats[which(stats$Suspected_Polymer == p_2), c])$p.value
        pval_ks_test <- ks.test(stats[which(stats$Suspected_Polymer == p_1), c],
                                stats[which(stats$Suspected_Polymer == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test, 
                                     pval_ks_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_suspect_pol[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
   
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_suspect_pol[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_suspect_pol[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(hplc_list[[1]]$collapsed_compound == list_suspect_pol[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 3))
  }
  
  list_suspect_pol[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_suspect_pol[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  print(length(unique(list_suspect_pol[[paste0("Min_obs_", min_obs)]]$comp)))
}

writexl::write_xlsx(x = list_suspect_pol, path = "Wilcoxon_test_ATDGCMS_by_Suspected_polymer.xlsx")
```

##### HPLCTOFMS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_suspect_pol <- list()
for (min_obs in 1:10) {
  stats <- hplc_list[[1]] %>%
    select(NewFile, collapsed_compound, Suspected_Polymer, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(NewFile, Suspected_Polymer, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "NewFile")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sus_p in unique(stats$Suspected_Polymer)) {
      # get all Suspected_Polymer that has > n obs
      if (sum(!is.na(stats[which(stats$Suspected_Polymer == sus_p), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sus_p)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Suspected_Polymer == p_1), c], 
                                        stats[which(stats$Suspected_Polymer == p_2), c])$p.value
        pval_ks_test <- ks.test(stats[which(stats$Suspected_Polymer == p_1), c],
                                stats[which(stats$Suspected_Polymer == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test, 
                                     pval_ks_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_suspect_pol[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_suspect_pol[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_suspect_pol[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(hplc_list[[1]]$collapsed_compound == list_suspect_pol[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 3))
  }
  
  list_suspect_pol[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_suspect_pol[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }

  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  print(length(unique(list_suspect_pol[[paste0("Min_obs_", min_obs)]]$comp)))
}

writexl::write_xlsx(x = list_suspect_pol, path = "Wilcoxon_test_HPLC_by_Suspected_polymer.xlsx")
```

#### By Category

##### Do we need to change Data processing-Step 4 (Subsetting Non-unique compounds), for ex At step 4, we need to subset compounds that appear in more than 2 categories instead of 2 files? =\> NO!!

##### COMBINED GC_HPLC

###### with Missing-Observation method
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplc <- list()
for (i in 1:length(gc_hplc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_hplc_list[[i]] %>%
      select(NewFile, collapsed_compound, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(NewFile, Category, collapsed_compound) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
      column_to_rownames(., var = "NewFile")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          pval_ks_test <- ks.test(stats[which(stats$Category == p_1), c],
                                  stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test, 
                                         pval_ks_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gchplc[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_list[[i]]$collapsed_compound == sig_comp_gchplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp_gchplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp_gchplc, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_Missing-Observation.xlsx")
```

###### with NA imputation method

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplc <- list()

for (i in 1:length(gc_hplc_list)) {
  stats <- gc_hplc_list[[i]] %>%
    select(NewFile, collapsed_compound, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(NewFile, Category, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "NewFile")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- stats::runif(length(which(base::is.na(stats[r,]))),
                                                            min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
                                                            max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      pval_ks_test <- ks.test(stats[which(stats$Category == p_1), c],
                              stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test, 
                                     pval_ks_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gchplc[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_list[[i]]$collapsed_compound == sig_comp_gchplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp_gchplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp_gchplc, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_NA imputed.xlsx")
```

##### ATDGCMS

###### Missing observation method
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp <- list()
for (i in 1:length(gc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_list[[i]] %>%
      select(NewFile, collapsed_compound, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(NewFile, Category, collapsed_compound) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
      column_to_rownames(., var = "NewFile")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      # initiate a vector for satisfied categories
      satisfied_cat <- c()
      # Loop through each category
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          pval_ks_test <- ks.test(stats[which(stats$Category == p_1), c],
                                  stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test, 
                                         pval_ks_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_list[[i]]$collapsed_compound == sig_comp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp, path = "Wilcoxon_test_ATDGCMS_by_Category.xlsx")
```

##### HPLCTOFMS

###### Missing Observation method
```{r, echo=FALSE, warning = FALSE, message=FALSE}
sig_comp <- list()
for (i in 1:length(hplc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- hplc_list[[i]] %>%
      select(NewFile, collapsed_compound, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(NewFile, Category, collapsed_compound) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
      column_to_rownames(., var = "NewFile")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      # Initialize vector for satisfied categories
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          pval_ks_test <- ks.test(stats[which(stats$Category == p_1), c],
                                  stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & " ,p_2), 
                                         pval_wilcox_test, 
                                         pval_ks_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_list[[i]]$collapsed_compound == sig_comp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp, path = "Wilcoxon_test_HPLC_by_Category.xlsx")
```

#### By Subcategory

Since the subcategory is the same across all 4 different consumer-based perspective groupings, we only need to run one dataframe of the list of df (this principle applies for either gc_hplc_list or gc_list or hplc_list).
##### Do we need to change Data processing-Step 4 (Subsetting Non-unique compounds), for ex At step 4, we need to subset compounds that appear in more than 2 categories instead of 2 files? =\> NO!!

##### COMBINED GC_HPLC

###### With Missing Observation method
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- gc_hplc_list[[1]] %>%
    select(NewFile, collapsed_compound, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(NewFile, Subcategory, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "NewFile")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
        pval_ks_test <- ks.test(stats[which(stats$Subcategory == p_1), c],
                                stats[which(stats$Subcategory == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test, 
                                     pval_ks_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 3))
  }
  
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  print(length(unique(list_subcat[[paste0("Min_obs_", min_obs)]]$comp)))
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Subcategory_Missing-observation.xlsx")
```

###### With NA imputation method

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- gc_hplc_list[[1]] %>%
  select(NewFile, collapsed_compound, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(NewFile, Subcategory, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "NewFile")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- stats::runif(length(which(base::is.na(stats[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/10000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/10000)
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
  
# create vector of satisfied category 
satisfied_subcat <- c()
for (subcat in unique(stats$Subcategory)) {
  satisfied_subcat <- c(subcat, satisfied_subcat)
}

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(satisfied_subcat, 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(satisfied_subcat, 2)[1,col]
    p_2 <- utils::combn(satisfied_subcat, 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    pval_ks_test <- ks.test(stats[which(stats$Subcategory == p_1), c],
                            stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                                   paste0(p_1, " & ", p_2), 
                                   pval_wilcox_test, 
                                   pval_ks_test)
    
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(sub_cat)[1] == 0) {
  next
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(sub_cat)) {
    idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == sub_cat[row,]$comp)
    mean_rt <- c(mean_rt, mean(gc_hplc_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(gc_hplc_list[[1]][idx, ]$m.z))
  }
  
  sub_cat$mean_rt <- mean_rt
  sub_cat$mean_mz <- mean_mz
  
  print(sub_cat %>% arrange(comp))
}


writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_NA imputed.xlsx")
```

When we ran a loop to investigate the significant compounds that can be obtained from getting all product_cat that has \> 3 to 6 obs, \> 5 obs gave the highest amount of significant compound.

##### ATDGCMS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp <- list()

for (i in 1:length(gc_list)) {
  list_subcat <- list()
  stats <- gc_list[[i]] %>%
      select(NewFile, collapsed_compound, Subcategory, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(NewFile, Subcategory, collapsed_compound) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
      column_to_rownames(., var = "NewFile")
  
  for (min_obs in 1:10) {
    subcat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_subcat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Subcategory)) {
        # get all Subcategory that has > min_obs
        if (sum(!is.na(stats[which(stats$Subcategory == cat), c])) > min_obs) {
          satisfied_subcat <- c(satisfied_subcat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_subcat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_subcat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_subcat, 2)[1,col]
          p_2 <- utils::combn(satisfied_subcat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                          stats[which(stats$Subcategory == p_2), c])$p.value
          pval_ks_test <- ks.test(stats[which(stats$Subcategory == p_1), c],
                                  stats[which(stats$Subcategory == p_2), c])$p.value
          # assigning row information
          subcat_df[nrow(subcat_df) + 1,] <- c(colnames(stats)[c], 
                                               paste0(p_1, " & ", p_2), 
                                               pval_wilcox_test, 
                                               pval_ks_test)
        }
      }
    }
    
    subcat_df$adjusted_pvalue_holm <- stats::p.adjust(subcat_df$pval_wilcox_test, method = "holm")
    
    list_subcat[[min_obs]] <- subcat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_subcat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp[[paste0("Grouping_", i)]] <- list_subcat[[which.max(lapply(list_subcat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_list[[i]]$collapsed_compound == sig_comp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp, path = "Wilcoxon_test_ATDGCMS_by_Subcategory.xlsx")
```

##### HPLCTOFMS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp <- list()

for (i in 1:length(hplc_list)) {
  list_subcat <- list()
  stats <- hplc_list[[i]] %>%
      select(NewFile, collapsed_compound, Subcategory, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(NewFile, Subcategory, collapsed_compound) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
      column_to_rownames(., var = "NewFile")
  
  for (min_obs in 1:10) {
    subcat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer(), pval_ks_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_subcat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Subcategory)) {
        # get all Subcategory that has > min_obs
        if (sum(!is.na(stats[which(stats$Subcategory == cat), c])) > min_obs) {
          satisfied_subcat <- c(satisfied_subcat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_subcat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_subcat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_subcat, 2)[1,col]
          p_2 <- utils::combn(satisfied_subcat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                          stats[which(stats$Subcategory == p_2), c])$p.value
          pval_ks_test <- ks.test(stats[which(stats$Subcategory == p_1), c],
                                  stats[which(stats$Subcategory == p_2), c])$p.value
          # assigning row information
          subcat_df[nrow(subcat_df) + 1,] <- c(colnames(stats)[c], 
                                               paste0(p_1, " & ", p_2), 
                                               pval_wilcox_test, 
                                               pval_ks_test)
        }
      }
    }
    
    subcat_df$adjusted_pvalue_holm <- stats::p.adjust(subcat_df$pval_wilcox_test, method = "holm")
    
    list_subcat[[min_obs]] <- subcat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>% # 0.05
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_subcat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp[[paste0("Grouping_", i)]] <- list_subcat[[which.max(lapply(list_subcat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_list[[i]]$collapsed_compound == sig_comp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp, path = "Wilcoxon_test_HPLC_by_Subcategory.xlsx")
```

## Demo RF

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(randomForestSRC)

rfsrc.result <- function(dat, split.ratio) {
  set.seed(1234)
  
  plastic_idx <- caret::createDataPartition(dat$Category, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]
  
  # Train

  rf <- randomForestSRC::rfsrc(Category ~ ., 
                                    ntree=1000, 
                                    splitrule = "auc", 
                                    nodesize = 1, #Minumum size of terminal node for classification (1)
                                    # mtry = 21,
                                    importance = "permute", 
                                    samptype = "swr",
                                    membership = TRUE,
                                    perf.type="misclass",
                                    block.size = 1, # cumulative error rate on every tree
                                    data = plastic_trn
  )
  
  # Prediction results
  pred_res <- predict(rf, newdata = plastic_tst, outcome = "test")$predicted
  rownames(pred_res) <- rownames(plastic_tst)
  
  # Variable importance
  imp_var <- vimp(rf, importance = "permute")$importance
  
  # md.obj <- max.subtree(mergePC.rf)
  # best.feature <- md.obj$topvars # extracts the names of the variables in the object md.obj
  
  return(list(rf, 
              pred_res, 
              # pred_avg, 
              imp_var))
}

rfsrc.plots <- function(preddat, rf.mod) {
  newdat <- as.data.frame(preddat) %>%
    tibble::rownames_to_column(., var = "NewFile") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  
  # plots <- list()
  # Make bar graph of prediction probability -----
  a <- ggplot(data = newdat) + 
    geom_col(aes(x = NewFile, y = prob, fill = Category), 
             position = "dodge" # separating stacking prob cols
    ) +
    scale_fill_brewer(palette = "Set2") +
    scale_y_continuous(n.breaks = 10) +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 90))
  
  # plot OOB error rate against the number of trees -------
  b <- plot(ggRandomForests::gg_error(rf.mod))
  
  # Estimate the variables importance --------
  my.rf.vimp <- ggRandomForests::gg_vimp(rf.mod, nvar = 100) # provides the predictor's importance of top 100 predictors
  # plots[[3]] <- 
  c <- plot(my.rf.vimp) # visualises the predictorâ€™s importance

  return(list(a,b,c))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df <- gc_list[[1]] %>%
  dplyr::select(NewFile, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(NewFile, Category, collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(gc_list[[1]]$Values)[1],
                                             max = sort(gc_list[[1]]$Values)[2])
}

rfsrc.result(dat = df, split.ratio = 0.6)[[2]]

rfsrc.plots(preddat = rfsrc.result(dat = df, split.ratio = 0.6)[[2]],
            rf.mod = rfsrc.result(dat = df, split.ratio = 0.6)[[1]])

```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(gc_list)) {
  df_pca <- input_df(gc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[item]][which(gc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

### With just HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
df <- hplctofms %>%
  dplyr::select(File, product_cat, collapsed_compound, Values) %>%
  mutate(product_cat = factor(product_cat, levels = unique(product_cat))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, product_cat, collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(hplctofms$Values)[1],
                                             max = sort(hplctofms$Values)[2])
}

RFresult <- rfsrc.result(dat = df, split.ratio = 0.6)
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[item]][which(hplc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE,message=FALSE}
df <- gc_hplc_list[[1]] %>%
  dplyr::select(File, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min1 = sort(gc_hplc_list[[1]]$Values)[1],
                                             min2 = sort(gc_hplc_list[[1]]$Values)[2])
}

RFresult <- rfsrc.result(dat = df, split.ratio = 0.6)

# Make column of true Category of each row
preddat <- base::as.data.frame(RFresult[[2]]) %>% 
  tibble::rownames_to_column(., var = "File") 

preddat$true_category <- gc_hplc_list[[1]][match(preddat$File, gc_hplc_list[[1]]$File),]$Category

# For each row of prediction df, return the column name (aka. Category) with largest prediction probability
predres<- data.frame(predict_cat = colnames(preddat[, 2:6])[apply(preddat[, 2:6], 1, which.max)], 
                     true_cat = preddat$true_category)

rfsrc.plots(preddat = RFresult[[2]],
            rf.mod = RFresult[[1]])
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[item]][which(gc_hplc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

## CONCLUSION OF RANDOMFOREST ON COMBINATION OF ANALYTICAL DATA

#### =\> For single ATDGCMS or HPLCTOF or Combined ATD/HPLC, all of them gave out the same (OOB) Requested performance error \~ 0.4. All of them have the same confusion matrix results, in every single category was mistakenly categorized as Food contact materials !

### With minimum sample size - Justa test, maybe irrelevant later on

#### PCAtools

```{r, echo=FALSE, warning = FALSE}
PCAtools_mindf <- ptype_n_samp(PCAtools_mergePC, use = "min")
PCAtools_mindf.RFresult <- rfsrc.result(PCAtools_mindf, split.ratio = 0.6, sep = FALSE)
rfsrc.plots(preddat = PCAtools_mindf.RFresult[[2]],
            rf.mod = PCAtools_mindf.RFresult[[1]])
```

### What is the average accuracy for USSB and USE for each product category?

For Mixed_Plastic_Waste, Other, Plastic_Bottles_and_Bottle_Caps, Food_Packaging_Waste, there is a lack of Store-Bought samples.

```{r, echo=FALSE, warning = FALSE}
print(PCAtools_alldf.RFresult[[3]] %>% arrange(plastic_type))
```

### What happens to the prediction accuracy if we use only USSB to predict USE samples? If product doesn't have USSB, then train w/ USE

```{r, echo=FALSE, warning = FALSE, }
PCAtools_alldf.RFresult <- rfsrc.result(PCAtools_alldf, split.ratio = 0.6, sep = TRUE)
rfsrc.plots(preddat = PCAtools_alldf.RFresult[[2]],
            rf.mod = PCAtools_alldf.RFresult[[1]])
```

## Demo LightGBM

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(lightgbm)
library(gridExtra)

lightgbm.result <- function(dat, split.ratio) { # dat must have same format as PCATools input
  
  test <- copy(dat)
  # Must convert product_cat from factors to numeric
  test$Category <- as.numeric(as.factor(test$Category)) - 1L
  # Split train and test sets
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(test$Category, p = 0.6, list = F)
  plastic_trn <- as.matrix(test[plastic_idx, ])
  plastic_tst <- as.matrix(test[-plastic_idx, ])
  
  dtrain <- lgb.Dataset(data = plastic_trn[, 2:ncol(plastic_trn)], # data is all the columns, aka. all the compounds
                        label = plastic_trn[, 1]) # label is Category column
  dtest <- lgb.Dataset.create.valid(dtrain, 
                                    data = plastic_tst[, 2:ncol(plastic_tst)], 
                                    label = plastic_tst[, 1])
  
  # Validation set to be used during training, not testing!!!
  valids <-  list(test = dtest)
  
  # Setup parameters
  params <- list(
    min_data = 1L
    , learning_rate = 0.1
    , objective = "multiclass"
    , metric = "multi_error"
    , num_class = length(unique(dat$Category))
    , boosting = "dart"
    , xgboost_dart_mode ="true"
    , num_threads = 2
  ) 
  
  lgb.model <- lgb.train(params,
                         dtrain,
                         nrounds = 1000,
                         valids, 
                         early_stopping_rounds = 25L
  )
  
  # prediction
  my_preds <- round(predict(object = lgb.model, 
                      data = plastic_tst[, 2:ncol(plastic_tst)],
                      # params = list(output_result = "response"),
                      reshape = TRUE),
                    3)

  colnames(my_preds) <- levels(dat$Category)
  rownames(my_preds) <- rownames(plastic_tst)
  
  newdat <- as.data.frame(my_preds) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  
  # Important features
  tree_imp <- lgb.importance(lgb.model, percentage = T)
  varimpmat <- lgb.plot.importance(tree_imp, measure = "Gain")

  return(list(my_preds, varimpmat, newdat))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# gbm_list <- list()
# for (i in 1:length(gc_list)) {
  df <- gc_list[[2]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_list[[2]]$Values)[1],
                                               max = sort(gc_list[[2]]$Values)[2])
  }
  
  # gbm_list[[i]]
gbm <- lightgbm.result(df, split.ratio = 0.6)
# }

# Make bar graph of prediction probability
ggplot(data = gbm[[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

Here, same problem happened where all most all the sample in test set were misclassified as "Food contact materials"

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_list)) {
  
  df_pca <- input_df(gc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[i]][which(gc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

With PCA df, there is a slight improvement, not everything is classified as food contact material anymore, but there are still lots of misclassification, for ex, some Balloons samples are classified as MPW or as clothes or Misc

### With just HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(hplc_list)) {
  df <- hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(hplc_list[[i]]$Values)[1],
                                               max = sort(hplc_list[[i]]$Values)[2])
  }
  
  gbm_list[[i]] <- lightgbm.result(df, split.ratio = 0.6)
}

# Make bar graph of prediction probability
ggplot(data = gbm_list[[1]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[i]][which(hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df <- gc_hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_hplc_list[[i]]$Values)[1],
                                               max = sort(gc_hplc_list[[i]]$Values)[2])
  }
  
  gbm_list[[i]] <- lightgbm.result(df, split.ratio = 0.6)
}

# Make bar graph of prediction probability
ggplot(data = gbm_list[[1]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[i]][which(gc_hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

## CONCLUSION OF LIGHTGBM ON COMBINATION OF ANALYTICAL DATA

#### =\> Regardless of single or combined dataset, the same misclassification pattern occured, in which almost everything was misclassified as Food Contact Materials (The error is always \> 0.3, EXCEPT FOR 1 Scenario, where single ATDGCMS-PCA merge df was used --\> error in this is \~ 0.24)

## Demo SVM

#### Data format should be same as PCATools input (column as "sample name", row as "collapsed_compounds")

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(e1071)
library(caret)
library(ROCR)
library(MLmetrics)


# e1071 package: ===========================
# PArtitioning train&test sets / training / predict on test set
e1071.SVM.result <- function(dat, split.ratio){
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(dat$Category, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]  
  
  # Create 1 million evenly spaced values on a log scale between 10^-6 and 10^2
  # lseq <- function(from = 0.000001, to = 100, length.out=1000000) {
  #   # logarithmic spaced sequence
  #   exp(seq(log(from), log(to), length.out = length.out))
  # }
  
  # Model tuning
  tune.out <- e1071::tune(e1071::svm, Category ~ ., 
                          data = plastic_trn,
                          kernel = "radial",
                          scale = TRUE,
                          # ranges = list(cost = lseq()),
                          decision.values = TRUE, 
                          probability = TRUE)
  bestmod <- tune.out$best.model
  
  # Prediction results
  pred_prob <- predict(bestmod, newdata = plastic_tst,
                       decision.values = TRUE, probability = TRUE)
  pred_res <- attr(pred_prob, "probabilities")
  
  return(pred_res)
}

plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}  
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
svm_list <- list()
for (i in 1:length(gc_list)) {
  df <- gc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_list[[i]]$Values)[1],
                                               max = sort(gc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# PCA-based feature data
svm_list <- list()
for (i in 1:length(gc_list)) {
  df_pca <- input_df(gc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[i]][which(gc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With just HPLC data

#### Original Data

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(hplc_list)) {
  df <- hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(hplc_list[[i]]$Values)[1],
                                               max = sort(hplc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[i]][which(hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df <- gc_hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_hplc_list[[i]]$Values)[1],
                                               max = sort(gc_hplc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[i]][which(gc_hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

## CONCLUSION SVM ON COMBINATION OF ANALYTICAL DATA

#### =\> The same problem with RF and GBM: Almost ALL of the classes are misclassified as Food contact material -\_\_-

## Demo Multinomial Logistic Regression

```{r, echo=FALSE, warning = FALSE}
library(class)
library(caret)
library(nnet)

## Link: https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/

# PArtitioning train&test sets / training / predict on test set
caret.multinomlog.result <- function(dat, split.ratio){
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(dat$Category, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]  
  
  plastic_multinomlog_mod <- caret::train(
    Category ~ .,
    data = plastic_trn,
    method = "multinom", # Penalized Multinomial Regression -> https://remiller1450.github.io/s230f19/caret3.html
    trControl = trainControl(method = "cv", # repeatedcv", 
                             number = 5,
                             # repeats = 3, # for repeated k-fold cv only 
                             verboseIter = TRUE
                             ),
    trace = FALSE,
    MaxNWts = 8 * (ncol(plastic_trn)) # maximum allowable number of weights
  )
  
  # Prediction results
  pred_res <- round(predict(plastic_multinomlog_mod, newdata = plastic_tst, type = "prob"), 5)
  
  # Interpreting variable importance for multinomial logistic regression - `nnet::multinom()` and `caret::varImp()`
  # https://stackoverflow.com/questions/60060292/interpreting-variable-importance-for-multinomial-logistic-regression-nnetmu
  varimp <- caret::varImp(plastic_multinomlog_mod)

  return(list(pred_res, varimp))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df <- gc_list[[2]] %>%
  dplyr::select(File, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(gc_list[[2]]$Values)[1],
                                             max = sort(gc_list[[2]]$Values)[2])
}

multinom <- caret.multinomlog.result(df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- multinom[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(multinom[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE}
# PCA-based feature data
df_pca <- input_df(gc_list[[1]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(gc_list[[1]][which(gc_list[[1]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_atdgcms.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_atdgcms.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_atdgcms.multinomlog.result[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With just HPLC data

#### Original Data

```{r, echo=FALSE, warning = FALSE}
df <- hplc_list[[2]] %>%
  dplyr::select(File, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(hplc_list[[2]]$Values)[1],
                                             max = sort(hplc_list[[2]]$Values)[2])
}

multinom <- caret.multinomlog.result(df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- multinom[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(multinom[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools-merged df

```{r, echo=FALSE, warning = FALSE}
df_pca <- input_df(hplc_list[[1]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(hplc_list[[1]][which(hplc_list[[1]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_hplc.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_hplc.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_hplc.multinomlog.result[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
df <- gc_hplc %>%
  dplyr::select(File, product_cat, collapsed_compound, Values) %>%
  mutate(product_cat = factor(product_cat, levels = unique(product_cat))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, product_cat, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(gc_hplc$Values)[1],
                                             max = sort(gc_hplc$Values)[2])
}

gc_hplc.multinomlog.result <- caret.multinomlog.result(df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- gc_hplc.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "product_cat", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(gc_hplc.multinomlog.result[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = product_cat), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df_pca <- input_df(gc_hplc_list[[3]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(gc_hplc_list[[3]][which(gc_hplc_list[[3]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_gchplc.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_gchplc.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

print(ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black')))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_gchplc.multinomlog.result[[1]])

print(ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90)))
```

Food contact materials may have a lot of chemical properties in them that shares with other categories.
