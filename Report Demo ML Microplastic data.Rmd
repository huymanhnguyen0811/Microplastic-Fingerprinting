---
title: "Microplastic fingerprinting"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width='750px', dpi=200)
```

## Documentation

This repo is accompanying the publication: "Computational fingerprinting workflow for environmental source tracking of microplastic based on representative additives"

Demo of each ML algorithms is shown below.

## Data processing

You can include R code in the document as follows:

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(writexl)
library(tidyr)
library(grid)
library(gridExtra)
library(PCAtools)
library(stats)
library(FactoMineR)
library(factoextra)
library(compositions)
library(ggforce)
library(latticeExtra)
library(cluster)

# Functions -------------------------------------------------------------------------------------------------------
'%notin%' <- Negate('%in%')

# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp <- function(data, rtthres1, rtthres2, mzthres, type) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rtthres: RT threshold window
  # mzthres: mz threshold window
  dat <- data[ , , drop = FALSE]  # Shallow copy to avoid modification of original data
  
  # Initialize the compound column filled with NA values
  dat$Feature <- NA
  i <- 1
  j <- 1 # Counter for HPLC data only
  
  # Checkpoint for 'type' parameter
  if (!(type %in% c("ATDGCMS", "HPLCTOFMS"))) {
    stop("Invalid type specified. Must be either 'ATDGCMS' or 'HPLCTOFMS'")
  }

  # Loop through each row to group compounds
  if (type %in% "HPLCTOFMS") {
      # Split HPLC data into RT <= 6 min and RT > 6 min
      dat1 <- dat %>% filter(RT <= 6)
      dat2 <- dat %>% filter(RT > 6)
      
      for (row in 1:nrow(dat1)) {
        # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
        rt <- base::as.numeric(dat1[row, "RT"])
        mz <- base::as.numeric(dat1[row, "m.z"])
        
        idx1 <- which(
          dat1$RT <= (rt + rtthres1) & dat1$RT >= (rt - rtthres1) &
            dat1$m.z <= (mz + mzthres) & dat1$m.z >= (mz - mzthres) &
            is.na(dat1$Feature)
        )
        
        if (length(idx1) > 0) {
          dat1[idx1, "Feature"] <- paste0("Compound_RT1_", i, ".", type)
          i <- i + 1
        }  
      }
      
      for (row in 1:nrow(dat2)) {
        # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
        rt <- base::as.numeric(dat2[row, "RT"])
        mz <- base::as.numeric(dat2[row, "m.z"])
        
        idx2 <- which(
          dat2$RT <= (rt + rtthres2) & dat2$RT >= (rt - rtthres2) &
            dat2$m.z <= (mz + mzthres) & dat2$m.z >= (mz - mzthres) &
            is.na(dat2$Feature)
        )
        
        if (length(idx2) > 0) {
          dat2[idx2, "Feature"] <- paste0("Compound_RT2_", j, ".", type)
          j <- j + 1
        }  
      }
      
      dat <- rbind(dat1, dat2)
      
  } else {
    for (row in 1:nrow(dat)) {
      
      # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
      rt <- base::as.numeric(dat[row, "RT"])
      mz <- base::as.numeric(dat[row, "m.z"])
      
      idx <- which(
        dat$RT <= (rt + rtthres1) & dat$RT >= (rt - rtthres1) &
          dat$m.z <= (mz + mzthres) & dat$m.z >= (mz - mzthres) &
          is.na(dat$Feature)
      )
      
      if (length(idx) > 0) {
        dat[idx, "Feature"] <- paste0("Compound_", i, ".", type)
        i <- i + 1
      }  
    }
  }
  
  return(dat)
}


# Filtering similar and unique compound 

comp_filter <- function(data) {
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()

  for (comp_grp in unique(data$Feature)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$Feature, fixed = TRUE))
    
    if (length(idx) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    } else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_other_compounds_idx, all_unique_compounds_idx))
}
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# STEP 1.1: Data import --------------------------------------------

# ATDGCMS
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/ATDGCMS")

file_list1 <- list.files(pattern = '*.csv') %>%
  .[!str_detect(., "2022")] %>%
  .[!str_detect(., "USE_08")] %>%
  .[!str_detect(., "USE_10")] %>%
  .[!str_detect(., "USE_16")] %>%
  .[!str_detect(., "USE_17")] %>%
  .[!str_detect(., "USE_18")] %>%
  .[!str_detect(., "USE_19")] %>%
  .[!str_detect(., "USE_20")] %>%
  .[!str_detect(., "USE_21")]
  # .[str_detect(., "_USE")] # exclude environmental samples

# Blank samples 
blank_list1 <- list.files(pattern = '*.csv') %>%
  .[str_detect(., "2022-")]

# Import samples to list
df_list1_step1.1 <- purrr::map(file_list1, read.csv)

df_list1_blank <- purrr::map(blank_list1, read.csv)

df_blank1 <- dplyr::bind_rows(df_list1_blank)

# # Sample information ATDGCMS
# sampleinfo1 <- readxl::read_excel(paste0(getwd(), '/SampleInfo.xlsx'))
# colnames(sampleinfo1)[1] <- 'File'
# 
# # sampleinfo1 <- sampleinfo1 %>%
# #   filter(., !str_detect(File, "_USE")) # exclude environmental samples
# 
# sampleinfo1$`Collection Date (YYYY-MM-DD)` <- as.Date(as.numeric(sampleinfo1$`Collection Date (YYYY-MM-DD)`), origin = "1899-12-30")

# HPLCTOFMS
setwd("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/HPLCTOFMS")

file_list2 <- list.files(pattern = '*.xls') %>%
  .[!str_detect(., "Blank")] %>%
  .[!str_detect(., "Info")]

# Blank samples 
blank_list2 <- list.files(pattern = '*.xls') %>%
  .[str_detect(., "Blank")]

# Import samples to list
df_list2_step1.1 <- purrr::map(file_list2, read_xls, skip = 1)

df_list2_blank <- purrr::map(blank_list2, read_xls, skip = 1)

df_blank2 <- dplyr::bind_rows(df_list2_blank)
```

#### Quality assurance for STEP 1.2: Plot Percentage coverage after removal of limit observation----------------------------
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# QUALITY CONTROL A OF STEP 1.2: Plot Percentage coverage after removal of limit observation----------------------------
plot_a <- list()
j <- 1
for (i in 1:length(df_list2_step1.1)) {
  for (threshold in c(seq(from = 0, to = 10000, by = 1000))) {
    df_filter_area <- df_list2_step1.1[[i]] %>%
      filter(., Height >= threshold)
    coverage <- sum(df_filter_area$Height)*100/sum(df_list2_step1.1[[i]]$Height)
    coverage_drop <- 100 - coverage
    df <- data.frame(thres = threshold, coverage = coverage,
                     coverage_drop = coverage_drop, file = unique(df_list2_step1.1[[i]]$File))
    plot_a[[j]] <- df
    j <- j+1
  }
}

# Examine the Average percentage coverage at each threshold of removal
View(bind_rows(plot_a) %>% group_by(thres) %>% summarise(across(coverage, base::mean)) %>% mutate(coverage = round(coverage, 3)))

# Examine the Average drop from original data (without any removal) in percentage coverage at each threshold of removal
View(bind_rows(plot_a) %>% group_by(thres) %>% summarise(across(coverage_drop, base::mean)) %>% mutate(coverage_drop = round(coverage_drop, 3)))

# QUALITY CONTROL B OF STEP 1.2: Plot number of peak remains after removal of limit observation----------------------------
plot_b <- list()
j <- 1
for (i in 1:length(df_list2_step1.1)) {
  for (threshold in c(seq(from = 0, to = 10000, by = 1000))) {
    df_filter_area <- df_list2_step1.1[[i]] %>%
      filter(., Height >= threshold)
    peak_remain <- dim(df_filter_area)[1]
    peaknumber_drop <- dim(df_list2_step1.1[[i]])[1] - peak_remain
    df <- data.frame(thres = threshold, peak_remain = peak_remain,
                     peaknumber_drop = peaknumber_drop, file = unique(df_list2_step1.1[[i]]$File))
    plot_b[[j]] <- df
    j <- j + 1
  }
}

# Examine the average number of Average percentage coverage at each threshold of removal
View(bind_rows(plot_b) %>% group_by(thres) %>% summarise(across(peak_remain, base::mean)) %>% mutate(peak_remain = round(peak_remain, 0)))

# Examine the average number of Average percentage coverage at each threshold of removal
View(bind_rows(plot_b) %>% group_by(thres) %>% summarise(across(peaknumber_drop, base::mean)) %>% mutate(peaknumber_drop = round(peaknumber_drop, 0)))
```


#### STEP 1.2 Filtering out chemical noise (aka. setting limit of observations) of samples (NOT blanks)

(Update 08 Feb 2024:) Prof. Mojca Usaj recommended that we should not remove these 'chemical Noises' since it will make our ML model more robust in the end.
An evaluation with the PCA biplot was done comparing between 2 cases: with and without removal of these small 'chemical Noises' => there is almost NO difference between these two cases in the PCA biplot across GC and HPLC data.

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS

# list1_remaining_area <- list()
# list1_removed_area <- list()
# for (i in 1:length(df_list1_step1.1)) {
#   list1_remaining_area[[i]] <- df_list1_step1.1[[i]] %>%
#     filter(., Area > 100000)
# 
#   list1_removed_area[[i]] <- df_list1_step1.1[[i]] %>%
#     filter(., Area <= 100000)
# }

# HPLCTOFMS
# list2_remaining_area <- list()
# list2_removed_area <- list()
# for (i in 1:length(df_list2_step1.1)) {
#   list2_remaining_area[[i]] <- df_list2_step1.1[[i]] %>%
#     filter(., Height > 5000)
# 
#   list2_removed_area[[i]] <- df_list2_step1.1[[i]] %>%
#     filter(., Height <= 5000)
# }

# STEP 1.3: Grouping compounds based on Retention time and molecular ions  -----------------------------------------------------------------------
# STEP 1.3A: Generate 1 grand data frame

# ATDGCMS -> in contrast to HPLC data, more variation later in the run , more consistency in the early elution
df1 <- bind_rows(df_list1_step1.1) %>%
  select(-c("Start", "End", "Width", "Base.Peak", "Cpd", "Label", "Height", "Ions")) %>%
  # Change all ATDGCMS file name with underscore-separated to hyphen-separated
  dplyr::mutate(File = gsub("_", "-", File)) %>%
  mutate(type = "Sample")

df_blank1 <- df_blank1 %>%
  select(c("Area", "File", "m.z", "RT")) %>%
  mutate(type = "Blanks")

combined_df1 <- rbind(df1, df_blank1) %>% 
  arrange(RT)

# HPLCTOFMS
df2 <- bind_rows(df_list2_step1.1) %>%
  select(c("m/z", "RT", "Height", "File")) %>%
  dplyr::mutate(File = gsub("_", "-", File)) %>%
  mutate(type = "Sample")

df_blank2 <- df_blank2 %>%
  select(c("m/z", "RT", "Height", "File")) %>%
  mutate(type = "Blanks")

combined_df2 <- rbind(df2, df_blank2) %>% 
  arrange(RT)

colnames(combined_df2)[[1]] <- "m.z"
```

### Quality assurance for STEP 1.3B: Examine variation in RT of 4 benchmark compounds in HPLC across all samples with histogram distribution of RT of 4 benchmark compounds 

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS data
# Use excel sheets of Toluene-d8 benchmark to create scatterplot 

# Histogram plot of the distribution of RT versus. no of peak in each sample
hist_ls <- list()
i <- 1
sample <- combined_df1 %>% filter(type %in% "Sample")
for (file in unique(sample$File)) {
  hist_ls[[i]] <- ggplot(sample[which(sample$File == file),]) + 
    geom_histogram(aes(x = RT))
  i <- i + 1
}

grid.arrange(grobs = hist_ls, ncol = 7)

# HPLC data
# Use excel sheets of benchmark day "0, 2, 15, 34" from Final Leachates Benchmarks and Recovery.xlsx to create scatterplot dataframe
bm_sheets <- readxl::excel_sheets(path = paste0(getwd(),"/data/Final Leachates Benchmarks and Recovery.xlsx"))
bm_df_list <- list()
for (i in 1:length(bm_sheets)) {
  x <- readxl::read_xlsx(paste0(getwd(),"/data/Final Leachates Benchmarks and Recovery.xlsx"), sheet = bm_sheets[i]) %>% 
    select(c("Acetaminophen-d4 (m/z 156.0957)", "dTEP (m/z 198.1723)", "dTPrP (m/z 246.2568)",
             "13C-6PPD (m/z 305.1956)", "dTPP (m/z 342.1722)"))
  bm_df_list[[i]] <- x[-1,] %>% pivot_longer(cols = 1:ncol(x), names_to = "Benchmark", values_to = "RT")
}

bm_df <- bind_rows(bm_df_list)

# Plot scatter of RT distribution
par(mfrow = c(3, 2))

for (i in 1:length(unique(bm_df$Benchmark))) {
  as.matrix(round(summary(as.numeric(bm_df[which(bm_df$Benchmark == unique(bm_df$Benchmark)[i]),]$RT)), 4))
  round(stats::sd(as.numeric(bm_df[which(bm_df$Benchmark == unique(bm_df$Benchmark)[i]),]$RT)), 4)
  plot(x = bm_df[which(bm_df$Benchmark == unique(bm_df$Benchmark)[i]),]$RT,
       y =1:length(bm_df[which(bm_df$Benchmark == unique(bm_df$Benchmark)[i]),]$RT),
       xlab = "RT", ylab ="", cex = 1, cex.axis = 1.5, cex.lab = 1.5, cex.main = 1.5,
       type = "p", main = unique(bm_df$Benchmark)[i])
  hist(as.numeric(bm_df[which(bm_df$Benchmark == unique(bm_df$Benchmark)[i]),]$RT), 
       xlab = "RT", ylab ="", cex.axis = 2, cex.lab = 2, cex.main = 2,
       main = unique(bm_df$Benchmark)[i])
}

# Histogram plot of the distribution of RT versus. no of peak in each sample
hist_ls <- list()
i <- 1
sample <- combined_df2 %>% filter(type %in% "Sample")
for (file in unique(sample$File)) {
  hist_ls[[i]] <- ggplot(sample[which(sample$File == file),]) + 
    geom_histogram(aes(x = RT))
  i <- i + 1
}

grid.arrange(grobs = hist_ls, ncol = 7)
```

### Quality assurance for STEP 1.3B compound alignment - HPLC data

(Update 09 Feb 2024:) Meeting with Roxana => Apply different compound alignment to different part of the chromatograms -> split compounds alignment to RT <= 6 Min and RT > 6 min.

There are nine combinations of RT windows that were tested:
                RT <= 6 min           RT > 6 min
1                0.1                   0.02
2                0.1                   0.03
3                0.1                   0.04
4                0.15                  0.02
5                0.15                  0.03
6                0.15                  0.04
7                0.2                   0.02
8                0.2                   0.03
9                0.2                   0.04

(Update 13 & 14 Feb 2024:) I tested out a bunch of RT for Rt > 6 min but it doesn't seem to bring down the

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Testing different combinations of RT window for 2 parts of the chromatogram ----------
combi <- tidyr::crossing(
  # c(0.1,0.15,0.2, 0.25, 0.3, 0.35, 
                           0.4, c(
                             # 0.02,0.03,0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2,
                             0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4))
rt_test_list <- list()
for (row in 1:nrow(combi)) {
  df <- grouping_comp(combined_df2,
                      rtthres1 = as.numeric(combi[row,][1]),
                      rtthres2 = as.numeric(combi[row,][2]),
                      mzthres = 0.0005,
                      type = "HPLCTOFMS")
  
  rt_test_list[[paste0(as.numeric(combi[row,][1]), "_", as.numeric(combi[row,][2]))]] <- df
  
  rt_6 <- df %>% filter(RT <= 6)
  rt_higher6 <- df %>% filter(RT > 6)
  
  print(paste0("RT window ", as.numeric(combi[row,][1]), " No. unique compounds that belongs to Benchmark_1 Acetaminophen-d4 : ", 
               length(unique((rt_6 %>% 
                      filter(m.z >= (156.0957 - 0.0001) &
                               m.z <= (156.0957 + 0.0001)))$Feature))))
  
  print(paste0("RT window ", as.numeric(combi[row,][1]), " No. of unique compounds : ", length(unique(rt_6$Feature))))
  
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_2 dTEP : ", 
               length(unique((rt_higher6 %>% 
                                filter(m.z >= (198.1723 - 0.0001) &
                                         m.z <= (198.1723 + 0.0001)))$Feature))))
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_3 dTPrP : ", 
               length(unique((rt_higher6 %>% 
                      filter(m.z >= (246.2568 - 0.0001) &
                               m.z <= (246.2568 + 0.0001)))$Feature))))
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_4 13C-6PPD : ", 
               length(unique((rt_higher6 %>% 
                      filter(m.z >= (305.1956 - 0.0001) &
                               m.z <= (305.1956 + 0.0001)))$Feature))))
  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. unique compounds that belongs to Benchmark_5 dTPP : ", 
               length(unique((rt_higher6 %>%
                      filter(m.z >= (342.1722 - 0.0001) &
                               m.z <= (342.1722 + 0.0001)))$Feature))))

  print(paste0("RT window ", as.numeric(combi[row,][2]), " No. of unique compounds : ", length(unique(rt_higher6$Feature))))
}
```

### STEP 1.3B: Collapsing compounds based on RT1, RT2, Ion1 threshold --------------------------

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS
combined_df1_grouped <- grouping_comp(combined_df1,
                                      rtthres1 = 0.05,
                                      mzthres = 0.05,
                                      type = "ATDGCMS")

# HPLCTOFMS
# Apply different alignment to 2 parts of chromatogram via rtthres1 (RT <= 6 min) and rtthres2 (RT > 6 min)
combined_df2_grouped <- grouping_comp(combined_df2,
                                      rtthres1 = 0.4,
                                      rtthres2 = 0.28,
                                      mzthres = 0.0005,
                                      type = "HPLCTOFMS")
```

### Step 1.3C: Remove all the benchmark compounds for HPLC (4) and GC (1 - Toluene)
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
benchmark_mz <- c(156.0957, 198.1723, 246.2568, 305.1956, 342.1722)
benchmark_idx <- c()
# Search for compound in combined_df2_grouped that fall in the window of benchmark' molecular ions
for (mz in benchmark_mz) {
  idx <- which(combined_df2_grouped$m.z <= (mz + 0.0001) & combined_df2_grouped$m.z >= (mz - 0.0001))
  # collect all these indices of benchmark compounds
  benchmark_idx <- c(benchmark_idx, idx)
}

combined_df2_grouped <- combined_df2_grouped[-benchmark_idx,]

### Quality control checkpoint:
# Approximate the number of non-unique compounds in HPLC RT <= 6 min
ggplot(combined_df2_grouped %>% filter(RT <= 6), aes(x=Feature)) +
  geom_bar()

# Approximate the number of non-unique compounds in HPLC RT > 6 min
ggplot(combined_df2_grouped %>% filter(RT > 6), aes(x=Feature)) +
  geom_bar()
```

### Step 2: Readjust compound peak area (sample) by substracting it by average blank peak area ======================
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATDGCMS

# Take the blank away from the sample that run right before it
# If the compound appear in blank and not in sample -> negative values after blank subtraction -> remove those compounds

# Create list to store temp dfs
temp_list <- list()
i <- 1
# Iterate through each Feature
for (comp in unique(combined_df1_grouped$Feature)) {
  temp <- combined_df1_grouped[which(combined_df1_grouped$Feature == comp),]
  # if compound does not exist in blanks then skip the compounds
  if (identical(which(temp$type == "Blanks"), integer(0))) {
    temp_list[[i]] <- temp
    i <- i + 1
    next
  }
  else {
    # Calculate avg_blank for that compound across all blanks
    avg_blank <- mean(temp[which(temp$type == "Blanks"),]$Area)
    # Then take out all the compound belong to blank from the df
    temp <- temp[which(temp$type != "Blanks"),]
    # iterate through each sample
    for (sample in unique(temp$File)) {
      # Adjust RA for each compound of each sample = RA (sample) - avg_blank
      temp[which(temp$File == sample),]$Area <- temp[which(temp$File == sample),]$Area - avg_blank
    } 
  }
  # Append current temp df to temp_list
  temp_list[[i]] <- temp
  i <- i + 1
}

adjusted_df1 <- bind_rows(temp_list)

# HPLCTOFMS

# Create list to store temp dfs
temp_list <- list()
i <- 1
# Iterate through each Feature
for (comp in unique(combined_df2_grouped$Feature)) {
  temp <- combined_df2_grouped[which(combined_df2_grouped$Feature == comp),]
  # if compound does not have blanks then append the compound to the list and move on to the next compounds in the loop
  if (identical(which(temp$type == "Blanks"), integer(0))) {
    temp_list[[i]] <- temp
    i <- i + 1
    next
  }
  else {
    # Calculate avg_blank for that compound across all blanks
    avg_blank <- mean(temp[which(temp$type == "Blanks"),]$Height)
    temp <- temp[which(temp$type != "Blanks"),]
    # iterate through each sample
    for (sample in unique(temp$File)) {
      # Adjust RA for each compound of each sample = RA (sample) - avg_blank
      temp[which(temp$File == sample),]$Height <- temp[which(temp$File == sample),]$Height - avg_blank
    } 
  }
  # Append current temp df to temp_list
  temp_list[[i]] <- temp
  i <- i + 1
}

adjusted_df2 <- bind_rows(temp_list)
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# STEP 3: Remove all detections with negative peak area and Normalizing data accordingly to different data frames of interest on with the one with positive Area values -----------------------------------

# ATDGCMS

# paste0("the number of compounds with negative Area values are: ", length(adjusted_df1$Area[adjusted_df1$Area < 0]))

temp_list <- list()
# neg_list <- list()
i <- 1
# Normalize Peak Area for each sample 
for (sample in unique(adjusted_df1$File)) {
  df <- adjusted_df1[which(adjusted_df1$File == sample),] %>%
    filter(Area > 0) %>%
    mutate(Percent_Area = Area/sum(.$Area))
  temp_list[[i]] <- df
  
  i <- i + 1
}

# For positive values => combine data again to 1 grand data frame
comp_normalized1 <- dplyr::bind_rows(temp_list)

# transform_filename_atdgcms <- function(filename) {
#   split_name <- base::strsplit(filename, "-")[[1]]
# 
#   # Remove 2nd and 3rd words and modify the 4th word (now 3rd after removal)
#   split_name <- c(split_name[-c(2, 3, 6)], paste0("rep", split_name[6]))
# 
#   # Combine steps: Glue string together and add "ATDGCMS-" prefix
#   return(paste0("ATDGCMS-", paste(split_name, collapse = "-")))
# }
# 
# 
# comp_normalized1 <- comp_normalized1 %>%
#   group_by(File) %>%
#   mutate(NewFile = transform_filename_atdgcms(File))
# 
# # Change name of Fishing Item to correct name
# comp_normalized1[which(str_detect("ATDGCMS-Fishing-1-USE-1-rep15", comp_normalized1$NewFile)),]$NewFile <- "ATDGCMS-Fishing-Items-USE-15-rep1"

# HPLCTOFMS

# paste0("the number of compounds with negative Area values are: ", length(adjusted_df2$Height[adjusted_df2$Height < 0]))

temp_list <- list()
# neg_list <- list()
i <- 1
# Normalize Peak Area for each sample 
for (sample in unique(adjusted_df2$File)) {
  df <- adjusted_df2[which(adjusted_df2$File == sample),] %>%
    dplyr::filter(Height > 0) %>%
    mutate(Percent_Height = Height/sum(.$Height))
  temp_list[[i]] <- df
  
  i <- i + 1
}

# Then combine data again to 1 grand data frame
comp_normalized2 <- dplyr::bind_rows(temp_list)

# transform_filename_hplctofms <- function(filename) {
#   # remove the last 3 elements of the splitted name
#   split_name <- head(base::strsplit(filename, "-")[[1]], -3)
# 
#   # glue the remaining elements together
#   match_phrase <- paste(split_name[c(1,2)], collapse = "-")
# 
#   # find the corresponding atdgcms file name that have the match first 6 letters -> add prefix HPLCTOFMS to filename -> Glue string together
#   return(paste0("HPLCTOFMS-", paste(c(base::strsplit(unique(comp_normalized1$File)[str_detect(unique(comp_normalized1$File), match_phrase)], "-")[[1]][[1]], split_name), collapse = "-")))
# }
# 
# comp_normalized2 <- comp_normalized2 %>%
#   group_by(File) %>%
#   mutate(NewFile = transform_filename_hplctofms(File))
```

### Quality assurance: How many compounds after blank subtraction between different RT windows?
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATD-GC-MS
length(unique(comp_normalized1)$Feature)

# HPLC-TOFMS
length(unique(comp_normalized2)$Feature)
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# STEP 4: Identify shared and unique compound groups across samples ------------------------------------------------
p <- "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/Table of product categorization_NewFileName.xlsx"

gc_list <- list()
hplc_list <- list()
gc_hplc_list <- list()

for (i in 1:length(excel_sheets(path = p))) {
  # Table of categorization
  sampinfo <- readxl::read_excel(path = p, 
                                 sheet = excel_sheets(path = p)[i])
  
    # at least in 2 samples
  # ATDGCMS
  # joining data frame with sampinfo df
  dat1 <- left_join(x = comp_normalized1, #%>% select(.,-c(File)),
                    y = sampinfo,  # %>% select(.,-c(File)), 
                    by = 'File')
  
  idx_list_filter1 <- comp_filter(dat1) 
  
  shared_comp1 <- dat1[idx_list_filter1[[1]],]
  
  
  # HPLCTOFMS
  # joining data frame with sampinfo df
  dat2 <- left_join(x = comp_normalized2, # %>% select(., -c(File)), 
                    y = sampinfo, # %>% select(., -c(File)),  
                    by = 'File')
  
  idx_list_filter2 <- comp_filter(dat2)
  
  shared_comp2 <- dat2[idx_list_filter2[[1]],]
  
  # Step 5: Merging Sample info with shared df ===========================================
  # ATDGCMS
  
  atdgcms <- shared_comp1 %>%
    filter(., !is.na(Feature)) %>%
    select('File', 'Suspected_Polymer', 'Category', 'Subcategory', 'Feature', 'm.z', 'RT', 'Percent_Area')
  
  colnames(atdgcms)[ncol(atdgcms)] <- 'Values'
  
  gc_list[[i]] <- atdgcms
  
  # HPLCTOFMS

  hplctofms <- shared_comp2 %>%
    filter(., !is.na(Feature)) %>%
    select('File', 'Suspected_Polymer', 'Category', 'Subcategory', 'Feature', 'm.z', 'RT', 'Percent_Height')
  
  colnames(hplctofms)[ncol(hplctofms)] <- 'Values'
  
  hplc_list[[i]] <- hplctofms
  
  ### COMBINATION OF HPLC AND ATDGC datasets
  gc_hplc <- rbind(atdgcms, hplctofms)
  
  gc_hplc_list[[i]] <- gc_hplc
}
```

### Quality assurance: How many compounds after filtering out 'unique-compounds-to-sample' between different RT windows?

### Export GC/HPLC/Combined df to excel as input for Python ML pipeline

```{r , echo=FALSE, warning = FALSE}
writexl::write_xlsx(x = gc_list, path = "gc_list_df.xlsx")
writexl::write_xlsx(x = hplc_list, path = "hplc_list_df.xlsx")
writexl::write_xlsx(x = gc_hplc_list, path = "gc_hplc_list_df.xlsx")
```

### Trace metal data

#### Data prepping

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Trace metal data

metal <- readxl::read_excel("C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/Trace metal/Trace metal data.xlsx")

p <- "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting/data/Table of product categorization_NewFileName.xlsx"

# Data Normalization
for (i in 1:nrow(metal)) {
  # Calculate the sum of values from column 2 to ncol
  sum_values <- sum(metal[i, 2:ncol(metal)])
  
  # Update each cell in column 2 to ncol by dividing by the sum
  metal[i, 2:ncol(metal)] <- metal[i, 2:ncol(metal)] / sum_values
}

# Merge with Sample Categorization information
icp_list <- list()

# Merge Categorization schemes with Trace metal data
for (i in 1:length(excel_sheets(path = p))) {
  # Table of categorization
  sampinfo <- readxl::read_excel(path = p, 
                                 sheet = excel_sheets(path = p)[i])
  
  # joining data frame with sampinfo df
  dat <- left_join(x = metal, 
                    y = sampinfo %>% select(., -NewFile), 
                    by = 'File')
  
  icp_list[[i]] <- dat %>% 
    select(., -c(Type)) %>%
    relocate(File, Suspected_Polymer, Category, Subcategory, .before = 1) %>%
    pivot_longer(., cols = 5:ncol(.), names_to = "Feature", values_to = "Values")

}

```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
writexl::write_xlsx(x = icp_list, path = "icp_list_df.xlsx")
```

### Combine Trace metal data with GC & HPLC data
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
gc_icp_list <- list()
hplc_icp_list <- list()
gc_hplc_icp_list <- list()
for (i in 1:4) {
  gc_icp_list[[i]] <- rbind(gc_list[[i]] %>% select(., -c(m.z, RT)), icp_list[[i]])
  hplc_icp_list[[i]] <- rbind(hplc_list[[i]] %>% select(., -c(m.z, RT)), icp_list[[i]])
  gc_hplc_icp_list[[i]] <- rbind(gc_hplc_list[[i]] %>% select(., -c(m.z, RT)), icp_list[[i]])
}
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
writexl::write_xlsx(x = gc_icp_list, path = "gc_icp_list_df.xlsx")
writexl::write_xlsx(x = hplc_icp_list, path = "hplc_icp_list_df.xlsx")
writexl::write_xlsx(x = gc_hplc_icp_list, path = "gc_hplc_icp_list_df.xlsx")
```

#### 1. What are the Top metal with highest Concen. in each sample?

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
newmetal <- icp_list[[1]] %>% 
  tidyr::pivot_wider(names_from = Feature, values_from = Values) %>% 
  column_to_rownames(., var = "File")

# Replace all NA in data frame with 0
newmetal[is.na(newmetal)] <- 0

# For each sample, Extract Top 10 metal with highest concen.
top_metal <- list()
for (row in 1:nrow(newmetal)) {
  top_metal[[row]] <- colnames(newmetal)[order(as.numeric(newmetal[row,]), decreasing = TRUE)[1:10]]
}

topmetal_list <- list()
i <- 1
for (file in unique(icp_list[[1]]$File)) {
  topmetal_list[[i]] <- icp_list[[1]] %>% filter(., File %in% file) %>% filter(., Feature %in% top_metal[[i]])
  i <- i + 1
}

top_metal_plot_df <- bind_rows(topmetal_list) %>%
   mutate(Category = ifelse(str_detect(File, "USE-01"), "Polystyrene food packaging", 
                                  ifelse(str_detect(File, "USE-02"), "Mixed plastic waste",
                                         ifelse(str_detect(File, "USE-03"), "Plastic drinking straws", 
                                                ifelse(str_detect(File, "USE-05"), "Cigarettes tips", 
                                                       ifelse(str_detect(File, "USE-06"), "Face masks", 
                                                              ifelse(str_detect(File, "USE-07"), "Food containers", 
                                                                     ifelse(str_detect(File, "USE-09"), "Food wrappers",
                                                                            ifelse(str_detect(File, "USE-11"), "Plastic toy balls", 
                                                                                   ifelse(str_detect(File, "USE-12"), "Ziploc bags", 
                                                                                          ifelse(str_detect(File, "USE-13"), "Food packaging",
                                                                                                 ifelse(str_detect(File, "USE-14"), "Bottle caps", "Fishing bait trays"))))))))))))
                                                              
### Stacked bar plot of values of  Top 10 trace metal for each metal data sample (x-axis: sample name; y-axis: metal values) --------------------------------------

# data_summary <- top10_plot %>%
#   group_by(NewFile, Metal_component) %>%
#   summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = top_metal_plot_df, aes(x = Category, y = Values, fill = Feature)) +
  geom_bar(stat = "identity") +
  # geom_errorbar(data = data_summary, aes(x = NewFile, ymin = mean - sd, 
  #                                        ymax = mean + sd, group = Metal_component), 
  #               position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "ICP-MS Trace metal data", x = "Plastic product types", y = "Normalized Abundance (%)",
       fill = "Top trace metals") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### 2.1 Among the top trace metal, if the metal is shared among different samples, is there a significant difference in the concentration of these trace metals?

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}

```

#### 2.2 Among the top trace metal, what are the metal that uniquely occur in just 1 sample?

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
```

### ATD-GCMS data:

### HPLC-TOFMS data:

#### For each plastic type, What are compounds that only appear in USSB and not USE?

```{r, echo=FALSE, warning = FALSE}
# compdiff <- list()
for (plt in unique(adjusted_df$plastic_type)) {
  # compdiff[[plt]] <- 
  print(plt)
  print(base::setdiff(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

#### For each plastic product, What are the compounds that consistently appear in both USSB & USE?

```{r, echo=FALSE, warning = FALSE}
for (plt in unique(adjusted_df$plastic_type)) {
  print(plt)
  print(base::intersect(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

#### What are the top compounds that have highest abundance in each plastic "Subcategory" and what are their identity (matched with EF suspect screening)
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Extract top compounds for each sample from HPLC only data ----------------------------------------------------------
hplc <- hplc_list[[1]] %>% 
  dplyr::select(Subcategory, Feature, Values) %>%
  dplyr::group_by(Subcategory, Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from = Feature, 
                     values_from = Values) 

# For each sample, Extract Top x compounds with highest concen.
top_hplc <- list()
for (row in 1:nrow(hplc)) {
  # go through each subcategory and Extract Top x compounds with highest concen.
  top_hplc[[hplc[row,]$Subcategory]] <- colnames(hplc)[order(as.numeric(hplc[row,]), decreasing = TRUE)[1:10]]
}

top_hplc_list <- list()
i <- 1
for (subcat in unique(hplc_list[[1]]$Subcategory)) {
  top_hplc_list[[i]] <- hplc_list[[1]] %>% filter(., Subcategory %in% subcat) %>% filter(., Feature %in% top_hplc[[i]])
  i <- i + 1
}

top_hplc_plot_df <- bind_rows(top_hplc_list) 

# Matching all the top compounds with EF's Suspect Screening ---------------------------------------------------------
all_unique <- data.frame(comp=unique(top_hplc_plot_df$Feature))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecular ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)

# Replace all the compounds in plot_df with suspect screening matches ------------------------------------------------
joined <- left_join(top_hplc_plot_df %>%
                      dplyr::select(Subcategory, Feature, Values), 
                    all_matching_sus_scr %>%
                      dplyr::select(Feature, `Suspect Name`), by = "Feature") %>%
  dplyr::filter(!is.na(`Suspect Name`)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, `Suspect Name`, Subcategory) %>%
  dplyr::summarise(across(Values, base::mean))


### Stacked bar plot of values of  Top 10 trace metal for each metal data sample (x-axis: sample name; y-axis: metal values) --------------------------------------

# data_summary <- top10_plot %>%
#   group_by(NewFile, Metal_component) %>%
#   summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = joined, aes(x = Subcategory, y = Values, fill = `Suspect Name`)) +
  geom_bar(stat = "identity") +
  # geom_errorbar(data = data_summary, aes(x = NewFile, ymin = mean - sd, 
  #                                        ymax = mean + sd, group = Metal_component), 
  #               position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "Non-targeted HPLC-qTOF-MS data", x = "Plastic product types", y = "Normalized Peak Abundance (%)",
       fill = "Top hit from HPLC PMT Suspect Screening") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Beyond-PMT Suspect Screening -------------------------------------------------
# Filter all the top HPLC compounds that "beyond" the PMT suspect screening list
beyond_pmt <- top_hplc_plot_df %>%
  filter(Feature %notin% unique(joined$Feature)) %>%
  select(Feature, RT, m.z) %>%
  group_by(Feature) %>%
  summarise(across(c(RT, m.z), base::mean))

# Export "beyond-PMT-Suspect Screening" list to excel
writexl::write_xlsx(x=beyond_pmt, path = "Top10_HPLC_data_beyond_pmt_suspect Screening.xlsx")
```

## Demo PCA

Since our data is compositional data, combining PCA with the Aitchison dissimilarity index (inherited from HCA for compositional data), however, is not straightforward due to the nature of compositional data. Standard PCA assumes the data does not have the constant sum constraint, which is a fundamental property of compositional data. Applying PCA directly to raw compositional data can lead to misleading results because it does not account for the compositional nature of the data.

To perform PCA on compositional data while considering the Aitchison distance, you would typically follow these steps:

Log-Ratio Transformation: Transform the compositional data using an appropriate log-ratio transformation, such as the Centered Log-Ratio (CLR) transformation, the Additive Log-Ratio (ALR), or the Isometric Log-Ratio (ILR) transformation. These transformations allow the data to be analyzed in real Euclidean space, where standard statistical techniques are valid.

(Update 04 Feb 2024:) Most common theme in comparison PCA biplot of zero versus. LOD imputation is that Zero imputation provides better higher percentage of data variation explained by PC1 and PC2 than LOD imputation. AND also original data is giving out better percentage variation explained than the compositional data transformed data (like with CLR)

(Update 05 Feb 2024:) I tried perform PCA with and without 1st step global filtering of minimum peak signal but the result of clustering is the same between these 2 cases.

(Update 09 Feb 2024:) For both PCA and Wilcoxon tests, in the NA exclusion insert another criteria for cases where feature appear consistently in only one product type.

```{r , echo=FALSE, warning = FALSE, message=FALSE}
df_pca <- gc_hplc_icp_list[[1]] %>%
  dplyr::select(File,
                Subcategory,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Subcategory,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var="File") %>%
  relocate(Subcategory, .after =1)

### Quality assurance: How many compounds appear 90% of the time in only one product type?
# IF conditioning, which feature column appear consistently (populate 90 % of that one product type`), KEEP IT!!!!!
col_id <- c()
for (i in 2:ncol(df_pca)) {
  temp <- df_pca[, c(1,i)]
  ele_count <- c()
  for (subcat in unique(temp$Subcategory)) {
    count <- sum(!is.na(temp[which(temp$Subcategory == subcat), 2]))/length(temp[which(temp$Subcategory == subcat), 2])
    ele_count <- c(ele_count, count)
  }
  if (sum(ele_count > 0) == 1 & any(ele_count > 0.9)) {
    col_id <- c(col_id, i)
  } else {
    next
  }
}

# View(df_pca[, c(1, col_id)])

# Remove columns with more than 90% NAs 
test <- df_pca %>% 
  purrr::discard(~sum(is.na(.x))/length(.x)* 100 >=90)

# Add back the compounds appear 90% of the time in only one product type
test2 <- cbind(test, df_pca[, col_id])

# Missing value imputation
for (r in 1:nrow(test2)) {
  test2[r, which(base::is.na(test2[r,]))] <- 0
  # sort(gc_list[[2]]$Values)[1] # no different in PCA biplot between LOD and zero imputed even with CLR-transformation
    # stats::runif(length(which(base::is.na(df_pca[r,]))),
    #                                                         min = sort(hplc_list[[2]]$Values)[1],
    #                                                         max = sort(hplc_list[[2]]$Values)[2])
}

# clr_transformed_pca <- compositions::clr(test[, -1])

res.pca <- FactoMineR::PCA(
  test2[, -1],
  # clr_transformed_pca,
  scale.unit = FALSE,
  graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            habillage = factor(df_pca$Subcategory),
                            addEllipses = TRUE,
                            ellipse.level=0.95,
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = "Plastic Sample"
                            ) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(40, 'mm')) +
  theme(legend.position = 'none') 
  # + coord_equal()

# df_pca <- input_df(gc_hplc_list[[1]])
# 
# # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
# p <- PCAtools::pca(mat = df_pca[[1]], 
#                    metadata = df_pca[[2]], 
#                    # center = FALSE,
#                    scale = FALSE 
#                    )
# 
# # A bi-plot =================
# PCAtools::biplot(p,
#                  lab = rownames(p$metadata),
#                  colby = "Category",
#                  hline = 0, vline = 0,
#                  legendPosition = 'right', labSize = 5,
#                  sizeLoadingsNames = 5,
#                  showLoadings = TRUE,
#                  showLoadingsNames = FALSE,
#                  ntopLoadings = 10,
#                  pointSize = 4, 
#                  legendLabSize = 15,
#                  legendTitleSize = 16,
#                  legendIconSize = 6)
# 
# # Retrieve PC and add as new variables to data frame 
# PCAtools_mergePC <- p$rotated
```

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# 3D PCa plot ===============================================================================
# FactoMineR_p <- FactoMineR::PCA(df_pca,
#                                 scale.unit = FALSE)
# 
# scores <- as.data.frame(FactoMineR_p$ind$coord[,1:3]) %>%
#   rownames_to_column(., var = "File")
# category <- sapply(scores$File, function(f) {
#   unique(gc_hplc_list[[1]]$Category[match(f, gc_hplc_list[[1]]$File)])
# })
# 
# # Add to scores data frame
# scores$Category <- as.factor(category)
# 
# rgl::plot3d(scores$Dim.1, scores$Dim.2, scores$Dim.3,
#        col = as.integer(scores$Category),
#        xlab = "PC1",
#        ylab = "PC2",
#        zlab = "PC3",
#        type = "p")
# 
# # Add legend
# rgl::legend3d("topright", legend = levels(scores$Category), col = 1:length(levels(scores$Category)), pch = 16)

# factoextra::fviz_screeplot(FactoMineR_p)
# factoextra::fviz_pca_biplot(FactoMineR_p,
#                             habillage = unique(gc_hplc_list[[1]]$File))
```

### "Trace metal"

#### 1. Can we use trace metal to distinguish between individual samples?

```{r , echo=FALSE, warning = FALSE, fig.width=14, fig.height=9}
df_pca <- metal_list[[1]] %>%
  dplyr::select(NewFile, 
                Metal_component, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    Metal_component) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Metal_component,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

# Replace all missing values with 0
df_pca[is.na(df_pca)] <- 0

res.pca <- FactoMineR::PCA(df_pca, 
                           scale.unit = FALSE,
                           graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "ind", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = "Trace metal samples grouping by samples ")
```

#### 2. Can we use trace metal to distinguish between Polymer type/Category/Subcategory?

We should not use trace metal data values to try grouping Polymer types in PCA because of our main goal was to classify plastic products 

The PCA biplot result would imply that say, for example, The samples that have this kind of polymer types (for ex, Cellulose acetate//Polystyrene // Polyethelene// etc.) will have high values of this and this and this trace metals (for ex, lithium or Fe or Barium or etc. ) as compared to the other polymer types.

```{r , echo=FALSE, warning = FALSE, fig.width=14, fig.height=9}
df_pca <- metal_list[[1]] %>%
  dplyr::select(NewFile, 
                Metal_component,
                Subcategory,
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    Subcategory,
    Metal_component) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Metal_component,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "Subcategory")

# Replace all missing values with 0
df_pca[is.na(df_pca)] <- 0

res.pca <- FactoMineR::PCA(df_pca, 
                           scale.unit = FALSE,
                           graph = FALSE)

factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "ind", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 6, 
                            ggtheme = ggplot2::theme_minimal(base_size = 15),
                            title = "Trace metal samples grouping by Subcategory") 
```

## Demo Hierarchical Clustering Analysis

If we filter unique compounds by File (compounds appear in at least 2 files), then all File in df in gc_hplc_list are the same. That's why here we used gc_hplc_list[[1]]

This exercise does not seems to affect the HCa. Keep in mind that for all 3 cases of data combinations: none of them has compounds that occur in all samples. NONE of THEM!!!!!

### Clustering of samples

(Jan 11th 2024): 
\ICP only data shows misgrouping between the following Subcategories: 
- Cigarillo tip//Bottle caps
- Food wrapper// Food packaging// Food containers//plastic drinking straws//Various plastic waste

\GC+ICP data show clear separation between GC and ICP data into 2 clusters. From ICP cluster, the misgrouping between the following Subcategories is the same as the case of ICP only. From the GC cluster, the grouping is more or less the same as the case of GC only.

\HPLC+ICP data, the cluster of both ICP and HPLC side change a bit. However for ICP side, misgrouping between the following Subcategories still remains as the case of ICP only. For HPLC side, the organization of the sample into grouping change a bit as the plastic toy balls are separated into their own top cluster (same height level with ICP and rest of HPLC branches) 

\GC+HPLC+ICP data, for ICP side, misgrouping between the following Subcategories still remains as the case of ICP only. For GC + HPLC side, it looks almost exactly like the case of GC+HPLC.

(UPdate 06 Feb 2024:) common metrics to evaluate performance of clustering models:
- Silhouette Score (https://stackoverflow.com/questions/33999224/silhouette-plot-in-r): Higher score => better clustering
- Calinski-Harabaz Index (https://search.r-project.org/CRAN/refmans/fpc/html/calinhara.html): Higher score => better clustering
- Davies-Bouldin Index ():



```{r echo=FALSE, warning=FALSE, message=FALSE}
# Data prep for HCA ---------------------
hc_df <- icp_list[[1]] %>%
  # filter(., Feature %in% comp_vec) %>%
  dplyr::select(File,
                Subcategory, # !! Subcategory to easier visualize the samples characteristic in HCA plot
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Subcategory,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values)

names <- hc_df$Subcategory

hc_df <- as.matrix(hc_df[, -c(1,2)])

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
}

rownames(hc_df) <- names

## Dissimilarity Indices Calculated by vegan::vegdist()
hca_samp <- stats::hclust(vegan::vegdist(hc_df,
                                         method = "robust.aitchison")) # Since our data is continous -> canberra // manhattan // aitchison // robust.aitchison

plot(hca_samp,  
     # labels = FALSE, 
     hang = -1,
     main = "")
```


A smaller Aitchison distance between two samples would indicate that their compositional structures are similar in terms of the ratios between components.

For selecting Dissimilarity Indices Calculated by vegan::vegdist(), since our data is continous, not normally distributed, and the data is sparse (meaning our data has lots of NA or NULL before data imputation, This basically because there are compounds that occur in some samples and not in the others). Manhattan dissimilarity index separating ATDGCMS and HPLCTOFMS into 2 big clusters, while Canberra//Aitchison//Robust.aitchison grouped ATDGCMS as a subgroup of a part of HPLCTOFMS data.

Since for each observations of each sample, the sum of values of collapsed compounds is sum up to 100%, Aitchison or Robust.Aitchison are currently the best option for dissimilarity index.

***Why Aitchison or Robust.Aitchison for OUR DATA, AKA. Compositional Data?***

Compositional data have particular characteristics and constraints that standard distance metrics like Euclidean don't handle well. Here are some reasons why Aitchison distance is more suitable:

**Closure Constraint:** Because the components in compositional data sum to a constant value (like 1 or 100%), the data is subject to a closure constraint. This constraint violates the assumption of independence between variables that many standard statistical techniques rely on. Aitchison distance accounts for this constraint.

*Implications of Closure Constraint* *Loss of Degrees of Freedom:* Because of the closure constraint, one variable is effectively 'dependent' on the others. For instance, in our example, knowing the proportions of Additive A and Additive B is enough to determine the proportion of Additive C.

*Spurious Correlations:* The closure constraint can lead to spurious or misleading correlations between variables. For example, an increase in the proportion of Additive A will necessarily result in a decrease in the sum of the proportions of Additives B and C, even if there's no biological or chemical reason for them to be inversely related.

*Non-Independence of Components:* In compositional data, the components are not independent of each other, violating the assumptions of independence that many statistical tests and measures (like the Euclidean distance) rely on.

*Comparisons are Relative:* In compositional data, what matters is the relative sizes of the components, not their absolute sizes. Any analysis should, therefore, focus on these relative proportions rather than absolute values.

For these reasons, specialized statistical methods like Aitchison distance are often used for compositional data. These methods take the closure constraint into account and focus on the relative proportions of the different components, providing a more accurate and interpretable analysis.

In this workflow on tracking microplastic additives in aquatic environments, understanding the closure constraint can be pivotal. If you're looking at the relative abundances of different types of plastic additives in water samples, those abundances are compositional data subject to the closure constraint. Using standard statistical techniques without accounting for this constraint could lead to misleading conclusions.

**Relative Importance:** In compositional data, what's generally important is the relative proportion between components, rather than their absolute values. The Aitchison distance is designed to capture the relationships between these relative values effectively.

**Log-Ratio Transformation:** The Aitchison distance is based on log-ratio transformations, which have been proven to be an effective way to analyze compositional data. The log-ratio removes the closure constraint, making the data easier to analyze statistically.

**Handling Zeros:** The robust variant of Aitchison distance often includes techniques for handling zeros in compositional data, which are common and challenging to deal with.

**Scale-Invariance:** Aitchison distance is scale-invariant, meaning that multiplying a component by a constant will not change the distance. This is crucial for compositional data, where only the relative relationships between components matter.

**Interpretable:** The distance measure reflects compositional difference in a way that's meaningful within the context of the data, making it easier to understand and interpret the results.

## Demo 2-way HCA dendrogram

```{r , echo=FALSE, warning = FALSE, message=FALSE}
library(pheatmap)
# Input matrix - sample as rows, features as columns
pheatmap(as.matrix(hc_df),
         cluster_rows = hclust(as.dist(vegan::vegdist(as.matrix(hc_df[55:75,50:100]), 
                                              method = "robust.aitchison"))),
         cluster_cols = hclust(dist(t(as.matrix(hc_df[55:75,50:100])))),
         clustering_method = "complete",
         color = colorRampPalette(c("navy", "white", "firebrick3"))(50),
         border_color = "grey20",
         cellwidth = 5,
         cellheight = 5,
         fontsize = 7,
         show_rownames = TRUE,
         show_colnames = TRUE)

```

(12th,Oct,2023) Current Problem with two-way HCA dendrogram is that we have too many samples (n = 172) and too many compounds (n = 13046). =\> Proposed solution: grouping the compounds together via the function cutree(hca_comp, h = 100) and combine all compounds that belongs to a group together by taking the mean/median across all compounds that belongs to group for each sample. The process is as demonstrated below.

```{r , echo=FALSE, warning = FALSE, message=FALSE}
cut_tree_comp <- data.frame(group = cutree(hca_comp, h = 130))
cut_tree_comp <- rownames_to_column(cut_tree_comp, var = "collapsed_compound")
joined <- full_join(cut_tree_comp, 
                    gc_hplc_list[[1]] %>%
                      select(File, collapsed_compound, Values) %>%
                      group_by(File, collapsed_compound) %>%
                      summarise(mean_value = mean(Values)) %>%
                      pivot_wider(names_from = File, values_from = mean_value), 
                    by = "collapsed_compound") %>% column_to_rownames(., var = "collapsed_compound")

# Iterating through each sample and combine values of compounds (FUN = mean) that belongs to the same grouping
tets <- joined %>%
  group_by(group) %>%
  summarise(across(everything(), \(x) mean(x, na.rm = TRUE))) %>% 
  column_to_rownames(., var = "group")  

for (r in 1:nrow(tets)) {
  tets[r, which(base::is.na(tets[r,]))] <- stats::runif(length(which(base::is.na(tets[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/10000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/10000)
}

hca_samp <- hclust(vegan::vegdist(t(as.matrix(tets)),
                           method = "robust.aitchison"))
plot(hca_samp, hang = -1)

```

So with the dimensional reduction of compounds from n = 13046 to 33, we still achieve a somewhat good clustering result in which sample of the same category tends to cluster together.

#### Here, we use a non-parametric test, Wilcoxon test, because our data violates the assumptions for other parametric test, such as normality and equality of variance between 2 samples, as demonstrate below:

```{r , echo=FALSE, warning = FALSE}
### Examining for normal distribution ----------------
## Histogram 
# Create empty list 
histlist <- list()
# For loop creating histograms of each plastic type
i <- 1
for (plastic in unique(transpose_df$plastic_type)) {
  grouped_plastic <- which(transpose_df$plastic_type == plastic)
  histlist[[i]] <- ggplot() +
    geom_histogram(mapping = aes(as.vector(t(transpose_df[grouped_plastic, 3:ncol(transpose_df)])))) + 
    labs(x = plastic, title = plastic)
  i <- i + 1
}
# Appending each histogram to the empty list in order to view all of them
gridExtra::grid.arrange(grobs = histlist)

## QQ Plot 
# Create empty list 
qqlist <- list()
# For loop creating QQ plots of each plastic type
i <- 1 
# Appending QQ plots together 
par(mfrow=c(3,3))
for (product in unique(transpose_df$product_cat)) {
  grouped_product <- which(transpose_df$product_cat == product)
  qqlist[[i]] <- qqnorm(as.vector(t(transpose_df[grouped_product, 3:ncol(transpose_df)])), main = product, xlab = product, col = 'steelblue')
  qqline(as.vector(t(transpose_df[grouped_product, 3:ncol(transpose_df)])), col = 'red')
  i <- i + 1
}

### Conclusion = they are not normally distributed 

### Examining for equality of variance -----------------------------
# Levenes test-non-normally distributed data (car::leveneTest()), significant if p-value < 0.05 
library(car)

results <- list()
for (col in 1:ncol(utils::combn(unique(transpose_df$product_cat), 2))) {
  # extract the combinations of plastic type pairs
  p_1 <- utils::combn(unique(transpose_df$product_cat), 2)[,col][1]
  p_2 <- utils::combn(unique(transpose_df$product_cat), 2)[,col][2]
  idx1 <- which(transpose_df$product_cat == p_1)
  idx2 <- which(transpose_df$product_cat == p_2)
  V1 <- as.vector(t(transpose_df[idx1, 3:ncol(transpose_df)]))
  V2 <- as.vector(t(transpose_df[idx2, 3:ncol(transpose_df)]))
  datat <- c(V1, V2)
  grouped <- as.factor(c(rep(p_1, times = length(V1)), rep(p_2, times = length(V2))))
  non_norm <- data.frame(datat, grouped)
  results[[paste0(p_1, p_2)]] <- car::leveneTest(datat ~ grouped, data = non_norm)
}
# -> p-value is 0.4096 which is greater the significance level of 0.05 therefore can conclude there is no significant difference between the variances


# Fligner-Killeen's test (fligner.test()), significant if p-value < 0.05 
fligner.test(datat ~ grouped, data = non_norm)
# p-value less than significance level therefore there is significant difference between variances.
# Test to determine the homogeneity of group variances. 
```

When we use combine gc_hplc data, 96.9556% of the observation in the resulting stats data frame are NA values. It would be bad to fill in this large amount of observations with LOD values, aka. leading to the inflation of small values in the data set.

one way to overcome this problem is group_by(Suspected_Polymer) or Category or Subcategory instead of by File =\> this is stupid because then for each comp in each product_cat only have 1 values ==\> it is meaningless to do Wilcoxon test comparing just 1 obs against 1 obs. ==\> we can go through each columns (compounds) and check which product_cat has more than x values and then only do the wilcoxon/ks test with those product_cat for that specific compound. IN this case, we don't need to fill our the missing values because the default for na.action of wilcox.test function is \*\*"na.omit", meaning all the NA values of the numerical vectors resulted from slicing into each column (compound) and each product_cat

## Demo K-means Clustering

### With Combined GC HPLC

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# Input numeric matrix data : sample as rows, collapsed_compounds as columns
df <- gc_hplc_list[[1]] %>%
  dplyr::select(NewFile,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- stats::runif(length(which(base::is.na(df[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

tot.withinss <- list()
for (n_clus in 1:nrow(df)) {
  set.seed(round(runif(2:20, 0, 999)))
  km.out <- stats::kmeans(df, n_clus, nstart = 50)
  tot.withinss[[n_clus]] <- km.out$tot.withinss
  print(paste0("The number of centers is ", n_clus, " and total within-cluster sum of square is ", km.out$tot.withinss))
}

plot(df, col = (stats::kmeans(df, 171, nstart = 50)$cluster + 1) ,
     main = "K- Means Clustering Results with K = 2",
     xlab = "", ylab = "", pch = 20, cex = 2)


```

## Normality and Homogeneity of Variance

Before running multiple Wilcoxon tests, it is recommended to examine whether criteria for univariate parametric test, such as t-test, are violated. If yes, then it is safe to proceed using non-parametric univariate test, such as Wilcoxon test.

First, equal variance and normally distributed between plastic product types were examined using histogram, Q-Q plots. Here, all plastic product types are NOT normally distributed.

```{r, echo = TRUE, message = FALSE, warning = FALSE}
GasData <- as.vector(t(cat_5[,c(1:21)])) 
DieselData <- as.vector(t(cat_5[,c(22:25)]))

# Histogram
hist(GasData, col='steelblue', main='Gas')
hist(DieselData, col='steelblue', main='Diesel')

# Q-Q plots aka. Normal Probability plots
stats::qqnorm(GasData, main='Gas')
stats::qqline(GasData)

stats::qqnorm(DieselData, main='Diesel')
stats::qqline(DieselData)
```

Then, equality of variance between Gas and Diesel populations can be examined using Levene's test and Fligner-Killeen test for non-normally distributed data. Here, for both tests, p values are \< 0.05, and thus, there is significant difference in variances between Gas and Diesel populations.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Levenes test-non-normally distributed data
library(car)
data <- c(GasData, DieselData)
group <- as.factor(c(rep("Gas", times = length(GasData)), rep("Diesel", times = length(DieselData))))
non_norm_dist_data <- data.frame(data, group)
car::leveneTest(data ~ group, data = non_norm_dist_data)

# Fligner-Killeen test
stats::fligner.test(data ~ group, data = non_norm_dist_data)
```
Here, multiple Wilcoxon tests followed by p-value correction for multiple testing was done. Different method for p-value correction from function *p.adjust* from package **stats** were used. After p-value correction, the threshold for p-value can be set (for example,p.adjust \< 0.05 or \< 0.1), to see how it affects the number of significant compounds that can be found. Importantly, the threshold should be set so that it can include as many ASTM reference compounds in the list of significant compounds as possible. For example here, no significant compounds can be found with adjusted p-value \< 0.05. But when adjusted p-value is \< 0.1, 127 significant compounds are found.

## Demo Wilcoxon test

### Wilcoxon test with HCA result

#### With Combined GC HPLC - TBA (ZeroImputed_Jan 15 2024) 

Since for HCA, we operate on sample level and for each 4 subdf of each gc_hplc/gc/hplc_list, they all have the same sample column. Therefore, just using one of the subdf is enough.

Here, we want to find what is the optimal cut-off in HCA-cluster-tree 's Height and min_obs, which will result in the maximum number of significant compounds.

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_hplc_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")

# Initialize variables
# sig_comp_CO160_171 <- list()
sig_comp_CO120_159 <- list()

# Main loop
for (ele in 120:159) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    # Loop through each collapsed_compound (as columns)
    
    for (c in 3:ncol(joined)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                        joined[which(joined$group == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                     paste0(p_1, "&", p_2), 
                                     pval_wilcox_test)
      }
    }
    
    
    sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
    
    sig_comp_CO120_159[[paste0("Cut height_", ele)]] <- sp_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sp_df %>%
            filter(., adjusted_pvalue_holm < 0.05) %>%
            arrange(adjusted_pvalue_holm))
  }
}

writexl::write_xlsx(x = sig_comp_CO120_159, path = "Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO120-159_zeroimputed_15Jan2024.xlsx")
```

###### Summary of significant compounds

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_Combined GC HPLC_by_Robust Aichitson_HCA clustering result.xlsx")

sheet_names <- excel_sheets(path = p)
sus_pol_combined <- lapply(sheet_names, function(sheet) read_excel(path = p, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list <- lapply(sus_pol_combined, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique <- unique(unlist(unique_values_list))

all_unique <- data.frame(comp=unique)

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_all_significant compounds.xlsx")
```


#### With Combined GC ICP - DONE (ZeroImputed_Jan 21 2024) 

(Update 21 Jan 2024:) Finally finished GC-ICP ZeroImputed, here from Cute_height [1] to Cut_height[13], we have cases of less than 2 satisfied_grp, so in the if conditioning, we jump out of the current for loop and go to the Cut_height[14] to Cut_height[97] (aka. the final Cut_height since Cut_Height[98], aka. top Cut_Height only have 1 group)

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_icp_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(gc_icp_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(gc_icp_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")

# Initialize variables
sig_comp_CO1_98 <- list()

# Main loop
for (ele in 1:98) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_grp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_grp <- c(satisfied_grp, gr)
    }
  }
  
  if (length(satisfied_grp) < 2) {
    next
  } else {
    # Loop through each collapsed_compound (as columns)
    
    for (c in 3:ncol(joined)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_grp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_grp, 2)[1,col]
        p_2 <- utils::combn(satisfied_grp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                        joined[which(joined$group == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                     paste0(p_1, "&", p_2), 
                                     pval_wilcox_test)
      }
    }
    
    
    sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
    
    sig_comp_CO1_98[[paste0("Cut height_", ele)]] <- sp_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sp_df %>%
            filter(., adjusted_pvalue_holm < 0.05) %>%
            arrange(adjusted_pvalue_holm))
  }
}

writexl::write_xlsx(x = sig_comp_CO1_98, path = "Wilcoxon_test_Combined GC-ICP_by_Robust Aichitson_HCA clustering result_CO14-97_zeroimputed_21Jan2024.xlsx")
```

#### With Combined HPLC ICP - TBA (ZeroImputed_Jan 21 2024) 

(Update 21 Jan 2024:) Start runnig HPLC-ICP HCA Wilcoxon tests

(Update 25 Jan 2024:) Fucking Microsoft update! I have run this wilcoxon test again, but now i am starting from CUT 20 so we don't waste time.

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- hplc_icp_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(hplc_icp_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(hplc_icp_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")

# Initialize variables
sig_comp_CO20_130 <- list()

# Main loop
for (ele in 20:130) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    # Loop through each collapsed_compound (as columns)
    
    for (c in 3:ncol(joined)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                        joined[which(joined$group == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                     paste0(p_1, "&", p_2), 
                                     pval_wilcox_test)
      }
    }
    
    
    sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
    
    sig_comp_CO20_130[[paste0("Cut height_", ele)]] <- sp_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sp_df %>%
            filter(., adjusted_pvalue_holm < 0.05) %>%
            arrange(adjusted_pvalue_holm))
  }
}

writexl::write_xlsx(x = sig_comp_CO20_130, path = "Wilcoxon_test_Combined HPLC-ICP_by_Robust Aichitson_HCA clustering result_CO20-130_zeroimputed_25Jan2024.xlsx")
```

#### With just GC - DONE (ZeroImputed_Jan 18 2024)
(Update 18 Jan 2024:) To maximise runtime, this will be run on Huy's Laptop

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(gc_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(gc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")


# Initialize variables
# sig_comp_CO56_75 <- list()
sig_comp_CO1_63<- list()

# Main loop
for (ele in 1:63) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_sp, 2)[1,col]
      p_2 <- utils::combn(satisfied_sp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp_CO1_63[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

# combine cut off dataframe
# sig_comp <- c(sig_comp_CO0_55, sig_comp_CO56_75)

writexl::write_xlsx(x = sig_comp_CO1_63, path = "Wilcoxon_test_GC_by_Robust Aichitson_HCA clustering result_CO1-63_zeroimputed_18Jan2024.xlsx")
```

###### Summary of significant compounds

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_by_Robust Aichitson_HCA clustering result.xlsx")

sheet_names <- excel_sheets(path = p)
sus_pol_combined <- lapply(sheet_names, function(sheet) read_excel(path = p, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list <- lapply(sus_pol_combined, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique <- unique(unlist(unique_values_list))

all_unique <- data.frame(comp=unique)

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_all_significant compounds.xlsx")
```

The code below run wilcoxcon test on NA imputed data frame and don't perform minimum observation method. It reports the number of significant compounds at each cutree Height.

After running the code with NA imputed data, it generates significant compounds (at best case for GCMS data =\> 31 significant compounds) as compared to Zero significant compounds if using Missing-Observation Method. Therefore, from now on, for Wilcoxon on HCA results, we impute both the HCA input and the Wilcoxon input in the same way.

#### With just HPLC - DONE (ZeroImputed_Jan 18 2024) 
(Update 18 Jan 2024:) To maximise runtime, this will be run on Huy's Laptop.

(Update 21 Jan 2024:) Finally finished HPLC ZeroImputed from 18 Jan 2024, here from Cute_height [1] to Cut_height[15], we have cases of less than 2 satisfied_grp, so in th eif conditioning, we jump out of the current for loop and go to the Cut_height[16] to Cut_height[94] (aka. the final Cut_height since Cut_Height[95], aka. top Cut_Height only have 1 group)

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- hplc_list[[1]] %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(hplc_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(hplc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")


# Initialize variables
sig_comp_CO16_94 <- list()

# Main loop
for (ele in 1:95) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_grp <- c()
  for (gr in unique(joined$group)) {
    # if the group less than 2 members, then that group is excluded from wilcoxon test b/c Wilcoxon required at least 3 members for significant calculation
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_grp <- c(satisfied_grp, gr)
    }
  }
  # after filtering out groups that have less than 2 members, if we have less than 2 satisfied group, then jump to the next cutoff's height
  if (length(satisfied_grp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_grp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_grp, 2)[1,col]
      p_2 <- utils::combn(satisfied_grp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp_CO16_94[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

# writexl::write_xlsx(x = sig_comp_CO16_94, path = "Wilcoxon_test_HPLC_by_Robust Aichitson_HCA clustering result_CO1-79_zeroimputed_18Jan2024.xlsx")
```

#### With just ICP - DONE (ZeroImputed_Jan 18 2024) 
(Update 18 Jan 2024:) To maximise runtime, this will be run on Huy's Laptop

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- icp_list[[1]] %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(icp_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(icp_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")


# Initialize variables
sig_comp_CO1_34<- list()

# Main loop
for (ele in 1:34) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_sp, 2)[1,col]
      p_2 <- utils::combn(satisfied_sp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp_CO1_34[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

# combine cut off dataframe
# sig_comp <- c(sig_comp_CO0_55, sig_comp_CO56_75)

writexl::write_xlsx(x = sig_comp_CO1_34, path = "Wilcoxon_test_ICP_by_Robust Aichitson_HCA clustering result_CO1-34_zeroimputed_18Jan2024.xlsx")
```

### Wilcoxon test without HCA clustering precursor

#### By Category

##### Do we need to change Data processing-Step 4 (Subsetting Non-unique compounds), for ex At step 4, we need to subset compounds that appear in more than 2 categories instead of 2 files? =\> NO!!

##### COMBINED GC_HPLC

###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplc <- list()
for (i in 1:length(gc_hplc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_hplc_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gchplc[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_list[[i]]$Feature == sig_comp_gchplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp_gchplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp_gchplc, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplc <- list()

for (i in 1:length(gc_hplc_list)) {
  stats <- gc_hplc_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
    # stats::runif(length(which(base::is.na(stats[r,]))),
    #                                                         min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
    #                                                         max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gchplc[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gchplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_list[[i]]$Feature == sig_comp_gchplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gchplc, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Category_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Category_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_all_significant compounds.xlsx")
```

##### COMBINED GC_ICP
###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gcicp <- list()
for (i in 1:length(gc_icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gcicp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_gcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_icp_list[[i]]$Feature == sig_comp_gcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gcicp, path = "Wilcoxon_test_COMBINED GC_ICP_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gcicp <- list()

for (i in 1:length(gc_icp_list)) {
  stats <- gc_icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gcicp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_icp_list[[i]]$Feature == sig_comp_gcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gcicp, path = "Wilcoxon_test_COMBINED GC_ICP_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### COMBINED HPLC_ICP

(Task need to be done [12th Jan 2024]): For the GC // HPLC combo with ICP data, I need to rewrite the part of the code to match the RT and m.z with only GC // HPLC compounds in the original df lists, because ICP data does not have RT and m.z values.

###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplcicp <- list()
for (i in 1:length(hplc_icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- hplc_icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_hplcicp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_hplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_icp_list[[i]]$Feature == sig_comp_hplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplcicp, path = "Wilcoxon_test_COMBINED HPLC_ICP_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplcicp <- list()

for (i in 1:length(hplc_icp_list)) {
  stats <- hplc_icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_hplcicp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_hplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_icp_list[[i]]$Feature == sig_comp_hplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplcicp, path = "Wilcoxon_test_COMBINED HPLC_ICP_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### COMBINED GC_HPLC_ICP
###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplcicp <- list()
for (i in 1:length(gc_hplc_icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_hplc_icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gchplcicp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_gchplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_icp_list[[i]]$Feature == sig_comp_gchplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gchplcicp, path = "Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplcicp <- list()

for (i in 1:length(gc_hplc_icp_list)) {
  stats <- gc_hplc_icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gchplcicp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gchplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_icp_list[[i]]$Feature == sig_comp_gchplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gchplcicp, path = "Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### ATDGCMS

###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gc <- list()
for (i in 1:length(gc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gc[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_list[[i]]$Feature == sig_comp_gc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp_gc[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp_gc, path = "Wilcoxon_test_GC_data_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gc <- list()

for (i in 1:length(gc_list)) {
  stats <- gc_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
     
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gc[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gc[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_list[[i]]$Feature == sig_comp_gc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_list[[i]][idx, ]$m.z))
    }

    sig_comp_gc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gc, path = "Wilcoxon_test_GC_data_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Category_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Category_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_GC_data_by_Category_all_significant compounds.xlsx")
```

##### HPLCTOFMS

###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplc <- list()
for (i in 1:length(hplc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- hplc_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_hplc[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_hplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_list[[i]]$Feature == sig_comp_hplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplc, path = "Wilcoxon_test_HPLC_data_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplc <- list()

for (i in 1:length(hplc_list)) {
  stats <- hplc_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
    # stats::runif(length(which(base::is.na(stats[r,]))),
    #                                                         min = sort(hplc_list[[i]]$Values)[1]/10000,
    #                                                         max = sort(hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
     
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_hplc[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_hplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_list[[i]]$Feature == sig_comp_hplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplc, path = "Wilcoxon_test_HPLC_data_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Category_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Category_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_HPLC_data_by_Category_all_significant compounds.xlsx")
```

##### ICP
###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_icp <- list()
for (i in 1:length(icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_icp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_icp[[paste0("Grouping_", i)]] %>% arrange(comp))
}

writexl::write_xlsx(x = sig_comp_icp, path = "Wilcoxon_test_ICP_data_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method -TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_icp <- list()

for (i in 1:length(icp_list)) {
  stats <- icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
    # stats::runif(length(which(base::is.na(stats[r,]))),
    #                                                         min = sort(hplc_list[[i]]$Values)[1]/10000,
    #                                                         max = sort(hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
     
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_icp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_icp[[paste0("Grouping_", i)]] %>% arrange(comp))
}

writexl::write_xlsx(x = sig_comp_icp, path = "Wilcoxon_test_ICP_data_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

#### By Subcategory

Since the subcategory is the same across all 4 different consumer-based perspective groupings, we only need to run one dataframe of the list of df (this principle applies for either gc_hplc_list or gc_list or hplc_list). \##### Do we need to change Data processing-Step 4 (Subsetting Non-unique compounds), for ex At step 4, we need to subset compounds that appear in more than 2 categories instead of 2 files? =\> NO!!

##### COMBINED GC_HPLC

###### With Missing Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- gc_hplc_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(gc_hplc_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 3))
  }

  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
  
  
  # print(length(unique(list_subcat[[paste0("Min_obs_", min_obs)]]$comp)))
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Subcategory_Missing-observation_12Jan2024.xlsx")
```

###### With NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- gc_hplc_list[[1]] %>%
  select(File, Feature, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Subcategory, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- 0
  # stats::runif(length(which(base::is.na(stats[r,]))),
  #                                                         min = sort(gc_hplc_list[[1]]$Values)[1]/10000,
  #                                                         max = sort(gc_hplc_list[[1]]$Values)[2]/10000)
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(stats$Subcategory), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(stats$Subcategory), 2)[1,col]
    p_2 <- utils::combn(unique(stats$Subcategory), 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                           paste0(p_1, " & ", p_2), 
                           pval_wilcox_test)
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

print(sub_cat %>% arrange(comp))

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(sub_cat)[1] == 0) {
  next
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(sub_cat)) {
    idx <- base::which(gc_hplc_list[[1]]$Feature == sub_cat[row,]$comp)
    mean_rt <- c(mean_rt, mean(gc_hplc_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(gc_hplc_list[[1]][idx, ]$m.z))
  }

  sub_cat$mean_rt <- mean_rt
  sub_cat$mean_mz <- mean_mz
}


writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Subcategory_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Subcategory_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Subcategory_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Subcategory_all_significant compounds.xlsx")
```

When we ran a loop to investigate the significant compounds that can be obtained from getting all product_cat that has \> 3 to 6 obs, \> 5 obs gave the highest amount of significant compound.

##### COMBINED GC_ICP
###### with Missing-Observation method -TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- gc_icp_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))

  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(gc_icp_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(gc_icp_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(gc_icp_list[[1]][idx, ]$m.z), 3))
  }

  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }

  
  # print(length(unique(list_subcat[[paste0("Min_obs_", min_obs)]]$comp)))
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_COMBINED GC_ICP_by_Subcategory_Missing-observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- gc_icp_list[[1]] %>%
  select(File, Feature, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Subcategory, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- 0
  # stats::runif(length(which(base::is.na(stats[r,]))),
  #                                                         min = sort(gc_hplc_list[[1]]$Values)[1]/10000,
  #                                                         max = sort(gc_hplc_list[[1]]$Values)[2]/10000)
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(stats$Subcategory), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(stats$Subcategory), 2)[1,col]
    p_2 <- utils::combn(unique(stats$Subcategory), 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                           paste0(p_1, " & ", p_2), 
                           pval_wilcox_test)
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

print(sub_cat %>% arrange(comp))

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(sub_cat)[1] == 0) {
  next
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(sub_cat)) {
    idx <- base::which(gc_icp_list[[1]]$Feature == sub_cat[row,]$comp)
    mean_rt <- c(mean_rt, mean(gc_icp_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(gc_icp_list[[1]][idx, ]$m.z))
  }

  sub_cat$mean_rt <- mean_rt
  sub_cat$mean_mz <- mean_mz
}


writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_COMBINED GC_ICP_by_Subcategory_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### COMBINED HPLC_ICP
###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- hplc_icp_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))

  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(hplc_icp_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(hplc_icp_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(hplc_icp_list[[1]][idx, ]$m.z), 3))
  }

  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_COMBINED HPLC_ICP_by_Subcategory_Missing-observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- hplc_icp_list[[1]] %>%
  select(File, Feature, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Subcategory, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- 0
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(stats$Subcategory), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(stats$Subcategory), 2)[1,col]
    p_2 <- utils::combn(unique(stats$Subcategory), 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                           paste0(p_1, " & ", p_2), 
                           pval_wilcox_test)
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

print(sub_cat %>% arrange(comp))

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(sub_cat)[1] == 0) {
  next
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(sub_cat)) {
    idx <- base::which(hplc_icp_list[[1]]$Feature == sub_cat[row,]$comp)
    mean_rt <- c(mean_rt, mean(hplc_icp_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(hplc_icp_list[[1]][idx, ]$m.z))
  }

  sub_cat$mean_rt <- mean_rt
  sub_cat$mean_mz <- mean_mz
}


writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_COMBINED HPLC_ICP_by_Subcategory_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### COMBINED GC_HPLC_ICP
###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- gc_hplc_icp_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))

  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(gc_hplc_icp_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(gc_hplc_icp_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(gc_hplc_icp_list[[1]][idx, ]$m.z), 3))
  }

  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Subcategory_Missing-observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- gc_hplc_icp_list[[1]] %>%
  select(File, Feature, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Subcategory, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- 0
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(stats$Subcategory), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(stats$Subcategory), 2)[1,col]
    p_2 <- utils::combn(unique(stats$Subcategory), 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                           paste0(p_1, " & ", p_2), 
                           pval_wilcox_test)
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

print(sub_cat %>% arrange(comp))

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(sub_cat)[1] == 0) {
  next
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(sub_cat)) {
    idx <- base::which(gc_hplc_icp_list[[1]]$Feature == sub_cat[row,]$comp)
    mean_rt <- c(mean_rt, mean(gc_hplc_icp_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(gc_hplc_icp_list[[1]][idx, ]$m.z))
  }

  sub_cat$mean_rt <- mean_rt
  sub_cat$mean_mz <- mean_mz
}


writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Subcategory_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### ATDGCMS

###### With Missing Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- gc_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(gc_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(gc_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(gc_list[[1]][idx, ]$m.z), 3))
  }

  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_GC_data_by_Subcategory_Missing-observation_12Jan2024.xlsx")
```

###### With NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- gc_list[[1]] %>%
  select(File, Feature, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Subcategory, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- 0
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(stats$Subcategory), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(stats$Subcategory), 2)[1,col]
    p_2 <- utils::combn(unique(stats$Subcategory), 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                                   paste0(p_1, " & ", p_2), 
                                   pval_wilcox_test)
    
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

print(sub_cat %>% arrange(comp))

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(sub_cat)[1] == 0) {
  next
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(sub_cat)) {
    idx <- base::which(gc_list[[1]]$Feature == sub_cat[row,]$comp)
    mean_rt <- c(mean_rt, mean(gc_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(gc_list[[1]][idx, ]$m.z))
  }

  sub_cat$mean_rt <- mean_rt
  sub_cat$mean_mz <- mean_mz
}

writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_GC_data_by_Subcategory_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Subcategory_Missing-observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Subcategory_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_GC_data_by_Subcategory_all_significant compounds.xlsx")
```

##### HPLCTOFMS

###### With Missing Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- hplc_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(hplc_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 3))
  }

  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_HPLC_data_by_Subcategory_Missing-observation_12Jan2024.xlsx")
```

###### With NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- hplc_list[[1]] %>%
  select(File, Feature, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Subcategory, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- 0
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
# create vector of satisfied category 
satisfied_subcat <- c()
for (subcat in unique(stats$Subcategory)) {
  satisfied_subcat <- c(subcat, satisfied_subcat)
}

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(satisfied_subcat, 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(satisfied_subcat, 2)[1,col]
    p_2 <- utils::combn(satisfied_subcat, 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                                   paste0(p_1, " & ", p_2), 
                                   pval_wilcox_test)
    
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

print(sub_cat %>% arrange(comp))

# Check if the df is empty before create mean_rt and mean_mz column
if (dim(sub_cat)[1] == 0) {
  print("No significant compound found")
} else {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(sub_cat)) {
    idx <- base::which(hplc_list[[1]]$Feature == sub_cat[row,]$comp)
    mean_rt <- c(mean_rt, mean(hplc_list[[1]][idx, ]$RT))
    mean_mz <- c(mean_mz, mean(hplc_list[[1]][idx, ]$m.z))
  }

  sub_cat$mean_rt <- mean_rt
  sub_cat$mean_mz <- mean_mz
}


writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_HPLC_data_by_Subcategory_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Subcategory_Missing-observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Subcategory_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_HPLC_data_by_Subcategory_all_significant compounds.xlsx")
```

##### ICP
###### with Missing-Observation method

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- icp_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_ICP_data_by_Subcategory_Missing-observation_12Jan2024.xlsx")
```

###### with NA imputation method

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
stats <- icp_list[[1]] %>%
  select(File, Feature, Subcategory, Values) %>%
  # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Subcategory, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")
  
for (r in 1:nrow(stats)) {
  stats[r, which(base::is.na(stats[r,]))] <- 0
  # stats::runif(length(which(base::is.na(stats[r,]))),
  #                                                         min = sort(hplc_list[[1]]$Values)[1]/10000,
  #                                                         max = sort(hplc_list[[1]]$Values)[2]/10000)
}

df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
# create vector of satisfied category 
satisfied_subcat <- c()
for (subcat in unique(stats$Subcategory)) {
  satisfied_subcat <- c(subcat, satisfied_subcat)
}

# Loop through each collapsed_compound (as columns)
for (c in 2:ncol(stats)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(satisfied_subcat, 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(satisfied_subcat, 2)[1,col]
    p_2 <- utils::combn(satisfied_subcat, 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                    stats[which(stats$Subcategory == p_2), c])$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(stats)[c], 
                                   paste0(p_1, " & ", p_2), 
                                   pval_wilcox_test)
    
  }
}

df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")

sub_cat <- df %>%
  filter(., adjusted_pvalue_holm < 0.05) %>%
  arrange(adjusted_pvalue_holm)

print(sub_cat %>% arrange(comp))

writexl::write_xlsx(x = sub_cat, path = "Wilcoxon_test_ICP_data_by_Subcategory_NA imputed_zeroimputed_12Jan2024.xlsx")
```

### Matching HPLC Wilcoxon significant compounds from HPLC suspect screening hits (EF)

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sus_scr <- readxl::read_xlsx(path = paste0(getwd(), "/data/PMT Suspect Screening Plastics Results EF.xlsx"), skip = 1)

# Remove all legacy empty rows from Eric's Excel file 
sus_scr <- sus_scr[rowSums(is.na(sus_scr)) != (ncol(sus_scr) - 1), ]
```

### 1. With HCA

(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from HCA clustering of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - Zeroimputed: AMong 2737 significant compounds -> we found 13 compounds that matches with EF's Suspect Screening
 [1] "4-ethenyl-cyclohexene"                             "4-Aminophenol"                                     "1,2,3,4-tetrahydro-naphthalene"                   
 [4] "1,1'-oxybis[2-methoxy-ethane]"                     "Benzothiazole"                                     "Acetaminophen (Paracetemol)"                      
 [7] "1-methyl-1-phenylethylhydroperoxide"               "Hexahydro-1,3-isobenzofurandione"                  "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"         
[10] "2-amino-5-methyl-benzenesulfonic acid"             "Dibutyl ester phosphoric acid"                     "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[13] "2-(2H-benzotriazol-2-yl)-4-methyl-phenol"  


```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO130-159_18Dec2023.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO151-162_06Dec2023.xlsx")
p3 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO161-172_06Dec2023.xlsx")

sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Inspect which subdf has maximum number of significant compounds
lapply(sus_pol_combined1, dim)
# Get the no of unique compounds in that subdf
length(unique(sus_pol_combined1[[]]$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unique(sus_pol_combined1[[71]]$comp))

all_unique <- data.frame(comp=unique(c(unique1))) %>%
  # get rid off all GC compounds if HPLC is in combination with other data sources.
  filter(., str_detect(comp,"HPLCTOFMS"))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  # print(idx)
  if (base::identical(idx, integer(0))) {
    next
  } else {
    # print(sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),])
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Polymer_Type_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

### 2. Without HCA
#### 2.1 Polymer Type level
[Update 17 Jan 2024:]
- For HPLC only data, we found the same 6 Suspects as with the "Category" level (Section 2.2 below)
[1] "4-Aminophenol"                       "2,6-dimethyl-benzenamine"           
[3] "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide"
[5] "TEP / Triethyl phosphate"            "N,N'-diphenyl-guanidine" 

- For GC + HPLC data, we found 4 suspects, all 4 of them are nested in the HPLC-only case ==> GC+HPLC found less suspect hit than HPLC-only data:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"   
[4] "N,N'-diphenyl-guanidine"

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_polymer_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Polymer_Type_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

#### 2.2 Category level
(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from "Category" of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - MO method: Among 24 significant compounds -> we found 3 compounds that matches with EF's Suspect Screening
 "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"
 
2. HPLC only - Zeroimputed: Among 664 significant compounds -> we found 11 compounds that matches with EF's Suspect Screening
[1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
 [4] "1,2,3,4-tetrahydro-naphthalene"                    "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "TEP / Triethyl phosphate"                          "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"         
[10] "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]" "2-(2H-benzotriazol-2-yl)-4-methyl-phenol"

3. HPLC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"      

4. HPLC + ICP - zeroimputed method:
 [1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
 [4] "1,2,3,4-tetrahydro-naphthalene"                    "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"          "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[10] "2-(2H-benzotriazol-2-yl)-4-methyl-phenol" 

5. HPLC + GC - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"   

6. HPLC + GC - Zeroimputed method:
 [1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1H-Benzotriazole"                                 
 [4] "1,3,5-Triazine-2,4,6-triamine"                     "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "TEP / Triethyl phosphate"                          "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[10] "Oxybenzone (Benzophenone-3)" 

7. HPLC + GC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"    

8.HPLC + GC + ICP - zeroimputed method:
[1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
[4] "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"               "Hexahydro-1,3-isobenzofurandione"                 
[7] "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"

[Update 17 Jan 2024:]
- For HPLC only data, we found also 6 suspect hits that match with Eric Fries's suspect screening excel file:
[1] "4-Aminophenol"                       "2,6-dimethyl-benzenamine"           
[3] "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide"
[5] "TEP / Triethyl phosphate"            "N,N'-diphenyl-guanidine" 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Zeroimputed_HPLCbenchmarksremoved/Wilcoxon_test_COMBINED GC_ICP_by_Subcategory_NA imputed_zeroimputed_18Jan2024.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))
lapply(sus_pol_combined1, dim)

# Unlisting and then finding unique values across all data frames
unique1 <- unique(sus_pol_combined1[[1]]$comp)

all_unique <- data.frame(comp=unique(c(unique1))) %>%
   filter(., str_detect(comp,"HPLCTOFMS"))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,]) # Here we use just hplc_list because when we merge hplc data with other data, the compounds stay the same anyway
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Category_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

#### 2.3 Subcategory level
(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from "Subcategory" of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - MO method: Among 24 total significant compounds -> we found 5 compounds that matches with EF's Suspect Screening
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine"

2. HPLC only - Zeroimputed: There is ZERO significant compound

3. HPLC + ICP - MO method: 
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine" 

4. HPLC + ICP - zeroimputed method: There is ZERO significant compound

5. HPLC + GC - MO method:
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine" 

6. HPLC + GC - Zeroimputed method: There is ZERO significant compound

7. HPLC + GC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine"  

8. HPLC + Gc + ICP - zeroimputed method: There is ZERO significant compound

[UPDATE 17 Jan 2024:] 
- For HPLC only resulted in ZERO significant compounds from Wilcoxon tests so we don't include it in this summary
- For GC + HPLC data, there are 4 compounds but they all ATDGCMS so we also don't include it in this summary 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Zeroimputed_HPLCbenchmarksremoved/Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Subcategory_Missing-observation_zeroimputedbatch_18Jan2024.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))
lapply(sus_pol_combined1, dim)

# Unlisting and then finding unique values across all data frames
unique1 <- unique(sus_pol_combined1[[7]]$comp)

all_unique <- data.frame(comp=unique(c(unique1)))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Category_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

## Demo RF

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(randomForestSRC)

rfsrc.result <- function(dat, split.ratio) {
  set.seed(1234)
  
  plastic_idx <- caret::createDataPartition(dat$Category, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]
  
  # Train

  rf <- randomForestSRC::rfsrc(Category ~ ., 
                                    ntree=1000, 
                                    splitrule = "auc", 
                                    nodesize = 1, #Minumum size of terminal node for classification (1)
                                    # mtry = 21,
                                    importance = "permute", 
                                    samptype = "swr",
                                    membership = TRUE,
                                    perf.type="misclass",
                                    block.size = 1, # cumulative error rate on every tree
                                    data = plastic_trn
  )
  
  # Prediction results
  pred_res <- predict(rf, newdata = plastic_tst, outcome = "test")$predicted
  rownames(pred_res) <- rownames(plastic_tst)
  
  # Variable importance
  imp_var <- vimp(rf, importance = "permute")$importance
  
  # md.obj <- max.subtree(mergePC.rf)
  # best.feature <- md.obj$topvars # extracts the names of the variables in the object md.obj
  
  return(list(rf, 
              pred_res, 
              # pred_avg, 
              imp_var))
}

rfsrc.plots <- function(preddat, rf.mod) {
  newdat <- as.data.frame(preddat) %>%
    tibble::rownames_to_column(., var = "NewFile") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  
  # plots <- list()
  # Make bar graph of prediction probability -----
  a <- ggplot(data = newdat) + 
    geom_col(aes(x = NewFile, y = prob, fill = Category), 
             position = "dodge" # separating stacking prob cols
    ) +
    scale_fill_brewer(palette = "Set2") +
    scale_y_continuous(n.breaks = 10) +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 90))
  
  # plot OOB error rate against the number of trees -------
  b <- plot(ggRandomForests::gg_error(rf.mod))
  
  # Estimate the variables importance --------
  my.rf.vimp <- ggRandomForests::gg_vimp(rf.mod, nvar = 100) # provides the predictor's importance of top 100 predictors
  # plots[[3]] <- 
  c <- plot(my.rf.vimp) # visualises the predictors importance

  return(list(a,b,c))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df <- gc_list[[1]] %>%
  dplyr::select(NewFile, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(NewFile, Category, collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(gc_list[[1]]$Values)[1],
                                             max = sort(gc_list[[1]]$Values)[2])
}

rfsrc.result(dat = df, split.ratio = 0.6)[[2]]

rfsrc.plots(preddat = rfsrc.result(dat = df, split.ratio = 0.6)[[2]],
            rf.mod = rfsrc.result(dat = df, split.ratio = 0.6)[[1]])

```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(gc_list)) {
  df_pca <- input_df(gc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[item]][which(gc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

### With just HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
df <- hplctofms %>%
  dplyr::select(File, product_cat, collapsed_compound, Values) %>%
  mutate(product_cat = factor(product_cat, levels = unique(product_cat))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, product_cat, collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(hplctofms$Values)[1],
                                             max = sort(hplctofms$Values)[2])
}

RFresult <- rfsrc.result(dat = df, split.ratio = 0.6)
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[item]][which(hplc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE,message=FALSE}
df <- gc_hplc_list[[1]] %>%
  dplyr::select(File, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min1 = sort(gc_hplc_list[[1]]$Values)[1],
                                             min2 = sort(gc_hplc_list[[1]]$Values)[2])
}

RFresult <- rfsrc.result(dat = df, split.ratio = 0.6)

# Make column of true Category of each row
preddat <- base::as.data.frame(RFresult[[2]]) %>% 
  tibble::rownames_to_column(., var = "File") 

preddat$true_category <- gc_hplc_list[[1]][match(preddat$File, gc_hplc_list[[1]]$File),]$Category

# For each row of prediction df, return the column name (aka. Category) with largest prediction probability
predres<- data.frame(predict_cat = colnames(preddat[, 2:6])[apply(preddat[, 2:6], 1, which.max)], 
                     true_cat = preddat$true_category)

rfsrc.plots(preddat = RFresult[[2]],
            rf.mod = RFresult[[1]])
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[item]][which(gc_hplc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

## CONCLUSION OF RANDOMFOREST ON COMBINATION OF ANALYTICAL DATA

#### =\> For single ATDGCMS or HPLCTOF or Combined ATD/HPLC, all of them gave out the same (OOB) Requested performance error \~ 0.4. All of them have the same confusion matrix results, in every single category was mistakenly categorized as Food contact materials !

### With minimum sample size - Justa test, maybe irrelevant later on

#### PCAtools

```{r, echo=FALSE, warning = FALSE}
PCAtools_mindf <- ptype_n_samp(PCAtools_mergePC, use = "min")
PCAtools_mindf.RFresult <- rfsrc.result(PCAtools_mindf, split.ratio = 0.6, sep = FALSE)
rfsrc.plots(preddat = PCAtools_mindf.RFresult[[2]],
            rf.mod = PCAtools_mindf.RFresult[[1]])
```

### What is the average accuracy for USSB and USE for each product category?

For Mixed_Plastic_Waste, Other, Plastic_Bottles_and_Bottle_Caps, Food_Packaging_Waste, there is a lack of Store-Bought samples.

```{r, echo=FALSE, warning = FALSE}
print(PCAtools_alldf.RFresult[[3]] %>% arrange(plastic_type))
```

### What happens to the prediction accuracy if we use only USSB to predict USE samples? If product doesn't have USSB, then train w/ USE

```{r, echo=FALSE, warning = FALSE, }
PCAtools_alldf.RFresult <- rfsrc.result(PCAtools_alldf, split.ratio = 0.6, sep = TRUE)
rfsrc.plots(preddat = PCAtools_alldf.RFresult[[2]],
            rf.mod = PCAtools_alldf.RFresult[[1]])
```

## Demo LightGBM

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(lightgbm)
library(gridExtra)

lightgbm.result <- function(dat, split.ratio) { # dat must have same format as PCATools input
  
  test <- copy(dat)
  # Must convert product_cat from factors to numeric
  test$Category <- as.numeric(as.factor(test$Category)) - 1L
  # Split train and test sets
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(test$Category, p = 0.6, list = F)
  plastic_trn <- as.matrix(test[plastic_idx, ])
  plastic_tst <- as.matrix(test[-plastic_idx, ])
  
  dtrain <- lgb.Dataset(data = plastic_trn[, 2:ncol(plastic_trn)], # data is all the columns, aka. all the compounds
                        label = plastic_trn[, 1]) # label is Category column
  dtest <- lgb.Dataset.create.valid(dtrain, 
                                    data = plastic_tst[, 2:ncol(plastic_tst)], 
                                    label = plastic_tst[, 1])
  
  # Validation set to be used during training, not testing!!!
  valids <-  list(test = dtest)
  
  # Setup parameters
  params <- list(
    min_data = 1L
    , learning_rate = 0.1
    , objective = "multiclass"
    , metric = "multi_error"
    , num_class = length(unique(dat$Category))
    , boosting = "dart"
    , xgboost_dart_mode ="true"
    , num_threads = 2
  ) 
  
  lgb.model <- lgb.train(params,
                         dtrain,
                         nrounds = 1000,
                         valids, 
                         early_stopping_rounds = 25L
  )
  
  # prediction
  my_preds <- round(predict(object = lgb.model, 
                      data = plastic_tst[, 2:ncol(plastic_tst)],
                      # params = list(output_result = "response"),
                      reshape = TRUE),
                    3)

  colnames(my_preds) <- levels(dat$Category)
  rownames(my_preds) <- rownames(plastic_tst)
  
  newdat <- as.data.frame(my_preds) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  
  # Important features
  tree_imp <- lgb.importance(lgb.model, percentage = T)
  varimpmat <- lgb.plot.importance(tree_imp, measure = "Gain")

  return(list(my_preds, varimpmat, newdat))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# gbm_list <- list()
# for (i in 1:length(gc_list)) {
  df <- gc_list[[2]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_list[[2]]$Values)[1],
                                               max = sort(gc_list[[2]]$Values)[2])
  }
  
  # gbm_list[[i]]
gbm <- lightgbm.result(df, split.ratio = 0.6)
# }

# Make bar graph of prediction probability
ggplot(data = gbm[[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

Here, same problem happened where all most all the sample in test set were misclassified as "Food contact materials"

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_list)) {
  
  df_pca <- input_df(gc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[i]][which(gc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

With PCA df, there is a slight improvement, not everything is classified as food contact material anymore, but there are still lots of misclassification, for ex, some Balloons samples are classified as MPW or as clothes or Misc

### With just HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(hplc_list)) {
  df <- hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(hplc_list[[i]]$Values)[1],
                                               max = sort(hplc_list[[i]]$Values)[2])
  }
  
  gbm_list[[i]] <- lightgbm.result(df, split.ratio = 0.6)
}

# Make bar graph of prediction probability
ggplot(data = gbm_list[[1]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[i]][which(hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df <- gc_hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_hplc_list[[i]]$Values)[1],
                                               max = sort(gc_hplc_list[[i]]$Values)[2])
  }
  
  gbm_list[[i]] <- lightgbm.result(df, split.ratio = 0.6)
}

# Make bar graph of prediction probability
ggplot(data = gbm_list[[1]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[i]][which(gc_hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

## CONCLUSION OF LIGHTGBM ON COMBINATION OF ANALYTICAL DATA

#### =\> Regardless of single or combined dataset, the same misclassification pattern occured, in which almost everything was misclassified as Food Contact Materials (The error is always \> 0.3, EXCEPT FOR 1 Scenario, where single ATDGCMS-PCA merge df was used --\> error in this is \~ 0.24)

## Demo SVM

#### Data format should be same as PCATools input (column as "sample name", row as "collapsed_compounds")

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(e1071)
library(caret)
library(ROCR)
library(MLmetrics)


# e1071 package: ===========================
# PArtitioning train&test sets / training / predict on test set
e1071.SVM.result <- function(dat, split.ratio){
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(dat$Category, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]  
  
  # Create 1 million evenly spaced values on a log scale between 10^-6 and 10^2
  # lseq <- function(from = 0.000001, to = 100, length.out=1000000) {
  #   # logarithmic spaced sequence
  #   exp(seq(log(from), log(to), length.out = length.out))
  # }
  
  # Model tuning
  tune.out <- e1071::tune(e1071::svm, Category ~ ., 
                          data = plastic_trn,
                          kernel = "radial",
                          scale = TRUE,
                          # ranges = list(cost = lseq()),
                          decision.values = TRUE, 
                          probability = TRUE)
  bestmod <- tune.out$best.model
  
  # Prediction results
  pred_prob <- predict(bestmod, newdata = plastic_tst,
                       decision.values = TRUE, probability = TRUE)
  pred_res <- attr(pred_prob, "probabilities")
  
  return(pred_res)
}

plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}  
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
svm_list <- list()
for (i in 1:length(gc_list)) {
  df <- gc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_list[[i]]$Values)[1],
                                               max = sort(gc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# PCA-based feature data
svm_list <- list()
for (i in 1:length(gc_list)) {
  df_pca <- input_df(gc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[i]][which(gc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With just HPLC data

#### Original Data

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(hplc_list)) {
  df <- hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(hplc_list[[i]]$Values)[1],
                                               max = sort(hplc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[i]][which(hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df <- gc_hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_hplc_list[[i]]$Values)[1],
                                               max = sort(gc_hplc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[i]][which(gc_hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

## CONCLUSION SVM ON COMBINATION OF ANALYTICAL DATA

#### =\> The same problem with RF and GBM: Almost ALL of the classes are misclassified as Food contact material -\__\-

## Demo Multinomial Logistic Regression

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(class)
library(caret)
library(nnet)
library(parallel)
library(doParallel)

## Link: https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/

# PArtitioning train&test sets / training / predict on test set
caret.multinomlog.result <- function(df, split.ratio){
  cl <- parallel::makeCluster(parallel::detectCores() - 1)
  doParallel::registerDoParallel(cl)
  
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(df$Category, p = 0.5, list = F)
  plastic_trn <- df[plastic_idx, ] %>%
    relocate(Category, .after = 1)
  plastic_tst <- df[-plastic_idx, ] %>%
    relocate(Category, .after = 1)
  
  plastic_multinomlog_mod <- caret::train(
    Category ~ .,
    data = plastic_trn,
    method = "multinom", # Penalized Multinomial Regression -> https://remiller1450.github.io/s230f19/caret3.html
    trControl = caret::trainControl(method = "cv", # "cv", repeatedcv",
                                    # number = 1,
                                    # repeats = 1, # for repeated k-fold cv only
                                    verboseIter = TRUE
    ),
    trace = FALSE,
    allowParallel= TRUE,
    MaxNWts = 6*ncol(plastic_trn) # maximum allowable number of weights
  )
  
  # Prediction results
  pred_res <- round(predict(plastic_multinomlog_mod, newdata = plastic_tst, type = "prob"), 5)
  
  # Extract the prediction label with highest probability for each sample in test set
  max_columns <- factor(unlist(apply(pred_res, 1, function(x) names(pred_res)[which.max(x)]),
                               recursive = FALSE, use.names = FALSE)
                        , levels = unique(plastic_tst$Category)
  )
  
  # Creating the Confusion Matrix
  cm <- caret::confusionMatrix(max_columns, plastic_tst$Category)
  print(cm)
  
  # Visualizing the Confusion Matrix
  ggplot(as.data.frame(cm$table), aes(Reference, Prediction)) +
    geom_tile(aes(fill = Freq), colour = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    geom_text(aes(label = sprintf("%0.0f", Freq)), vjust = 1) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Interpreting variable importance for multinomial logistic regression - `nnet::multinom()` and `caret::varImp()`
  # https://stackoverflow.com/questions/60060292/interpreting-variable-importance-for-multinomial-logistic-regression-nnetmu
  varimp <- caret::varImp(plastic_multinomlog_mod)

  return(list(pred_res, varimp))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df <- gc_list[[2]] %>%
  dplyr::select(File, Category, Feature, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(gc_list[[2]]$Values)[1],
                                             max = sort(gc_list[[2]]$Values)[2])
}

multinom <- caret.multinomlog.result(df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- multinom[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(multinom[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE}
# PCA-based feature data
df_pca <- input_df(gc_list[[1]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(gc_list[[1]][which(gc_list[[1]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_atdgcms.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_atdgcms.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_atdgcms.multinomlog.result[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With just HPLC data

#### Original Data

```{r, echo=FALSE, warning = FALSE}
df <- hplc_list[[2]] %>%
  dplyr::select(File, Category, Feature, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(hplc_list[[2]]$Values)[1],
                                             max = sort(hplc_list[[2]]$Values)[2])
}

multinom <- caret.multinomlog.result(df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- multinom[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(multinom[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools-merged df

```{r, echo=FALSE, warning = FALSE}
df_pca <- input_df(hplc_list[[1]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(hplc_list[[1]][which(hplc_list[[1]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_hplc.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_hplc.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_hplc.multinomlog.result[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

gc_list (1,2,3,4): DONE
icp_list (1,2,3,4): DONE
gc_icp_list (1,2,3,4): DONE

```{r, echo=FALSE, warning = FALSE}
# cl <- parallel::makeCluster(parallel::detectCores() /2)
# doParallel::registerDoParallel(cl)

df <- hplc_list[[2]] %>%
  dplyr::select(File, Category, Feature, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- 0 
  # runif(length(which(base::is.na(df[r,]))),
  #                                            min = sort(gc_hplc_list[[1]]$Values)[1],
  #                                            max = sort(gc_hplc_list[[1]]$Values)[2])
}

set.seed(1234)
plastic_idx <- caret::createDataPartition(df$Category, p = 0.5, list = F)
plastic_trn <- df[plastic_idx, ] %>%
  relocate(Category, .after = 1)
plastic_tst <- df[-plastic_idx, ] %>%
  relocate(Category, .after = 1)

table(plastic_trn$Category)
table(plastic_tst$Category)

system.time(plastic_multinomlog_mod <- caret::train(
  Category ~ .,
  data = plastic_trn,
  method = "multinom", # Penalized Multinomial Regression -> https://remiller1450.github.io/s230f19/caret3.html
  trControl = caret::trainControl(method = "cv", # "cv", repeatedcv",
                                  # number = 1,
                                  # repeats = 1, # for repeated k-fold cv only
                                  verboseIter = TRUE
  ),
  trace = FALSE,
  # allowParallel= TRUE,
  MaxNWts = 9*ncol(plastic_trn) # maximum allowable number of weights
)
)

# Prediction results
pred_res <- round(predict(plastic_multinomlog_mod, newdata = plastic_tst, type = "prob"), 5)

# Extract the prediction label with highest probability for each sample in test set
max_columns <- factor(unlist(apply(pred_res, 1, function(x) names(pred_res)[which.max(x)]),
                             recursive = FALSE, use.names = FALSE)
                      , levels = unique(plastic_tst$Category)
)

# Creating the Confusion Matrix
cm <- caret::confusionMatrix(max_columns, plastic_tst$Category)
print(cm)

# Visualizing the Confusion Matrix
ggplot(as.data.frame(cm$table), aes(Reference, Prediction)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  geom_text(aes(label = sprintf("%0.0f", Freq)), vjust = 1) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# gc_hplc.multinomlog.result <- caret.multinomlog.result(df, split.ratio = 0.5)

# # Plot top 20 variable important
# varImp <- gc_hplc.multinomlog.result[[2]]$importance %>% 
#   rownames_to_column(., var = "comp") %>%
#   filter(., Overall > 50) %>% # this is a random threshold
#   arrange(Overall)
# 
# ggplot(data = varImp, 
#        aes(x=Overall,y=comp)) +
#   geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
#   geom_point(color='skyblue') + 
#   xlab(" Importance Score")+
#   ggtitle("Variable Importance") + 
#   theme(plot.title = element_text(hjust = 0.5)) +
#   theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# # Make bar graph of prediction probability
# plot.dat <- function(dat) {
#   newdat <- as.data.frame(dat) %>%
#     tibble::rownames_to_column(., var = "File") %>%
#     tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "product_cat", values_to = "prob")
#   return(newdat)
# }   
# 
# mydat <- plot.dat(gc_hplc.multinomlog.result[[1]])
# 
# ggplot(data = mydat) + 
#   geom_col(aes(x = File, y = prob, fill = product_cat), 
#            position = "dodge" # separating stacking prob cols
#   ) +
#   scale_fill_brewer(palette = "Set2") +
#   scale_y_continuous(n.breaks = 10) +
#   theme_bw() + 
#   theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df_pca <- input_df(gc_hplc_list[[3]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(gc_hplc_list[[3]][which(gc_hplc_list[[3]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_gchplc.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_gchplc.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

print(ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black')))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_gchplc.multinomlog.result[[1]])

print(ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90)))
```

Food contact materials may have a lot of chemical properties in them that shares with other categories.
