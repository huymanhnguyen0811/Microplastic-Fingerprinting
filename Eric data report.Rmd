---
title: "Microplastic fingerprinting"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/huyng/OneDrive - Toronto Metropolitan University/Microplastic/Microplastic-Fingerprinting")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width='750px', dpi=200)
```

## Documentation

This repo is accompanying the publication: "Computational fingerprinting workflow for environmental source tracking of microplastic based on representative additives"

Demo of each ML algorithms is shown below.

## Data processing

You can include R code in the document as follows:

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# Loading Packages --------------------------------------------------------
library(ggplot2)
library(vegan)
library(readxl)
library(tidyverse)
library(dplyr)
library(data.table)
library(writexl)
library(tidyr)
library(grid)
library(gridExtra)

library(stats)
library(FactoMineR)
library(factoextra)
library(compositions)
library(ggforce)
library(latticeExtra)
library(cluster)

# Functions -------------------------------------------------------------------------------------------------------
'%notin%' <- Negate('%in%')

# Grouping compounds based on RT1, RT2, and Ion1 - Version 1
grouping_comp <- function(data, rtthres1, rtthres2, mzthres, type) {
  
  # create empty list, each sub-list is a compound group with following criteria:
  # rtthres: RT threshold window
  # mzthres: mz threshold window
  dat <- data[ , , drop = FALSE]  # Shallow copy to avoid modification of original data
  
  # Initialize the compound column filled with NA values
  dat$Feature <- NA
  i <- 1
  j <- 1 # Counter for HPLC data only
  
  # Checkpoint for 'type' parameter
  if (!(type %in% c("ATDGCMS", "HPLCTOFMS"))) {
    stop("Invalid type specified. Must be either 'ATDGCMS' or 'HPLCTOFMS'")
  }

  # Loop through each row to group compounds
  if (type %in% "HPLCTOFMS") {
      # Split HPLC data into RT <= 6 min and RT > 6 min
      dat1 <- dat %>% filter(RT <= 6) %>% arrange(Name, m.z, RT)
      dat2 <- dat %>% filter(RT > 6) %>% arrange(Name, m.z, RT)
      
      for (row in 1:nrow(dat1)) {
        # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
        comp1 <- dat1[row,]$Name
        rt <- base::as.numeric(dat1[row, "RT"])
        mz <- base::as.numeric(dat1[row, "m.z"])
        
        idx1 <- which(
          dat1$Name == comp1 &
          dat1$RT <= (rt + rtthres1) & dat1$RT >= (rt - rtthres1) &
            dat1$m.z <= (mz + mzthres) & dat1$m.z >= (mz - mzthres) &
            is.na(dat1$Feature)
        )
        
        if (length(idx1) > 0) {
          dat1[idx1, "Feature"] <- paste0("Compound_RT1_", i, ".", type)
          i <- i + 1
        }  
      }
      
      for (row in 1:nrow(dat2)) {
        # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
        comp2 <- dat2[row,]$Name
        rt <- base::as.numeric(dat2[row, "RT"])
        mz <- base::as.numeric(dat2[row, "m.z"])
        
        idx2 <- which(
          dat2$Name == comp2 &
          dat2$RT <= (rt + rtthres2) & dat2$RT >= (rt - rtthres2) &
            dat2$m.z <= (mz + mzthres) & dat2$m.z >= (mz - mzthres) &
            is.na(dat2$Feature)
        )
        
        if (length(idx2) > 0) {
          dat2[idx2, "Feature"] <- paste0("Compound_RT2_", j, ".", type)
          j <- j + 1
        }  
      }
      
      dat <- rbind(dat1, dat2)
      
  } else {
    for (row in 1:nrow(dat)) {
      
      # filter data by index, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
      rt <- base::as.numeric(dat[row, "RT"])
      mz <- base::as.numeric(dat[row, "m.z"])
      
      idx <- which(
        dat$RT <= (rt + rtthres1) & dat$RT >= (rt - rtthres1) &
          dat$m.z <= (mz + mzthres) & dat$m.z >= (mz - mzthres) &
          is.na(dat$Feature)
      )
      
      if (length(idx) > 0) {
        dat[idx, "Feature"] <- paste0("Compound_", i, ".", type)
        i <- i + 1
      }  
    }
  }
  
  return(dat)
}


# Filtering similar and unique compound 

comp_filter <- function(data) {
  all_other_compounds_idx <- c()
  all_unique_compounds_idx <- c()

  for (comp_grp in unique(data$Feature)) {
    # filter data by indexing, ALWAYS DO THIS INSTEAD OF CREATE SUBSET DATAFRAME
    idx <- which(grepl(comp_grp, data$Feature, fixed = TRUE))
    
    if (length(idx) < 2) {
      all_unique_compounds_idx <- c(all_unique_compounds_idx, idx)
    } else {
      all_other_compounds_idx <- c(all_other_compounds_idx, idx)
    }
  }
  return(list(all_other_compounds_idx, all_unique_compounds_idx))
}
```

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# STEP 1.1: Data import --------------------------------------------
data <- readxl::read_xlsx("C:/Users/Emerging contaminant/Desktop/Huy-Plastic-Fingerprint/Microplastic-Fingerprinting/data/0_Huy/Combined_Leachates_14-05-24.xlsx") %>% 
  filter((Product %notin% "USSB-18")) %>% 
  select(-c(Type, Score, Hits, `Diff (Tgt, ppm)`))

colnames(data)[8] <- "m.z"
data$Sample_name <- paste0(data$Product, "_", data$Day)

data <- data %>% 
  select(-c(Product)) %>% 
  relocate(Sample_name, .after = 2)

# For each row, if the all blanks are missing then delete that row
rows_to_remove <- rowSums(is.na(data[, 12:25])) == length(12:25)

data <- data[!rows_to_remove, ]
data$`Acetaminophen-d4 RT` <- as.numeric(unlist(data$`Acetaminophen-d4 RT`))
```

### Quality assurance for STEP 1.3B: Examine variation in RT of 4 benchmark compounds in HPLC across all samples with histogram distribution of RT of 4 benchmark compounds 

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
bm_df <- data %>%
  group_by(Sample_name, Replicate, Day) %>%
  summarise(Acetaminophen_RT = mean(`Acetaminophen-d4 RT`),
            dTEP_RT = mean(`dTEP RT`),
            dTPrP_RT = mean(`dTPrP RT`),
            PPD_RT = mean(`13C-6PPD RT`),
            dTPP_RT = mean(`dTPP RT`))

# Plot scatter of RT distribution
par(mfrow = c(3, 2))

hist(bm_df$Acetaminophen_RT[bm_df$Acetaminophen_RT < 6], main = "Histogram of Acetaminophen_RT", xlab = "Values", ylab = "Frequency")
hist(bm_df$dTEP_RT, main = "Histogram of dTEP_RT", xlab = "Values", ylab = "Frequency")
hist(bm_df$dTPrP_RT, main = "Histogram of dTPrP_RT", xlab = "Values", ylab = "Frequency")
hist(bm_df$PPD_RT, main = "Histogram of 6PPD_RT", xlab = "Values", ylab = "Frequency")
hist(bm_df$dTPP_RT, main = "Histogram of dTPP_RT", xlab = "Values", ylab = "Frequency")
```

### STEP 1.3B: Collapsing compounds based on RT1, RT2, Ion1 threshold --------------------------

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# HPLCTOFMS
# Apply different alignment to 2 parts of chromatogram via rtthres1 (RT <= 6 min) and rtthres2 (RT > 6 min)
combined_df2_grouped <- grouping_comp(data,
                                      rtthres1 = 0.4,
                                      rtthres2 = 0.15,
                                      mzthres = 0.005,
                                      type = "HPLCTOFMS")
```

### Step 1.3C: Cal. RPA using closest eluting benchmarks

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# remove all unique aligned compound
non_unique <- c()
for (comp in unique(combined_df2_grouped$Feature)) {
  idx <- which(grepl(comp, combined_df2_grouped$Feature, fixed = TRUE))
  if (length(idx) < 2) {
    next
  } else {
    non_unique <- c(non_unique, idx)
  }
}

non_unique_df <- combined_df2_grouped[non_unique,]

# for each aligned compound, if the compound occurs in 2 replicates of the same sample, keep those occurences and remove the rest
row_to_keep <- c()
for (comp in unique(non_unique_df$Feature)) {
  idx <- which(grepl(comp, non_unique_df$Feature, fixed = TRUE))
  count <- c()
  for (i in 1:length(unique(non_unique_df[idx,]$Sample_name))) {
    count <- c(count, length(unique(non_unique_df[idx, ][which(non_unique_df[idx,]$Sample_name == unique(non_unique_df[idx,]$Sample_name)[i]), ]$Replicate)))
  }
  if (any(count > 1)) {
    samp_to_keep <- unique(non_unique_df[idx,]$Sample_name)[which(count > 1)]
    row_to_keep <- c(row_to_keep, idx[which(non_unique_df[idx,]$Sample_name %in% samp_to_keep)])
  }
}

non_unique_df <- non_unique_df[row_to_keep,]

# Cal. RPA
tempdf_list <- list()
i <- 1
# cal. closest eluting benchmark
for (comp in unique(non_unique_df$Feature)) {
  idx <- which(non_unique_df$Feature == comp)
  tempdf <- non_unique_df[idx,]
  
  min_Acetaminophen <- abs(mean(non_unique_df[idx,]$RT) - mean(non_unique_df[idx,]$`Acetaminophen-d4 RT`))
  min_dTEP <- abs(mean(non_unique_df[idx,]$RT) - mean(non_unique_df[idx,]$`dTEP RT`))
  min_dTPrP <- abs(mean(non_unique_df[idx,]$RT) - mean(non_unique_df[idx,]$`dTPrP RT`))
  min_6PPD <- abs(mean(non_unique_df[idx,]$RT) - mean(non_unique_df[idx,]$`13C-6PPD RT`))
  min_dTPP <- abs(mean(non_unique_df[idx,]$RT) - mean(non_unique_df[idx,]$`dTPP RT`))
  all_min <- c(min_Acetaminophen, min_dTEP, min_dTPrP, min_6PPD, min_dTPP)
  
  if (which(all_min == min(all_min, na.rm = TRUE)) == 1) {
    tempdf$RPA <- tempdf$Area / tempdf$`Acetaminophen-d4 Area`
    tempdf_list[[i]] <- tempdf
    i <- i + 1
  } else if (which(all_min == min(all_min, na.rm = TRUE)) == 2) {
    tempdf$RPA <- tempdf$Area / tempdf$`dTEP Area`
    tempdf_list[[i]] <- tempdf
    i <- i + 1
  } else if (which(all_min == min(all_min, na.rm = TRUE)) == 3) {
    tempdf$RPA <- tempdf$Area / tempdf$`dTPrP Area`
    tempdf_list[[i]] <- tempdf
    i <- i + 1
  } else if (which(all_min == min(all_min, na.rm = TRUE)) == 4) {
    tempdf$RPA <- tempdf$Area / tempdf$`13C-6PPD Area`
    tempdf_list[[i]] <- tempdf
    i <- i + 1
  } else {
    tempdf$RPA <- tempdf$Area / tempdf$`dTPP Area`
    tempdf_list[[i]] <- tempdf
    i <- i + 1
  }
}

rpa_df <- bind_rows(tempdf_list)
```

### Step 2: Cal. concentration corrected RPA by Day

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
rpa_df$corrected_rpa <- NA

for (row in 1:nrow(rpa_df)) {
  if (rpa_df[row,]$Day == 0) {
    rpa_df[row,]$corrected_rpa <- rpa_df[row,]$RPA * rpa_df[row,]$`Conc Factor Day 0`
  } else if (rpa_df[row,]$Day == 2) {
    rpa_df[row,]$corrected_rpa <- rpa_df[row,]$RPA * rpa_df[row,]$`Conc Factor Day 2`
  } else if (rpa_df[row,]$Day == 15) {
    rpa_df[row,]$corrected_rpa <- rpa_df[row,]$RPA * rpa_df[row,]$`Conc Factor Day 15`
  } else {
    rpa_df[row,]$corrected_rpa <- rpa_df[row,]$RPA * rpa_df[row,]$`Conc Factor Day 34`
  }
}
```

### Step 3: Average replicates and blanks and cal. SD

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
avg_df <- rpa_df %>%
  group_by(Feature, Sample_name, Day) %>%
  summarise(mean_corrected_rpa = mean(corrected_rpa),
            sd_corrected_rpa = sd(corrected_rpa), 
            mean_RT = mean(RT))
```


### Step 4: Blank Subtraction per batch

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
batch_label <- readxl::read_xlsx(paste0(getwd(), "/data/0_Huy/Product_Batch_Matrix.xlsx"))

new_df <- full_join(avg_df, batch_label, by = "Sample_name") %>%
  mutate(type = ifelse(str_detect(Sample_name, "Blank"), "Blank", "Sample")) %>%
  filter(Sample_name %notin% c("USSB-18_0", "USSB-18_2", "USSB-18_15", "USSB-18_34", "Blank-Batch-2_34"))
  
  
# blank_substracted <- new_df %>%
#   select(Feature, Day, Batch, Sample_name, mean_corrected_rpa, type) %>%
#   group_by(Feature, Day, Batch) %>%
#   summarize(blank_subtraction = mean_corrected_rpa[type == "Sample"] - mean_corrected_rpa[type == "Blank"])


df_list <- list()
i <- 1
for (comp in unique(new_df$Feature)) {
  tempdf1 <- new_df[which(new_df$Feature == comp),]
  for (batch in unique(tempdf1$Batch)) {
    tempdf2 <- tempdf1[which(tempdf1$Batch == batch),]
    for (day in unique(tempdf2$Day)) {
      tempdf3 <- tempdf2[which(tempdf2$Day == day),]
      blank_mean_corrected_rpa <- tempdf3[which(tempdf3$type == "Blank"),]$mean_corrected_rpa
      blank_sd_corrected_rpa <- tempdf3[which(tempdf3$type == "Blank"),]$sd_corrected_rpa
      if (length(blank_mean_corrected_rpa) != 0) {
        # tempdf3 <- tempdf3 %>% filter(type != "Blank")
        tempdf3$blank_substracted <- tempdf3$mean_corrected_rpa - blank_mean_corrected_rpa
        negative_idx <- which(tempdf3$blank_substracted < 0)
        positive_idx <- which(tempdf3$blank_substracted > 0 & tempdf3$blank_substracted > 3*blank_sd_corrected_rpa)
        
        tempdf3 <- tempdf3[positive_idx,]
        tempdf3 <- tempdf3[-negative_idx,]
        df_list[[i]] <- tempdf3
        i <- i + 1
      } else {
        tempdf3$blank_substracted <- tempdf3$mean_corrected_rpa
        df_list[[i]] <- tempdf3
        i <- i + 1
      }
    }
  }
}

bs_df <- bind_rows(df_list)

bs_df <- rbind(bs_df, new_df %>% filter(type == "Blank"))

writexl::write_xlsx(bs_df, path = "blank_substracted_df.xlsx")
writexl::write_xlsx(rpa_df, path = "rpa_df.xlsx")
```


### Quality assurance: How many compounds after blank subtraction between different RT windows?
```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
# ATD-GC-MS
length(unique(comp_normalized1)$Feature)

# HPLC-TOFMS
length(unique(comp_normalized2)$Feature)
```


### STEP 4A: Identify shared and unique compound groups across samples 

(Update 20 Feb 2024:)  Add column to distinguish which sample is from GC or HPLC or ICP-MS datasetss.

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}

p <- "C:/Users/Emerging contaminant/Desktop/Huy-Plastic-Fingerprint/Microplastic-Fingerprinting/data/Table of product categorization_NewFileName.xlsx"


# Table of categorization
sampinfo <- readxl::read_excel(path = p, 
                               sheet = excel_sheets(path = p)[1])

# HPLCTOFMS
# joining data frame with sampinfo df
dat2 <- left_join(x = comp_normalized2, # %>% select(., -c(File)), 
                  y = sampinfo, # %>% select(., -c(File)),  
                  by = 'File')

idx_list_filter2 <- comp_filter(dat2)

shared_comp2 <- dat2[idx_list_filter2[[1]],]

# Step 5: Merging Sample info with shared df ===========================================
# HPLCTOFMS

hplctofms <- shared_comp2 %>%
  filter(., !is.na(Feature)) %>%
  select('File', 'Suspected_Polymer', 'Category', 'Subcategory', 'Feature', 'm.z', 'RT', 'Height')

colnames(hplctofms)[ncol(hplctofms)] <- 'Values'

```

### STEP 5: Prepping df for Multivariate analyses, Wilcoxon tests and ML

(Update 20 Feb 2024:) If MVs occur when the feature only found in one analytical technique and not in the other -> then impute these missing values with zero

==> for the case of zero imputation, nothing changein the PCA biplot actually

** for the case of other imputation techniques (like minimum, LOD or iterative): need to apply the alternative replacement mentioned above.

(Update 22 Feb 2024:) 
- For combination of analytical technique, for each technique, Remove features (of that technique, for ex Compound_10.ATDGCMS) that contains more than 90% NAs
- FOr ICP data, there is NO missing data what soever so no need to impute MVs.


```{r , echo=FALSE, warning = FALSE, message=FALSE}

df_pca <- hplctofms %>%
  dplyr::select(File,
                Subcategory,
                Feature,
                technique,
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Subcategory,
    Feature,
    technique) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var="File") %>%
  relocate(Subcategory, .after = 1)

# APPLY for ALL data combinations: IF conditioning, which feature column appear consistently (populate 90 % of that one product type`), KEEP IT!!!!!
col_id <- c()
for (i in 3:ncol(df_pca)) {
  temp <- df_pca[, c(1, 2, i)]
  temp[which(temp$technique != "HPLC" & is.na(temp[, 3])), 3] <- 0
  ele_count <- c()
  for (subcat in unique(temp[which(temp$technique == "HPLC"),]$Subcategory)) {
    count <- sum(!is.na(temp[which(temp$Subcategory == subcat & 
                                     temp[,3] != 0), 3]))/length(temp[which(temp$Subcategory == subcat & 
                                                                              temp$technique == "HPLC"), 3])
    ele_count <- c(ele_count, count)
  }
  # Is there any compounds appear in > 90% of the time in a product?
  if (any(ele_count > 0.9)) {
    col_id <- c(col_id, i)
  } else {
    next
  }
  
  
}

#**!!! ONLY For combination of analytical data: If MVs occur when the feature only found in one analytical technique and not in the other -> then impute these missing values with zero
colid_90_na <- c()
for (i in 3:ncol(df_pca)) {
 
    }  } else if (str_detect(colnames(df_pca)[i], "HPLC")) {
    df_pca[which(df_pca$technique != "HPLC" & is.na(df_pca[, i])), i] <- 0
    # For combination of analytical technique, for each technique, Remove features (of that technique, for ex Compound_x.ATDGCMS) that contains more than 90% NAs 
    if (length(df_pca[which(df_pca$technique == "HPLC" & !is.na(df_pca[, i])), i])/length(df_pca[which(df_pca$technique == "HPLC"), i]) < 0.1) {
      colid_90_na <- c(colid_90_na, i)
    }
  
  }
}

# Add back the compounds appear 90% of the time in one product type & remove any duplicate column
# Now here we have df with Feature appear in 90% of a product type 
test <- df_pca[, col_id]
# and here we remove feature that has more than 90% NA for an analytical technique
if (!is.null(colid_90_na)) {
  test1 <- df_pca[, -colid_90_na]
} else {
  test1 <- df_pca
}

# Merge the two filtered df above to grand df
test2 <- df_pca %>% 
  select(., base::union(colnames(test), colnames(test1))) %>% 
  relocate(Subcategory, technique, .before = 1)

```

### STEP 6: Missing value imputation

#### Option 1: Zero
```{r , echo=FALSE, warning = FALSE, message=FALSE}
for (r in 1:nrow(test2)) {
  test2[r, which(base::is.na(test2[r,]))] <- 0 
}

```

#### Option 2: Minimum 

(Update 21 Feb 2024:) For analytical data combination, need to fill in the values accordingly to the technique, for ex, GC samples will be imputed with min of GC data only, and do same for HPLC or ICP data separatedly. 
***For ICP data, since the min value is also zero, we keeep it as it is!

(Update 21 Feb 2024:) For combinations of analytical technique, need to normalize data separatedly for sample of each technique. Can do this by pivot_longer, using for loop, then pivot_wider again.

```{r , echo=FALSE, warning = FALSE, message=FALSE}
for (r in 1:nrow(test2)) {
  test2[r, which(base::is.na(test2[r,]))] <- min(test2, na.rm = TRUE)
} 
```

#### Option 3: LOD imputation
```{r , echo=FALSE, warning = FALSE, message=FALSE}

# HPLCTOFMS RELATED datasets ----------
# A. Replace missing values in compounds that don't appear in Blank at all.

  for (i in 1:length(lod1)) {
    # If the feature was removed during removal of >90% NAs, then next
    if (sum(colnames(test2) == names(lod1[i])) == 0) {
      next
    }
    # In case the feature occur in blanks and have no missing values, then next
    else if (identical(which(is.na(test2[, names(lod1[i])])), integer(0))) {
      next
    } else {
      list_new[[j]][which(is.na(list_new[[j]][,names(hplctofms_col1[i])])), names(hplctofms_col1[i])] <- hplctofms_col1[[i]]
    }
  }
}

# B. Replace missing values in compounds that appear in >= 2 Blanks.
for (j in 1:length(list_new)) {
  for (i in 1:length(hplctofms_col2)) {
    # If the feature was removed during removal of >90% NAs, then next
    if (sum(colnames(list_new[[j]]) == names(hplctofms_col2[i])) == 0) {
      next
    }
    # In case the feature have no missing values, then next
    else if (identical(which(is.na(list_new[[j]][, names(hplctofms_col2[i])])), integer(0))) {
      next
    } else {
      list_new[[j]][which(is.na(list_new[[j]][, names(hplctofms_col2[i])])), names(hplctofms_col2[i])] <- hplctofms_col2[[i]]
    }
  }
}
```

#### Option 4: Iterative imputation
```{r , echo=FALSE, warning = FALSE, message=FALSE}
library(missForest)
impute_list <- list()
i <- 1
for (df in list(test2_gc, test2_hplc, test2_gchplc, test2_gcicp, test2_hplcicp, test2_gchplcicp)) {
  subset <- df[, 3:ncol(df)]
  colnames(subset) <- paste0("x", 1:ncol(subset))
  impute_list[[i]] <- missForest(subset)
  i <- i + 1
}

```

### STEP 7: Data normalization

(Update 19 Feb 2024:) Here we do TSN normalization just right before clustering analyses, Wilcoxon tests and ML models.

(Update 20 Feb 2024:) I successfully implement the LODimpute to create PCA biplot, the HPLC data only showed the same clustering pattern as Zero and Minimum imputation approach.


** # If there is any sample aka. row that only have zeros (for zero imputation), then remove that sample/row from the analysis

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# For zero imputation ONLY !!: If there is any sample aka. row that only have zeros (for zero imputation), then remove that sample/row from the analysis

# for (j in 1:length(list_new)) {
  row_to_remove <- c()
  for (i in 1:nrow(test2)) { # list_new[[j]]
    if (rowSums(test2[i, 3:ncol(test2)]) == 0) { # list_new[[j]]
      row_to_remove <- c(row_to_remove, i)
    }
  }
  
  if (!is.null(row_to_remove)) {
    test2 <- test2[-row_to_remove,] # list_new[[j]]
  }
# }
```

(Update 15 March 2024:) This part of the code below "apply(impute_list[[j]][, 3:ncol(list_new[[j]])]" was wrong!!, it is now fixed like this "apply(impute_list[[i]][, 3:ncol(list_new[[i]])]"
==> this means that we have to export all the subcategory dataframe from 7 ML techniques and run all Wilcoxon test Subcategory again.

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# df_percentage_list <- list()
# for (i in 1:length(test2)) {
  # df_percentage_list[[i]]
  df_percentage <- as.data.frame(t(apply(test2[, 2:ncol(test2)],  #impute_list[[i]]$ximp, 
                                         MARGIN = 1, 
                                         function(row) {row/sum(row, na.rm = TRUE)}))) 
  print(sum(is.na(df_percentage)))
# }

```

### Export test2 df for ML models
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# names <- c("test2_gc", "test2_hplc", "test2_gchplc", "test2_gcicp", "test2_hplcicp", "test2_gchplcicp")
# for (i in 1:length(df_percentage_list)) { 
  writexl::write_xlsx(df_percentage %>% # df_percentage_list[[i]]
                        mutate(Subcategory = test2$Subcategory) %>%
                        # mutate(train_test = test2$train_test) %>%
                        relocate(Subcategory,
                                 # train_test,
                                 .before = 1),
                      path = paste0("C:/Users/Emerging contaminant/Desktop/Huy-Plastic-Fingerprint/Microplastic-Fingerprinting/Python/MLdata_USEonly/",
                                    "gc_hplc_icp_min",
                                    ".xlsx"))
# }

df_percentage  <- df_percentage %>% 
  mutate(Subcategory = test2$Subcategory) %>%
  mutate(technique = test2$technique) %>%
  relocate(Subcategory, 
           technique,
           .before = 1)

# For ICP data byitself
colnames(df_percentage) <- paste0("col_", 1:ncol(df_percentage))
```

### ANOVA
(Update 11 April 2024):
- GC (with USE only) - without feature removal - Imputation with Zeros = > not significant
Df    Sum Sq   Mean Sq              F value Pr(>F)
Subcategory 17 1.304e-29 7.672e-31    0.66  0.794
Residuals   14 1.627e-29 1.162e-30 

- HPLC (with USE only)- without feature reduction -> - Imputation with Zeros  => too many variables -> Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restart

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros => not significant
Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory 11 2.100e-30 1.913e-31   0.471  0.916
Residuals   84 3.411e-29 4.061e-31 

- ICP (with USE only) - without feature reduction -> not significant
 Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory 11 3.567e-30 3.243e-31    1.34  0.266
Residuals   23 5.567e-30 2.420e-31 

- GC + HPLC(with USE only) - without feature reduction -> imputation with Zeros => too many variables ==> Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restart

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros =>  Significant!!
Df    Sum Sq   Mean Sq F value   Pr(>F)    
Subcategory  18 1.404e-28 7.798e-30   2.745 0.000658 ***
Residuals   109 3.097e-28 2.841e-30                     

Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

- GC + ICP (with USE only) 
*-** without feature reduction - imputation with Zeros => Significant!!
            Df    Sum Sq   Mean Sq F value Pr(>F)  
Subcategory 18 3.097e-30 1.721e-31   2.045  0.025 *
Residuals   48 4.039e-30 8.415e-32                 

Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

*-** WITH feature reduction - imputation with Zeros => NO SIGNIFICANT!
Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory 18 8.440e-31 4.689e-32    0.72  0.774
Residuals   48 3.125e-30 6.510e-32 

- HPLC + ICP (with USE only) 
*-** without feature reduction - imputation with Zeros => too many variables => Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restar

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros =>  NOT Significant
Df    Sum Sq   Mean Sq F value Pr(>F)
Subcategory  11 1.949e-29 1.772e-30   1.587  0.111
Residuals   119 1.329e-28 1.117e-30 

- GC+ HPLC + ICP (with USE only) 
*-** without feature reduction - imputation with Zeros => too many variables => Error: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error during wrapup: evaluation nested too deeply: infinite recursion / options(expressions=)?
Error: no more error handlers available (recursive errors?); invoking 'abort' restar

==> have to reduce feature 
==> After feature reduction => - Imputation with Zeros => SIGNIFICANT !!
Df    Sum Sq   Mean Sq F value   Pr(>F)    
Subcategory  18 4.557e-28 2.532e-29   3.804 2.97e-06 ***
Residuals   144 9.584e-28 6.656e-30                     

Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


```{r , echo=FALSE, warning = FALSE, message=FALSE}
summary(aov(as.formula(paste(paste(setdiff(names(df_percentage), "Subcategory"), collapse = "+"), " ~ Subcategory")), data = df_percentage))
```

## Import Subcategory data for PCA/stats/ML
```{r, echo=FALSE, warning = FALSE, message=FALSE}
# Import newly curated data \\  
file_list <- list.files(path = "C:/Users/Emerging contaminant/Desktop/Huy-Plastic-Fingerprint/Microplastic-Fingerprinting/data/MLdata_Subcategory", full.names = TRUE)
icp_subcat <- readxl::read_xlsx(path = "C:/Users/Emerging contaminant/Desktop/Huy-Plastic-Fingerprint/Microplastic-Fingerprinting/data/MLdata_Subcategory/icp_df_all.xlsx")
gc_subcat <- list()
hplc_subcat <- list()
gc_icp_subcat <- list()
gc_hplc_subcat <- list()
hplc_icp_subcat <- list()
gc_hplc_icp_subcat <- list()

for (file in file_list) {
  if (grepl("gc_hplc_icp_", file)) {

    gc_hplc_icp_subcat[[file]] <- readxl::read_excel(file)
  } else if (grepl("gc_hplc_", file)) {

    gc_hplc_subcat[[file]] <- readxl::read_excel(file)
  } else if (grepl("gc_icp_", file)) {
 
    gc_icp_subcat[[file]] <- readxl::read_excel(file)
  } else if (grepl("hplc_icp_", file)) {

    hplc_icp_subcat[[file]] <- readxl::read_excel(file)
  } else if (grepl("hplc_", file)) {

    hplc_subcat[[file]] <- readxl::read_excel(file)
  } else if (grepl("gc_", file)) {
    gc_subcat[[file]] <- readxl::read_excel(file)
  }
}
```

## Import Category data for PCA/stats/ML
```{r, echo=FALSE, warning = FALSE, message=FALSE}
# Import newly curated data \\  
file_list <- list.files(path = paste0(getwd(), "/ML_statsdata_Category"), full.names = TRUE)

gc_cat <- list()
hplc_cat <- list()
icp_cat <- list()
gc_icp_cat <- list()
gc_hplc_cat <- list()
hplc_icp_cat <- list()
gc_hplc_icp_cat <- list()

for (file in file_list) {
  if (grepl("gc_hplc_icp_cat", file)) {
    gc_hplc_icp_cat[[file]] <- readxl::read_excel(file)
  } else if (grepl("gc_hplc_cat", file)) {
    gc_hplc_cat[[file]] <- readxl::read_excel(file)
  } else if (grepl("gc_icp_cat", file)) {
    gc_icp_cat[[file]] <- readxl::read_excel(file)
  } else if (grepl("hplc_icp_cat", file)) {
    hplc_icp_cat[[file]] <- readxl::read_excel(file)
  } else if (grepl("gc_cat", file)) {
    gc_cat[[file]] <- readxl::read_excel(file)
  } else if (grepl("hplc_cat", file)) {
    hplc_cat[[file]] <- readxl::read_excel(file)
  } else {
    icp_cat[[file]] <- readxl::read_excel(file)
  }
}
```

# Exploratory data analysis

### Boxplot of plastic products
```{r , echo=FALSE, warning = FALSE, message=FALSE}
plotdat <- hplc_icp_subcat[[1]] %>%
  pivot_longer(-c(Subcategory, technique), names_to = "Compounds", values_to = "Values")
ggplot(data = plotdat) + 
  stat_boxplot(aes(x = Subcategory, 
                   y = Values)) +
  labs(x = "Plastic products", 
       y = "Normalized peak area") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

### Boxplot of Top 10 compounds with highest Concen. in each sample?
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# For each row, Extract Top 10 compound with highest concen.
top <- c()
dat <- hplc_subcat[[3]][,-2] %>%
# For each product type, calculate average of all compounds
  group_by(Subcategory) %>%
  summarise_all(.funs = mean, na.rm = TRUE)

# Select top 10 compound with highest abundance in each plastic product
top <- list()
for (subcat in unique(dat$Subcategory)) {
  top[[subcat]] <- colnames(dat)[order(as.numeric(dat %>% filter(., Subcategory %in% subcat)), decreasing = TRUE)[1:10]]
}

dat2 <- dat %>% pivot_longer(cols = 2:ncol(.), names_to = "Feature", values_to = "Values")

top_list <- list()
i <- 1
for (subcat in unique(dat2$Subcategory)) {
  top_list[[i]] <- dat2 %>% filter(., Subcategory %in% subcat) %>% filter(., Feature %in% top[[i]])
  i <- i + 1
}

plotdat <- bind_rows(top_list)

ggplot(data = plotdat) +
  geom_boxplot(aes(x = Subcategory, y = Values, fill = Feature)) +
  # geom_errorbar(data = data_summary, aes(x = NewFile, ymin = mean - sd, 
  #                                        ymax = mean + sd, group = Metal_component), 
  #               position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "", x = "Plastic product types", y = "Normalized Abundance (%)",
       fill = "Top features") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### 1. What are the Top metal with highest Concen. in each sample?

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
newmetal <- icp_list[[1]] %>% 
  tidyr::pivot_wider(names_from = Feature, values_from = Values) %>% 
  column_to_rownames(., var = "File")

# Replace all NA in data frame with 0
newmetal[is.na(newmetal)] <- 0

# For each sample, Extract Top 10 metal with highest concen.
top_metal <- list()
for (row in 1:nrow(newmetal)) {
  top_metal[[row]] <- colnames(newmetal)[order(as.numeric(newmetal[row,]), decreasing = TRUE)[1:10]]
}

topmetal_list <- list()
i <- 1
for (file in unique(icp_list[[1]]$File)) {
  topmetal_list[[i]] <- icp_list[[1]] %>% filter(., File %in% file) %>% filter(., Feature %in% top_metal[[i]])
  i <- i + 1
}

top_metal_plot_df <- bind_rows(topmetal_list) %>%
   mutate(Category = ifelse(str_detect(File, "USE-01"), "Polystyrene food packaging", 
                                  ifelse(str_detect(File, "USE-02"), "Mixed plastic waste",
                                         ifelse(str_detect(File, "USE-03"), "Plastic drinking straws", 
                                                ifelse(str_detect(File, "USE-05"), "Cigarettes tips", 
                                                       ifelse(str_detect(File, "USE-06"), "Face masks", 
                                                              ifelse(str_detect(File, "USE-07"), "Food containers", 
                                                                     ifelse(str_detect(File, "USE-09"), "Food wrappers",
                                                                            ifelse(str_detect(File, "USE-11"), "Plastic toy balls", 
                                                                                   ifelse(str_detect(File, "USE-12"), "Ziploc bags", 
                                                                                          ifelse(str_detect(File, "USE-13"), "Food packaging",
                                                                                                 ifelse(str_detect(File, "USE-14"), "Bottle caps", "Fishing bait trays"))))))))))))
                                                              
### Stacked bar plot of values of  Top 10 trace metal for each metal data sample (x-axis: sample name; y-axis: metal values) --------------------------------------

# data_summary <- top10_plot %>%
#   group_by(NewFile, Metal_component) %>%
#   summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = top_metal_plot_df, aes(x = Category, y = Values, fill = Feature)) +
  geom_bar(stat = "identity") +
  # geom_errorbar(data = data_summary, aes(x = NewFile, ymin = mean - sd, 
  #                                        ymax = mean + sd, group = Metal_component), 
  #               position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "ICP-MS Trace metal data", x = "Plastic product types", y = "Normalized Abundance (%)",
       fill = "Top trace metals") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### 2.1 Among the top trace metal, if the metal is shared among different samples, is there a significant difference in the concentration of these trace metals?

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}

```

#### 2.2 Among the top trace metal, what are the metal that uniquely occur in just 1 sample?

```{r class.source = 'fold-hide', echo = FALSE, message = FALSE, warning = FALSE}
```

#### For each plastic type, What are compounds that only appear in USSB and not USE?

```{r, echo=FALSE, warning = FALSE}
# compdiff <- list()
for (plt in unique(adjusted_df$plastic_type)) {
  # compdiff[[plt]] <- 
  print(plt)
  print(base::setdiff(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

#### For each plastic product, What are the compounds that consistently appear in both USSB & USE?

```{r, echo=FALSE, warning = FALSE}
for (plt in unique(adjusted_df$plastic_type)) {
  print(plt)
  print(base::intersect(adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USSB")),]$collapsed_compound, 
                adjusted_df[which(adjusted_df$plastic_type == plt &
                                    str_detect(adjusted_df$File, "USE")),]$collapsed_compound))
}
```

### What are the top compounds that have highest abundance in each plastic "Subcategory" and what are their identity (matched with EF suspect screening)?

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# Extract top compounds for each sample from HPLC only data ----------------------------------------------------------
hplc <- hplc_subcat[[3]] %>% 
  pivot_longer(-c("Subcategory", "technique"), names_to = "Feature", values_to = "Values") %>%
  dplyr::select(Subcategory, Feature, Values) %>%
  dplyr::group_by(Subcategory, Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from = Feature, 
                     values_from = Values) 

# For each sample, Extract Top x compounds with highest concen.
top_hplc <- list()
for (row in 1:nrow(hplc)) {
  # go through each subcategory and Extract Top x compounds with highest concen.
  top_hplc[[hplc[row,]$Subcategory]] <- colnames(hplc)[order(as.numeric(hplc[row,]), decreasing = TRUE)[1:10]]
}

dat2 <- dat %>% pivot_longer(cols = 2:ncol(.), names_to = "Feature", values_to = "Values")

top_list <- list()
i <- 1
for (subcat in unique(dat2$Subcategory)) {
  top_list[[i]] <- dat2 %>% filter(., Subcategory %in% subcat) %>% filter(., Feature %in% top_hplc[[i]])
  i <- i + 1
}

top_hplc_plot_df <- bind_rows(top_list)

# Matching all the top compounds with EF's Suspect Screening ---------------------------------------------------------
all_unique <- data.frame(comp=unique(top_hplc_plot_df$Feature))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Read in suspect screening result from Eric
sus_scr <- readxl::read_xlsx(path = paste0(getwd(), "/data/PMT Suspect Screening Plastics Results EF.xlsx"), skip = 1)

# Remove all legacy empty rows from Eric's Excel file 
sus_scr <- sus_scr[rowSums(is.na(sus_scr)) != (ncol(sus_scr) - 1), ]


# Find in all_unique the compounds that match with suspect hit list in all_unique with molecular ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp[1,] # here we just need 1 row of suspect name and corresponding Feature
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)

# Replace all the compounds in plot_df with suspect screening matches ------------------------------------------------
joined <- left_join(top_hplc_plot_df %>%
                      dplyr::select(Subcategory, Feature, Values), 
                    all_matching_sus_scr %>%
                      dplyr::select(Feature, `Suspect Name`), by = "Feature") %>%
  dplyr::filter(!is.na(`Suspect Name`)) %>%
  # group and calculate the mean of Features with same subcategory
  group_by(Feature, `Suspect Name`, Subcategory) %>%
  dplyr::summarise(across(Values, base::mean))


### Stacked bar plot of values of  Top 10 trace metal for each metal data sample (x-axis: sample name; y-axis: metal values) --------------------------------------

# data_summary <- top10_plot %>%
#   group_by(NewFile, Metal_component) %>%
#   summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = joined, aes(x = Subcategory, y = Values, fill = `Suspect Name`)) +
  geom_bar(stat = "identity") +
  # geom_errorbar(data = data_summary, aes(x = NewFile, ymin = mean - sd, 
  #                                        ymax = mean + sd, group = Metal_component), 
  #               position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "Non-targeted HPLC-qTOF-MS data", x = "Plastic product types", y = "Normalized Peak Abundance (%)",
       fill = "Top hit from HPLC PMT Suspect Screening") +
  theme_minimal(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Beyond-PMT Suspect Screening -------------------------------------------------
# Filter all the top HPLC compounds that "beyond" the PMT suspect screening list
beyond_pmt <- top_hplc_plot_df %>%
  filter(Feature %notin% unique(joined$Feature)) %>%
  select(Feature, RT, m.z) %>%
  group_by(Feature) %>%
  summarise(across(c(RT, m.z), base::mean))

# Export "beyond-PMT-Suspect Screening" list to excel
writexl::write_xlsx(x=beyond_pmt, path = "Top10_HPLC_data_beyond_pmt_suspect Screening.xlsx")
```

# Statistical fingerprinting

## PCA
```{r , echo=FALSE, warning = FALSE, message=FALSE}
# clr_transformed_pca <- compositions::clr(test[, -1])
data <- icp_subcat
res.pca <- FactoMineR::PCA(
  df_percentage,
  # impute_list[[4]][[1]],
  # test2[, -1],
  # clr_transformed_pca,
  scale.unit = FALSE,
  graph = FALSE)

# Scree plot
fviz_screeplot(res.pca, ncp=10)

# Biplot
factoextra::fviz_pca_biplot(res.pca,  
                            geom = c("point", "text"),
                            label = "none", 
                            invisible = "var", 
                            repel = TRUE,
                            labelsize = 10, 
                            habillage = factor(test2$Subcategory),
                            addEllipses = TRUE,
                            ellipse.level=0.95,
                            ggtheme = ggplot2::theme_minimal(base_size = 20),
                            title = "Plastic Sample"
                            ) + 
  # if has error "Too few points to calculate an ellipse"
  # ggforce::geom_mark_ellipse(aes(fill = Groups,
  #                                color = Groups),
  #                            label.buffer = unit(40, 'mm')) +
  theme(legend.position = 'bottom') + coord_equal()
```

### Barplot of Top 10 Variable contributions to first n dimensions

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# Variable contributions to first n dimensions
## To PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
## To PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)

# Extract top 10 contribution to PC1
dim1 <- as.data.frame(res.pca$var$contrib) %>% arrange(desc(Dim.1))
top10 <- rownames(dim1[1:10,])

# Plot bar plot
plotdat <- icp_subcat %>% 
  select(c(Subcategory, top10)) %>%
  pivot_longer(cols = 2:ncol(.), names_to = "Features", values_to = "Values")

data_summary <- plotdat %>%
  group_by(Subcategory, Features) %>%
  summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = data_summary, aes(x = Subcategory, y = mean, fill = Features)) +
  geom_bar(stat = "identity", position = 'dodge') +
  geom_errorbar(aes(ymin = mean - sd,
                    ymax = mean + sd),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Plastic product group", y = "Normalized Concentration") + 
  theme_classic(base_size = 30) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r , echo=FALSE, warning = FALSE, message=FALSE}
# 3D PCa plot ===============================================================================
# FactoMineR_p <- FactoMineR::PCA(df_pca,
#                                 scale.unit = FALSE)
# 
# scores <- as.data.frame(FactoMineR_p$ind$coord[,1:3]) %>%
#   rownames_to_column(., var = "File")
# category <- sapply(scores$File, function(f) {
#   unique(gc_hplc_list[[1]]$Category[match(f, gc_hplc_list[[1]]$File)])
# })
# 
# # Add to scores data frame
# scores$Category <- as.factor(category)
# 
# rgl::plot3d(scores$Dim.1, scores$Dim.2, scores$Dim.3,
#        col = as.integer(scores$Category),
#        xlab = "PC1",
#        ylab = "PC2",
#        zlab = "PC3",
#        type = "p")
# 
# # Add legend
# rgl::legend3d("topright", legend = levels(scores$Category), col = 1:length(levels(scores$Category)), pch = 16)

# factoextra::fviz_screeplot(FactoMineR_p)
# factoextra::fviz_pca_biplot(FactoMineR_p,
#                             habillage = unique(gc_hplc_list[[1]]$File))
```

## HCA

If we filter unique compounds by File (compounds appear in at least 2 files), then all File in df in gc_hplc_list are the same. That's why here we used gc_hplc_list[[1]]

This exercise does not seems to affect the HCa. Keep in mind that for all 3 cases of data combinations: none of them has compounds that occur in all samples. NONE of THEM!!!!!

### Clustering of samples

(Jan 11th 2024): 
\ICP only data shows misgrouping between the following Subcategories: 
- Cigarillo tip//Bottle caps
- Food wrapper// Food packaging// Food containers//plastic drinking straws//Various plastic waste

\GC+ICP data show clear separation between GC and ICP data into 2 clusters. From ICP cluster, the misgrouping between the following Subcategories is the same as the case of ICP only. From the GC cluster, the grouping is more or less the same as the case of GC only.

\HPLC+ICP data, the cluster of both ICP and HPLC side change a bit. However for ICP side, misgrouping between the following Subcategories still remains as the case of ICP only. For HPLC side, the organization of the sample into grouping change a bit as the plastic toy balls are separated into their own top cluster (same height level with ICP and rest of HPLC branches) 

\GC+HPLC+ICP data, for ICP side, misgrouping between the following Subcategories still remains as the case of ICP only. For GC + HPLC side, it looks almost exactly like the case of GC+HPLC.

(UPdate 06 Feb 2024:) common metrics to evaluate performance of clustering models:
- Silhouette Score (https://stackoverflow.com/questions/33999224/silhouette-plot-in-r): Higher score => better clustering
- Calinski-Harabaz Index (https://search.r-project.org/CRAN/refmans/fpc/html/calinhara.html): Higher score => better clustering
- Davies-Bouldin Index ():

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Data prep for HCA ---------------------
# hc_df <- gc_subcat[[3]] %>%
#   dplyr::select(File,
#                 Subcategory, # !! Subcategory to easier visualize the samples characteristic in HCA plot
#                 Feature, 
#                 Values) %>%
#   # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
#   dplyr::group_by(
#     File,
#     Subcategory,
#     Feature) %>%
#   dplyr::summarise(across(Values, base::mean)) %>%
#   tidyr::pivot_wider(names_from =  Feature,
#                      values_from = Values)
# 
# names <- hc_df$Subcategory
# 
# hc_df <- as.matrix(hc_df[, -c(1,2)])
# 
# for (r in 1:nrow(hc_df)) {
#   hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
# }
# 
# rownames(hc_df) <- names

hc_df <- icp_subcat %>% select(-c(Subcategory))


## Dissimilarity Indices Calculated by vegan::vegdist()
hca_samp <- stats::hclust(vegan::vegdist(hc_df,
                                         method = "robust.aitchison")) # Since our data is continous -> canberra // manhattan // aitchison // robust.aitchison

plot(hca_samp,
     labels = icp_subcat$Subcategory,
     hang = -1,
     main = "")
```


A smaller Aitchison distance between two samples would indicate that their compositional structures are similar in terms of the ratios between components.

For selecting Dissimilarity Indices Calculated by vegan::vegdist(), since our data is continous, not normally distributed, and the data is sparse (meaning our data has lots of NA or NULL before data imputation, This basically because there are compounds that occur in some samples and not in the others). Manhattan dissimilarity index separating ATDGCMS and HPLCTOFMS into 2 big clusters, while Canberra//Aitchison//Robust.aitchison grouped ATDGCMS as a subgroup of a part of HPLCTOFMS data.

Since for each observations of each sample, the sum of values of collapsed compounds is sum up to 100%, Aitchison or Robust.Aitchison are currently the best option for dissimilarity index.

***Why Aitchison or Robust.Aitchison for OUR DATA, AKA. Compositional Data?***

Compositional data have particular characteristics and constraints that standard distance metrics like Euclidean don't handle well. Here are some reasons why Aitchison distance is more suitable:

**Closure Constraint:** Because the components in compositional data sum to a constant value (like 1 or 100%), the data is subject to a closure constraint. This constraint violates the assumption of independence between variables that many standard statistical techniques rely on. Aitchison distance accounts for this constraint.

*Implications of Closure Constraint* *Loss of Degrees of Freedom:* Because of the closure constraint, one variable is effectively 'dependent' on the others. For instance, in our example, knowing the proportions of Additive A and Additive B is enough to determine the proportion of Additive C.

*Spurious Correlations:* The closure constraint can lead to spurious or misleading correlations between variables. For example, an increase in the proportion of Additive A will necessarily result in a decrease in the sum of the proportions of Additives B and C, even if there's no biological or chemical reason for them to be inversely related.

*Non-Independence of Components:* In compositional data, the components are not independent of each other, violating the assumptions of independence that many statistical tests and measures (like the Euclidean distance) rely on.

*Comparisons are Relative:* In compositional data, what matters is the relative sizes of the components, not their absolute sizes. Any analysis should, therefore, focus on these relative proportions rather than absolute values.

For these reasons, specialized statistical methods like Aitchison distance are often used for compositional data. These methods take the closure constraint into account and focus on the relative proportions of the different components, providing a more accurate and interpretable analysis.

In this workflow on tracking microplastic additives in aquatic environments, understanding the closure constraint can be pivotal. If you're looking at the relative abundances of different types of plastic additives in water samples, those abundances are compositional data subject to the closure constraint. Using standard statistical techniques without accounting for this constraint could lead to misleading conclusions.

**Relative Importance:** In compositional data, what's generally important is the relative proportion between components, rather than their absolute values. The Aitchison distance is designed to capture the relationships between these relative values effectively.

**Log-Ratio Transformation:** The Aitchison distance is based on log-ratio transformations, which have been proven to be an effective way to analyze compositional data. The log-ratio removes the closure constraint, making the data easier to analyze statistically.

**Handling Zeros:** The robust variant of Aitchison distance often includes techniques for handling zeros in compositional data, which are common and challenging to deal with.

**Scale-Invariance:** Aitchison distance is scale-invariant, meaning that multiplying a component by a constant will not change the distance. This is crucial for compositional data, where only the relative relationships between components matter.

**Interpretable:** The distance measure reflects compositional difference in a way that's meaningful within the context of the data, making it easier to understand and interpret the results.



So with the dimensional reduction of compounds from n = 13046 to 33, we still achieve a somewhat good clustering result in which sample of the same category tends to cluster together.


## K-means Clustering

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# Input numeric matrix data : sample as rows, collapsed_compounds as columns
df <- gc_hplc_list[[1]] %>%
  dplyr::select(NewFile,
                collapsed_compound, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    NewFile,
    collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  collapsed_compound,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "NewFile")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- stats::runif(length(which(base::is.na(df[r,]))),
                                                          min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
                                                          max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

tot.withinss <- list()
for (n_clus in 1:nrow(df)) {
  set.seed(round(runif(2:20, 0, 999)))
  km.out <- stats::kmeans(df, n_clus, nstart = 50)
  tot.withinss[[n_clus]] <- km.out$tot.withinss
  print(paste0("The number of centers is ", n_clus, " and total within-cluster sum of square is ", km.out$tot.withinss))
}

plot(df, col = (stats::kmeans(df, 171, nstart = 50)$cluster + 1) ,
     main = "K- Means Clustering Results with K = 2",
     xlab = "", ylab = "", pch = 20, cex = 2)


```

## t-SNE
```{r, echo=FALSE, warning = FALSE, message=FALSE}
# REFERENCES VISUALIZATION: 
# https://plotly.com/r/t-sne-and-umap-projections/
# https://distill.pub/2016/misread-tsne/

library(tsne)
library(plotly)

dat <- df_percentage # icp_subcat[[3]]
# remove any Subcategory that have less than 3 sample
dat <- dat[!(dat$Subcategory %in% names(table(dat$Subcategory)[table(dat$Subcategory) < 3])),]
features <- subset(dat, select = -c(Subcategory, technique))

tsne <- tsne(features,
             initial_dims = 3, 
             k = 3, 
             perplexity = 20, # Hyperparameter: perplexity (optimal number of neighbors) < number of samples
             max_iter = 5000
             )

pdb <- cbind(data.frame(tsne), dat$Subcategory)
options(warn = -1)
tsne_plot <- plot_ly(data = pdb ,x =  ~X1, y = ~X2, z = ~X3, 
               color = ~dat$Subcategory) %>% 
  add_markers(size = 8) %>%
  layout( 
    xaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'), 
    yaxis = list(
      zerolinecolor = "#ffff",
      zerolinewidth = 2,
      gridcolor='#ffff'),
    scene =list(bgcolor = "#e5ecf6"))

tsne_plot
```

## UMAP clustering 

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(umap)

dat <- df_percentage # gc_subcat[[3]]
# removgc_e any Subcategory that have less than 3 sample
dat <- dat[!(dat$Subcategory %in% names(table(dat$Subcategory)[table(dat$Subcategory) < 3])),]

features <- subset(dat, select = -c(Subcategory,
                                    technique
                                    ))

umap <- umap(features, n_components = 3,
             method = 'naive', metric= "manhattan",
             alpha = 0.0001, gamma = 0.0001)

layout <- cbind(data.frame(umap[["layout"]]), dat$Subcategory)
umap_plot <- plot_ly(layout, x = ~X1, y = ~X2, z = ~X3, 
                color = ~dat$Subcategory) %>% 
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'x-axis'), 
                                   yaxis = list(title = 'y-axis'), 
                                   zaxis = list(title = 'z-axis'))) 
umap_plot
```

## Muli-way ANOVA

(Update 25 Mar 2024): The multiway ANOVA has a problem => Error in if (ssr < 1e-10 * mss) warning("ANOVA F-tests on an essentially perfect fit are unreliable") : 
  missing value where TRUE/FALSE needed
  
``**This warning suggests that the model might be fitting the data too well, to the extent that it's potentially overfitting or encountering numerical precision issues. This can happen when the model fits the data so perfectly that it's essentially a perfect fit, which can lead to unreliable statistical tests.

Then, I tried implement ridge or lasso regression with glmnet() but we have the error:
Error in UseMethod("anova") : 
  no applicable method for 'anova' applied to an object of class "c('elnet', 'glmnet')"
  
So in the end, I gotta find another method outside of ANOVA to statistically test multiple variables. ==> Tried permutation test

```{r, echo=FALSE, warning = FALSE, message=FALSE}
dat <- gc_subcat[[3]]
# remove any Subcategory that have less than 3 sample
dat <- dat[!(dat$Subcategory %in% names(table(dat$Subcategory)[table(dat$Subcategory) < 3])),] %>%
  mutate(Subcategory = factor(Subcategory)) %>%
  select(-c(technique))

testlm <- lm(formula_string, data = dat)
anova(testlm)

explanatory_vars <- colnames(dat)[!colnames(dat) %in% "Subcategory"]

# Create the formula string
formula_string <- paste("Subcategory ~", paste(explanatory_vars, collapse = " + "))
```

## Wilcoxon test

(Update 25 Feb 2024:) Rerun all wilcoxon tests with new curated data of 6 combinations of analytical data after removal of features with >90% NAs and retain features that appears in 90% of a product type. Here we only look at Subcategory and not Category anymore.

### Wilcoxon shortcut functions
```{r, echo=FALSE, warning = FALSE, message=FALSE}
wilcox_category <- function(dflist, na_impute) {
  
  df_list <- dflist[grepl(paste0("_", na_impute), names(dflist))]
  sig_comp <- list()
  
  for (i in 1:length(df_list)) {
    dat <- df_list[[i]]
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(dat)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(unique(dat$Category), 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(unique(dat$Category), 2)[1,col]
        p_2 <- utils::combn(unique(dat$Category), 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(as.numeric(unlist(dat[which(dat$Category == p_1), c])), 
                                        as.numeric(unlist(dat[which(dat$Category == p_2), c])))$p.value
        # assigning row information
        cat_df[nrow(cat_df) + 1,] <- c(colnames(dat)[c], 
                                       paste0(p_1, " & ", p_2), 
                                       pval_wilcox_test)
        
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    sig_comp[[i]] <-  cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sig_comp[[i]])
    
    filename <- str_split(names(df_list)[i], pattern = "/", simplify = TRUE)
    writexl::write_xlsx(x = sig_comp[[i]], path = paste0("Wilcoxon_test_", format(Sys.Date()), "_", filename[ncol(filename)]))
  }
  return(sig_comp)
}  

wilcox_subcategory <- function(dflist) {
  sig_comp <- list()
  j <- 1
  # for (i in 1:length(dflist)) {
    df <- data.frame(Feature=character(), comparison_pair=character(), pval_wilcox_test=integer())
    dat <- dflist # [[i]]
    # Loop through each chemical feature columns
    for (c in 2:ncol(dat)) {
      # if the compound is GC then only include data point of GC sample in the Wilcoxon test, then subset the data to only include GC sample
      if (str_detect(colnames(dat)[c], "GC")) {
        tempdf <- dat[which(dat$technique == "GC"), c(1,c)]
      }
      # if the compound is HPLC then only include data point of HPLC sample in the Wilcoxon test
      else if (str_detect(colnames(dat)[c], "HPLC")) {
        tempdf <- dat[which(dat$technique == "HPLC"), c(1,c)]
      }
      # if the compound is ICP then only include data point of ICP sample in the Wilcoxon test
      else {
        tempdf <- dat[which(dat$technique == "ICP"), c(1,c)]
      }
      
      # remove any Subcategory that have less than 3 samples
      tempdf_filtered <- tempdf[!(tempdf$Subcategory %in% names(table(tempdf$Subcategory)[table(tempdf$Subcategory) < 3])),]
      
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(unique(tempdf_filtered$Subcategory), 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(unique(tempdf_filtered$Subcategory), 2)[1,col]
        p_2 <- utils::combn(unique(tempdf_filtered$Subcategory), 2)[2,col]
        
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(as.numeric(unlist(tempdf_filtered[which(tempdf_filtered$Subcategory == p_1), 2])), # 2 here is because it is the 2nd column of the subset dataframe
                                        as.numeric(unlist(tempdf_filtered[which(tempdf_filtered$Subcategory == p_2), 2])))$p.value
        # assigning row information
        df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                               paste0(p_1, " & ", p_2), 
                               pval_wilcox_test)
      }
    }
    
    # filename <- str_split(names(dflist)[i], pattern = "/", simplify = TRUE)
    
    pvaluecorrect <- c("holm", "hochberg", "hommel", "BH", "BY")
    for (m in pvaluecorrect) {
      df$adjusted_pvalue <- stats::p.adjust(df$pval_wilcox_test, method = m)
      # writexl::write_xlsx(x = df %>%
      #                       filter(., adjusted_pvalue < 0.05) %>%
      #                       arrange(adjusted_pvalue), 
      #                     path = paste0("Wilcoxon_test_Subcategory_", m, "_", format(Sys.Date()), "_", filename[ncol(filename)]))
      print(df %>%
              filter(., adjusted_pvalue < 0.05) %>%
              arrange(adjusted_pvalue))
      
      sig_comp[[j]] <- df
      j <- j + 1
    }
  # }
  
  return(sig_comp)
}
```

### Here, we use a non-parametric test, Wilcoxon test, because our data violates the assumptions for other parametric test, such as normality and equality of variance between 2 samples, as demonstrate below:

```{r , echo=FALSE, warning = FALSE}
### Examining for normal distribution ----------------
## Histogram 
# Create empty list 
histlist <- list()
# For loop creating histograms of each plastic type
i <- 1
for (plastic in unique(transpose_df$plastic_type)) {
  grouped_plastic <- which(transpose_df$plastic_type == plastic)
  histlist[[i]] <- ggplot() +
    geom_histogram(mapping = aes(as.vector(t(transpose_df[grouped_plastic, 3:ncol(transpose_df)])))) + 
    labs(x = plastic, title = plastic)
  i <- i + 1
}
# Appending each histogram to the empty list in order to view all of them
gridExtra::grid.arrange(grobs = histlist)

## QQ Plot 
# Create empty list 
qqlist <- list()
# For loop creating QQ plots of each plastic type
i <- 1 
# Appending QQ plots together 
par(mfrow=c(3,3))
for (product in unique(transpose_df$product_cat)) {
  grouped_product <- which(transpose_df$product_cat == product)
  qqlist[[i]] <- qqnorm(as.vector(t(transpose_df[grouped_product, 3:ncol(transpose_df)])), main = product, xlab = product, col = 'steelblue')
  qqline(as.vector(t(transpose_df[grouped_product, 3:ncol(transpose_df)])), col = 'red')
  i <- i + 1
}

### Conclusion = they are not normally distributed 

### Examining for equality of variance -----------------------------
# Levene’s test-non-normally distributed data (car::leveneTest()), significant if p-value < 0.05 
library(car)

results <- list()
for (col in 1:ncol(utils::combn(unique(transpose_df$product_cat), 2))) {
  # extract the combinations of plastic type pairs
  p_1 <- utils::combn(unique(transpose_df$product_cat), 2)[,col][1]
  p_2 <- utils::combn(unique(transpose_df$product_cat), 2)[,col][2]
  idx1 <- which(transpose_df$product_cat == p_1)
  idx2 <- which(transpose_df$product_cat == p_2)
  V1 <- as.vector(t(transpose_df[idx1, 3:ncol(transpose_df)]))
  V2 <- as.vector(t(transpose_df[idx2, 3:ncol(transpose_df)]))
  datat <- c(V1, V2)
  grouped <- as.factor(c(rep(p_1, times = length(V1)), rep(p_2, times = length(V2))))
  non_norm <- data.frame(datat, grouped)
  results[[paste0(p_1, p_2)]] <- car::leveneTest(datat ~ grouped, data = non_norm)
}
# -> p-value is 0.4096 which is greater the significance level of 0.05 therefore can conclude there is no significant difference between the variances


# Fligner-Killeen's test (fligner.test()), significant if p-value < 0.05 
fligner.test(datat ~ grouped, data = non_norm)
# p-value less than significance level therefore there is significant difference between variances.
# Test to determine the homogeneity of group variances. 
```

When we use combine gc_hplc data, 96.9556% of the observation in the resulting stats data frame are NA values. It would be bad to fill in this large amount of observations with LOD values, aka. leading to the inflation of small values in the data set.

one way to overcome this problem is group_by(Suspected_Polymer) or Category or Subcategory instead of by File =\> this is stupid because then for each comp in each product_cat only have 1 values ==\> it is meaningless to do Wilcoxon test comparing just 1 obs against 1 obs. ==\> we can go through each columns (compounds) and check which product_cat has more than x values and then only do the wilcoxon/ks test with those product_cat for that specific compound. IN this case, we don't need to fill our the missing values because the default for na.action of wilcox.test function is \*\*"na.omit", meaning all the NA values of the numerical vectors resulted from slicing into each column (compound) and each product_cat

(Update 25 Feb 2024:) Rerun all wilcoxon tests with new curated data of 6 combinations of analytical data after removal of features with >90% NAs and retain features that appears in 90% of a product type. Here we only look at Subcategory and not Category anymore.

### Wilcoxon shortcut functions
```{r, echo=FALSE, warning = FALSE, message=FALSE}
wilcox_category <- function(dflist, na_impute) {
  
  df_list <- dflist[grepl(paste0("_", na_impute), names(dflist))]
  sig_comp <- list()
  
  for (i in 1:length(df_list)) {
    dat <- df_list[[i]]
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(dat)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(unique(dat$Category), 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(unique(dat$Category), 2)[1,col]
        p_2 <- utils::combn(unique(dat$Category), 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(as.numeric(unlist(dat[which(dat$Category == p_1), c])), 
                                        as.numeric(unlist(dat[which(dat$Category == p_2), c])))$p.value
        # assigning row information
        cat_df[nrow(cat_df) + 1,] <- c(colnames(dat)[c], 
                                       paste0(p_1, " & ", p_2), 
                                       pval_wilcox_test)
        
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    sig_comp[[i]] <-  cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sig_comp[[i]])
    
    filename <- str_split(names(df_list)[i], pattern = "/", simplify = TRUE)
    writexl::write_xlsx(x = sig_comp[[i]], path = paste0("Wilcoxon_test_", format(Sys.Date()), "_", filename[ncol(filename)]))
  }
  return(sig_comp)
}  

wilcox_subcategory <- function(dflist) {
  sig_comp <- list()
  j <- 1
  for (i in 1:length(dflist)) {
    df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    dat <- dflist[[i]]
    # Loop through each chemical feature columns
    for (c in 3:ncol(dat)) {
      # if the compound is GC then only include data point of GC sample in the Wilcoxon test, then subset the data to only include GC sample
      if (str_detect(colnames(dat)[c], "GC")) {
        tempdf <- dat[which(dat$technique == "GC"), c(1,c)]
      }
      # if the compound is HPLC then only include data point of HPLC sample in the Wilcoxon test
      else if (str_detect(colnames(dat)[c], "HPLC")) {
        tempdf <- dat[which(dat$technique == "HPLC"), c(1,c)]
      }
      # if the compound is ICP then only include data point of ICP sample in the Wilcoxon test
      else {
        tempdf <- dat[which(dat$technique == "ICP"), c(1,c)]
      }
      
      # remove any Subcategory that have less than 3 sample
      tempdf_filtered <- tempdf[!(tempdf$Subcategory %in% names(table(tempdf$Subcategory)[table(tempdf$Subcategory) < 3])),]
      
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(unique(tempdf_filtered$Subcategory), 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(unique(tempdf_filtered$Subcategory), 2)[1,col]
        p_2 <- utils::combn(unique(tempdf_filtered$Subcategory), 2)[2,col]
        
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(as.numeric(unlist(tempdf_filtered[which(tempdf_filtered$Subcategory == p_1), 2])), 
                                        as.numeric(unlist(tempdf_filtered[which(tempdf_filtered$Subcategory == p_2), 2])))$p.value
        # assigning row information
        df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                               paste0(p_1, " & ", p_2), 
                               pval_wilcox_test)
      }
    }
    
    filename <- str_split(names(dflist)[i], pattern = "/", simplify = TRUE)
    
    pvaluecorrect <- c("holm", "hochberg", "hommel", "BH", "BY")
    for (m in pvaluecorrect) {
      df$adjusted_pvalue <- stats::p.adjust(df$pval_wilcox_test, method = m)
      writexl::write_xlsx(x = df %>%
                            filter(., adjusted_pvalue < 0.05) %>%
                            arrange(adjusted_pvalue), 
                          path = paste0("Wilcoxon_test_Subcategory_", m, "_", format(Sys.Date()), "_", filename[ncol(filename)]))
      print(df %>%
              filter(., adjusted_pvalue < 0.05) %>%
              arrange(adjusted_pvalue))
      
      sig_comp[[j]] <- df
      j <- j + 1
    }
  }
  
  return(sig_comp)
}
```

### Wilcoxon test with HCA result

#### With Combined GC HPLC - TBA (ZeroImputed_Jan 15 2024) 

Since for HCA, we operate on sample level and for each 4 subdf of each gc_hplc/gc/hplc_list, they all have the same sample column. Therefore, just using one of the subdf is enough.

Here, we want to find what is the optimal cut-off in HCA-cluster-tree 's Height and min_obs, which will result in the maximum number of significant compounds.

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_hplc_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(gc_hplc_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(gc_hplc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")

# Initialize variables
# sig_comp_CO160_171 <- list()
sig_comp_CO120_159 <- list()

# Main loop
for (ele in 120:159) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    # Loop through each collapsed_compound (as columns)
    
    for (c in 3:ncol(joined)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                        joined[which(joined$group == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                     paste0(p_1, "&", p_2), 
                                     pval_wilcox_test)
      }
    }
    
    
    sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
    
    sig_comp_CO120_159[[paste0("Cut height_", ele)]] <- sp_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sp_df %>%
            filter(., adjusted_pvalue_holm < 0.05) %>%
            arrange(adjusted_pvalue_holm))
  }
}

writexl::write_xlsx(x = sig_comp_CO120_159, path = "Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO120-159_zeroimputed_15Jan2024.xlsx")
```

###### Summary of significant compounds

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_Combined GC HPLC_by_Robust Aichitson_HCA clustering result.xlsx")

sheet_names <- excel_sheets(path = p)
sus_pol_combined <- lapply(sheet_names, function(sheet) read_excel(path = p, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list <- lapply(sus_pol_combined, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique <- unique(unlist(unique_values_list))

all_unique <- data.frame(comp=unique)

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_all_significant compounds.xlsx")
```


#### With Combined GC ICP - DONE (ZeroImputed_Jan 21 2024) 

(Update 21 Jan 2024:) Finally finished GC-ICP ZeroImputed, here from Cute_height [1] to Cut_height[13], we have cases of less than 2 satisfied_grp, so in the if conditioning, we jump out of the current for loop and go to the Cut_height[14] to Cut_height[97] (aka. the final Cut_height since Cut_Height[98], aka. top Cut_Height only have 1 group)

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_icp_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(gc_icp_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(gc_icp_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")

# Initialize variables
sig_comp_CO1_98 <- list()

# Main loop
for (ele in 1:98) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_grp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_grp <- c(satisfied_grp, gr)
    }
  }
  
  if (length(satisfied_grp) < 2) {
    next
  } else {
    # Loop through each collapsed_compound (as columns)
    
    for (c in 3:ncol(joined)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_grp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_grp, 2)[1,col]
        p_2 <- utils::combn(satisfied_grp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                        joined[which(joined$group == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                     paste0(p_1, "&", p_2), 
                                     pval_wilcox_test)
      }
    }
    
    
    sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
    
    sig_comp_CO1_98[[paste0("Cut height_", ele)]] <- sp_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sp_df %>%
            filter(., adjusted_pvalue_holm < 0.05) %>%
            arrange(adjusted_pvalue_holm))
  }
}

writexl::write_xlsx(x = sig_comp_CO1_98, path = "Wilcoxon_test_Combined GC-ICP_by_Robust Aichitson_HCA clustering result_CO14-97_zeroimputed_21Jan2024.xlsx")
```

#### With Combined HPLC ICP - TBA (ZeroImputed_Jan 21 2024) 

(Update 21 Jan 2024:) Start runnig HPLC-ICP HCA Wilcoxon tests

(Update 25 Jan 2024:) Fucking Microsoft update! I have run this wilcoxon test again, but now i am starting from CUT 20 so we don't waste time.

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- hplc_icp_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(hplc_icp_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(hplc_icp_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")

# Initialize variables
sig_comp_CO20_130 <- list()

# Main loop
for (ele in 20:130) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    # Loop through each collapsed_compound (as columns)
    
    for (c in 3:ncol(joined)) {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                        joined[which(joined$group == p_2), c])$p.value
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                     paste0(p_1, "&", p_2), 
                                     pval_wilcox_test)
      }
    }
    
    
    sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
    
    sig_comp_CO20_130[[paste0("Cut height_", ele)]] <- sp_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    
    print(sp_df %>%
            filter(., adjusted_pvalue_holm < 0.05) %>%
            arrange(adjusted_pvalue_holm))
  }
}

writexl::write_xlsx(x = sig_comp_CO20_130, path = "Wilcoxon_test_Combined HPLC-ICP_by_Robust Aichitson_HCA clustering result_CO20-130_zeroimputed_25Jan2024.xlsx")
```

#### With just GC - DONE (ZeroImputed_Jan 18 2024)
(Update 18 Jan 2024:) To maximise runtime, this will be run on Huy's Laptop

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- gc_list[[1]] %>%
  # filter(., File %notin% "ATDGCMS-Blank-repNA") %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(gc_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(gc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")


# Initialize variables
# sig_comp_CO56_75 <- list()
sig_comp_CO1_63<- list()

# Main loop
for (ele in 1:63) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_sp, 2)[1,col]
      p_2 <- utils::combn(satisfied_sp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp_CO1_63[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

# combine cut off dataframe
# sig_comp <- c(sig_comp_CO0_55, sig_comp_CO56_75)

writexl::write_xlsx(x = sig_comp_CO1_63, path = "Wilcoxon_test_GC_by_Robust Aichitson_HCA clustering result_CO1-63_zeroimputed_18Jan2024.xlsx")
```

###### Summary of significant compounds

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_by_Robust Aichitson_HCA clustering result.xlsx")

sheet_names <- excel_sheets(path = p)
sus_pol_combined <- lapply(sheet_names, function(sheet) read_excel(path = p, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list <- lapply(sus_pol_combined, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique <- unique(unlist(unique_values_list))

all_unique <- data.frame(comp=unique)

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_all_significant compounds.xlsx")
```

The code below run wilcoxcon test on NA imputed data frame and don't perform minimum observation method. It reports the number of significant compounds at each cutree Height.

After running the code with NA imputed data, it generates significant compounds (at best case for GCMS data =\> 31 significant compounds) as compared to Zero significant compounds if using Missing-Observation Method. Therefore, from now on, for Wilcoxon on HCA results, we impute both the HCA input and the Wilcoxon input in the same way.

#### With just HPLC - DONE (ZeroImputed_Jan 18 2024) 
(Update 18 Jan 2024:) To maximise runtime, this will be run on Huy's Laptop.

(Update 21 Jan 2024:) Finally finished HPLC ZeroImputed from 18 Jan 2024, here from Cute_height [1] to Cut_height[15], we have cases of less than 2 satisfied_grp, so in th eif conditioning, we jump out of the current for loop and go to the Cut_height[16] to Cut_height[94] (aka. the final Cut_height since Cut_Height[95], aka. top Cut_Height only have 1 group)

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- hplc_list[[1]] %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(hplc_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(hplc_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")


# Initialize variables
sig_comp_CO16_94 <- list()

# Main loop
for (ele in 1:95) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_grp <- c()
  for (gr in unique(joined$group)) {
    # if the group less than 2 members, then that group is excluded from wilcoxon test b/c Wilcoxon required at least 3 members for significant calculation
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_grp <- c(satisfied_grp, gr)
    }
  }
  # after filtering out groups that have less than 2 members, if we have less than 2 satisfied group, then jump to the next cutoff's height
  if (length(satisfied_grp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_grp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_grp, 2)[1,col]
      p_2 <- utils::combn(satisfied_grp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp_CO16_94[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

# writexl::write_xlsx(x = sig_comp_CO16_94, path = "Wilcoxon_test_HPLC_by_Robust Aichitson_HCA clustering result_CO1-79_zeroimputed_18Jan2024.xlsx")
```

#### With just ICP - DONE (ZeroImputed_Jan 18 2024) 
(Update 18 Jan 2024:) To maximise runtime, this will be run on Huy's Laptop

```{r, echo=FALSE, warning = FALSE, message=FALSE}
hc_df <- icp_list[[1]] %>%
  dplyr::select(File,
                Feature, 
                Values) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  dplyr::group_by(
    File,
    Feature) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  tidyr::pivot_wider(names_from =  Feature,
                     values_from = Values) %>%
  tibble::column_to_rownames(., var = "File")

for (r in 1:nrow(hc_df)) {
  hc_df[r, which(base::is.na(hc_df[r,]))] <- 0
  # stats::runif(length(which(base::is.na(hc_df[r,]))),
  #                                                         min = sort(icp_list[[1]]$Values)[1]/100000,
  #                                                         max = sort(icp_list[[1]]$Values)[2]/100000)
}

hca <- stats::hclust(vegan::vegdist(as.matrix(hc_df),
                                    method = "robust.aitchison"))

stats <- hc_df %>% tibble::rownames_to_column(., var = "File")


# Initialize variables
sig_comp_CO1_34<- list()

# Main loop
for (ele in 1:34) {
  t <- data.frame(group = cutree(hca, h = ceiling(hca$height[ele]))) %>% tibble::rownames_to_column(., var = "File")
  joined <- full_join(t, stats, by = "File")
  
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  satisfied_sp <- c()
  for (gr in unique(joined$group)) {
    # if the group only have 1 member, then that group is excluded from wilcoxon test
    if (sum(joined$group == gr) < 3) {
      next
    } else {
      satisfied_sp <- c(satisfied_sp, gr)
    }
  }
  
  if (length(satisfied_sp) < 2) {
    next
  } else {
    
  }
  # Loop through each collapsed_compound (as columns)
  
  for (c in 3:ncol(joined)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_sp, 2)[1,col]
      p_2 <- utils::combn(satisfied_sp, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(joined[which(joined$group == p_1), c], 
                                      joined[which(joined$group == p_2), c])$p.value
      # assigning row information
      sp_df[nrow(sp_df) + 1,] <- c(colnames(joined)[c], 
                                   paste0(p_1, "&", p_2), 
                                   pval_wilcox_test)
    }
  }
  
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  sig_comp_CO1_34[[paste0("Cut height_", ele)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm))
}

# combine cut off dataframe
# sig_comp <- c(sig_comp_CO0_55, sig_comp_CO56_75)

writexl::write_xlsx(x = sig_comp_CO1_34, path = "Wilcoxon_test_ICP_by_Robust Aichitson_HCA clustering result_CO1-34_zeroimputed_18Jan2024.xlsx")
```

### Wilcoxon test without HCA clustering precursor

#### By Category

##### Do we need to change Data processing-Step 4 (Subsetting Non-unique compounds), for ex At step 4, we need to subset compounds that appear in more than 2 categories instead of 2 files? =\> NO!!

##### COMBINED GC_HPLC

###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplc <- list()
for (i in 1:length(gc_hplc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_hplc_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gchplc[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_list[[i]]$Feature == sig_comp_gchplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp_gchplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp_gchplc, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplc <- list()

for (i in 1:length(gc_hplc_list)) {
  stats <- gc_hplc_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
    # stats::runif(length(which(base::is.na(stats[r,]))),
    #                                                         min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
    #                                                         max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gchplc[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gchplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_list[[i]]$Feature == sig_comp_gchplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gchplc, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Category_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Category_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_COMBINED GC_HPLC_by_Category_all_significant compounds.xlsx")
```

##### COMBINED GC_ICP
###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gcicp <- list()
for (i in 1:length(gc_icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gcicp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_gcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_icp_list[[i]]$Feature == sig_comp_gcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gcicp, path = "Wilcoxon_test_COMBINED GC_ICP_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gcicp <- list()

for (i in 1:length(gc_icp_list)) {
  stats <- gc_icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gcicp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_icp_list[[i]]$Feature == sig_comp_gcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gcicp, path = "Wilcoxon_test_COMBINED GC_ICP_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### COMBINED HPLC_ICP

(Task need to be done [12th Jan 2024]): For the GC // HPLC combo with ICP data, I need to rewrite the part of the code to match the RT and m.z with only GC // HPLC compounds in the original df lists, because ICP data does not have RT and m.z values.

###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplcicp <- list()
for (i in 1:length(hplc_icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- hplc_icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_hplcicp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_hplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_icp_list[[i]]$Feature == sig_comp_hplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplcicp, path = "Wilcoxon_test_COMBINED HPLC_ICP_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplcicp <- list()

for (i in 1:length(hplc_icp_list)) {
  stats <- hplc_icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_hplcicp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_hplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_icp_list[[i]]$Feature == sig_comp_hplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplcicp, path = "Wilcoxon_test_COMBINED HPLC_ICP_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### COMBINED GC_HPLC_ICP
###### with Missing-Observation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplcicp <- list()
for (i in 1:length(gc_hplc_icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_hplc_icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gchplcicp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_gchplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_icp_list[[i]]$Feature == sig_comp_gchplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gchplcicp, path = "Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gchplcicp <- list()

for (i in 1:length(gc_hplc_icp_list)) {
  stats <- gc_hplc_icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_hplc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gchplcicp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gchplcicp[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gchplcicp[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gchplcicp[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_hplc_icp_list[[i]]$Feature == sig_comp_gchplcicp[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_hplc_icp_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_hplc_icp_list[[i]][idx, ]$m.z))
    }

    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gchplcicp[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gchplcicp, path = "Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

##### ATDGCMS

###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gc <- list()
for (i in 1:length(gc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- gc_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_gc[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_list[[i]]$Feature == sig_comp_gc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_gc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
    
    print(sig_comp_gc[[paste0("Grouping_", i)]] %>% arrange(comp))
  }
}

writexl::write_xlsx(x = sig_comp_gc, path = "Wilcoxon_test_GC_data_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_gc <- list()

for (i in 1:length(gc_list)) {
  stats <- gc_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
      # stats::runif(length(which(base::is.na(stats[r,]))),
      #                                                       min = sort(gc_list[[i]]$Values)[1]/10000,
      #                                                       max = sort(gc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
     
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_gc[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_gc[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_gc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_gc[[paste0("Grouping_", i)]])) {
      idx <- base::which(gc_list[[i]]$Feature == sig_comp_gc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(gc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(gc_list[[i]][idx, ]$m.z))
    }

    sig_comp_gc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_gc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_gc, path = "Wilcoxon_test_GC_data_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Category_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Category_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_GC_data_by_Category_all_significant compounds.xlsx")
```

##### HPLCTOFMS

###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplc <- list()
for (i in 1:length(hplc_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- hplc_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_hplc[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_hplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_list[[i]]$Feature == sig_comp_hplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplc, path = "Wilcoxon_test_HPLC_data_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_hplc <- list()

for (i in 1:length(hplc_list)) {
  stats <- hplc_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
    # stats::runif(length(which(base::is.na(stats[r,]))),
    #                                                         min = sort(hplc_list[[i]]$Values)[1]/10000,
    #                                                         max = sort(hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
     
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_hplc[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_hplc[[paste0("Grouping_", i)]] %>% arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (dim(sig_comp_hplc[[paste0("Grouping_", i)]])[1] == 0) {
    next
  } else {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(sig_comp_hplc[[paste0("Grouping_", i)]])) {
      idx <- base::which(hplc_list[[i]]$Feature == sig_comp_hplc[[paste0("Grouping_", i)]][row,]$comp)
      mean_rt <- c(mean_rt, mean(hplc_list[[i]][idx, ]$RT))
      mean_mz <- c(mean_mz, mean(hplc_list[[i]][idx, ]$m.z))
    }
    
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_rt <- mean_rt
    sig_comp_hplc[[paste0("Grouping_", i)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = sig_comp_hplc, path = "Wilcoxon_test_HPLC_data_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Category_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Category_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_HPLC_data_by_Category_all_significant compounds.xlsx")
```

##### ICP
###### with Missing-Observation method - TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_icp <- list()
for (i in 1:length(icp_list)) {
  list_cat <- list()
  for (min_obs in 1:10) {
    stats <- icp_list[[i]] %>%
      select(File, Feature, Category, Values) %>%
      # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
      group_by(File, Category, Feature) %>%
      summarise(across(Values, mean)) %>%
      pivot_wider(names_from = Feature, values_from = Values) %>%
      column_to_rownames(., var = "File")
    
    cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
    # Loop through each collapsed_compound (as columns)
    for (c in 2:ncol(stats)) {
      satisfied_cat <- c()
      # Loop through each suspected_polymer
      for (cat in unique(stats$Category)) {
        # get all Category that has > min_obs
        if (sum(!is.na(stats[which(stats$Category == cat), c])) > min_obs) {
          satisfied_cat <- c(satisfied_cat, cat)
        } else {
          next
        }
      }
      if (length(satisfied_cat) < 2) {
        next
      } else {
        # looping through the combinations of product categories 
        for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
          # extract the combinations of product category pairs
          p_1 <- utils::combn(satisfied_cat, 2)[1,col]
          p_2 <- utils::combn(satisfied_cat, 2)[2,col]
          # calculating the p-value between each product category pair 
          pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                          stats[which(stats$Category == p_2), c])$p.value
          
          # assigning row information
          cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                         paste0(p_1, " & ", p_2), 
                                         pval_wilcox_test)
        }
      }
    }
    
    cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
    
    list_cat[[min_obs]] <- cat_df %>%
      filter(., adjusted_pvalue_holm < 0.05) %>%
      arrange(adjusted_pvalue_holm)
    # print(paste0("Category ", i, " unique sig is ", length(unique(list_cat[[min_obs]]$comp))))
  }
  
  # Select among all the min obs sub data frame to get the one that has highest number of significant compounds (assuming the one with highest number of significant compounds has nested all the subdf that has less number of compounds)
  sig_comp_icp[[paste0("Grouping_", i)]] <- list_cat[[which.max(lapply(list_cat, nrow))]]
  
  print(sig_comp_icp[[paste0("Grouping_", i)]] %>% arrange(comp))
}

writexl::write_xlsx(x = sig_comp_icp, path = "Wilcoxon_test_ICP_data_by_Category_Missing-Observation_12Jan2024.xlsx")
```

###### with NA imputation method -TBA

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sig_comp_icp <- list()

for (i in 1:length(icp_list)) {
  stats <- icp_list[[i]] %>%
    select(File, Feature, Category, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(stats)) {
    stats[r, which(base::is.na(stats[r,]))] <- 0
    # stats::runif(length(which(base::is.na(stats[r,]))),
    #                                                         min = sort(hplc_list[[i]]$Values)[1]/10000,
    #                                                         max = sort(hplc_list[[i]]$Values)[2]/10000)
  }
  
  cat_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # create vector of satisfied category 
  satisfied_cat <- c()
  for (cat in unique(stats$Category)) {
    satisfied_cat <- c(cat, satisfied_cat)
  }
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(satisfied_cat, 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(satisfied_cat, 2)[1,col]
      p_2 <- utils::combn(satisfied_cat, 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(stats[which(stats$Category == p_1), c], 
                                      stats[which(stats$Category == p_2), c])$p.value
     
      # assigning row information
      cat_df[nrow(cat_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      
    }
  }
  
  cat_df$adjusted_pvalue_holm <- stats::p.adjust(cat_df$pval_wilcox_test, method = "holm")
  
  sig_comp_icp[[paste0("Grouping_", i)]] <- cat_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sig_comp_icp[[paste0("Grouping_", i)]] %>% arrange(comp))
}

writexl::write_xlsx(x = sig_comp_icp, path = "Wilcoxon_test_ICP_data_by_Category_NA imputed_zeroimputed_12Jan2024.xlsx")
```

#### By Subcategory

Since the subcategory is the same across all 4 different consumer-based perspective groupings, we only need to run one dataframe of the list of df (this principle applies for either gc_hplc_list or gc_list or hplc_list). \##### Do we need to change Data processing-Step 4 (Subsetting Non-unique compounds), for ex At step 4, we need to subset compounds that appear in more than 2 categories instead of 2 files? =\> NO!!

##### COMBINED GC_HPLC

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
gc_hplc_subcat_wilcox <- wilcox_subcategory(gc_hplc_subcat)
```

##### COMBINED GC_ICP

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
gc_icp_subcat_wilcox <- wilcox_subcategory(gc_icp_subcat)
```

##### COMBINED HPLC_ICP

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
hplc_icp_subcat_wilcox <- wilcox_subcategory(hplc_icp_subcat)
```

###### with Missing-Observation method - ZeroImputed18Jan2024
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- hplc_icp_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))

  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) != 0) {
  # Add RT and m.z info to the significant table
  mean_rt <- c()
  mean_mz <- c()
  for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
    idx <- base::which(hplc_icp_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
    mean_rt <- c(mean_rt, round(mean(hplc_icp_list[[1]][idx, ]$RT), 3))
    mean_mz <- c(mean_mz, round(mean(hplc_icp_list[[1]][idx, ]$m.z), 3))
  }

  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
  list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_COMBINED HPLC_ICP_by_Subcategory_Missing-observation_18Jan2024.xlsx")
```

###### with NA imputation method - ZeroImputed18Jan2024
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# stats <- hplc_icp_list[[1]] %>%
#   select(File, Feature, Subcategory, Values) %>%
#   # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
#   group_by(File, Subcategory, Feature) %>%
#   summarise(across(Values, mean)) %>%
#   pivot_wider(names_from = Feature, values_from = Values) %>%
#   column_to_rownames(., var = "File")
#   
# for (r in 1:nrow(stats)) {
#   stats[r, which(base::is.na(stats[r,]))] <- 0
# }

i <- 1
for (dat in hplc_icp) {
  df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(dat)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(unique(dat$Subcategory), 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(unique(dat$Subcategory), 2)[1,col]
      p_2 <- utils::combn(unique(dat$Subcategory), 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(as.numeric(unlist(dat[which(dat$Subcategory == p_1), c])), 
                                      as.numeric(unlist(dat[which(dat$Subcategory == p_2), c])))$p.value
      # assigning row information
      df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                             paste0(p_1, " & ", p_2), 
                             pval_wilcox_test)
    }
  }
  
  df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")
  
  sub_cat <- df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sub_cat %>% arrange(comp))
  
  # # Check if the df is empty before create mean_rt and mean_mz column
  # if (dim(sub_cat)[1] != 0) {
  #   mean_rt <- c()
  #   mean_mz <- c()
  #   for (row in 1:nrow(sub_cat)) {
  #     idx <- base::which(gc_hplc_list[[1]]$Feature == sub_cat[row,]$comp)
  #     mean_rt <- c(mean_rt, mean(gc_hplc_list[[1]][idx, ]$RT))
  #     mean_mz <- c(mean_mz, mean(gc_hplc_list[[1]][idx, ]$m.z))
  #   }
  #   
  #   sub_cat$mean_rt <- mean_rt
  #   sub_cat$mean_mz <- mean_mz
  # }
  
  filename <- str_split(names(hplc_icp)[i], pattern = "/", simplify = TRUE)
  writexl::write_xlsx(x = sub_cat, path = paste0("Wilcoxon_test_Subcategory_",format(Sys.Date()), "_", filename[ncol(filename)]))
  i <- i + 1
}
```




##### COMBINED GC_HPLC_ICP

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
gc_hplc_icp_subcat_wilcox <- wilcox_subcategory(gc_hplc_icp_subcat)
```

###### with Missing-Observation method - ZeroImputed18Jan2024
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- gc_hplc_icp_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))

  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) != 0) {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
      idx <- base::which(gc_hplc_icp_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
      mean_rt <- c(mean_rt, round(mean(gc_hplc_icp_list[[1]][idx, ]$RT), 3))
      mean_mz <- c(mean_mz, round(mean(gc_hplc_icp_list[[1]][idx, ]$m.z), 3))
    }
    
    list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
    list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}


writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Subcategory_Missing-observation_18Jan2024.xlsx")
```

###### with NA imputation method - ZeroImputed18Jan2024
```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# stats <- gc_hplc_icp_list[[1]] %>%
#   select(File, Feature, Subcategory, Values) %>%
#   # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
#   group_by(File, Subcategory, Feature) %>%
#   summarise(across(Values, mean)) %>%
#   pivot_wider(names_from = Feature, values_from = Values) %>%
#   column_to_rownames(., var = "File")
#   
# for (r in 1:nrow(stats)) {
#   stats[r, which(base::is.na(stats[r,]))] <- 0
# }

i <- 1
for (dat in gc_hplc_icp) {
  df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(dat)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(unique(dat$Subcategory), 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(unique(dat$Subcategory), 2)[1,col]
      p_2 <- utils::combn(unique(dat$Subcategory), 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(as.numeric(unlist(dat[which(dat$Subcategory == p_1), c])), 
                                      as.numeric(unlist(dat[which(dat$Subcategory == p_2), c])))$p.value
      # assigning row information
      df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                             paste0(p_1, " & ", p_2), 
                             pval_wilcox_test)
    }
  }
  
  df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")
  
  sub_cat <- df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sub_cat %>% arrange(comp))
  
  # # Check if the df is empty before create mean_rt and mean_mz column
  # if (dim(sub_cat)[1] != 0) {
  #   mean_rt <- c()
  #   mean_mz <- c()
  #   for (row in 1:nrow(sub_cat)) {
  #     idx <- base::which(gc_hplc_list[[1]]$Feature == sub_cat[row,]$comp)
  #     mean_rt <- c(mean_rt, mean(gc_hplc_list[[1]][idx, ]$RT))
  #     mean_mz <- c(mean_mz, mean(gc_hplc_list[[1]][idx, ]$m.z))
  #   }
  #   
  #   sub_cat$mean_rt <- mean_rt
  #   sub_cat$mean_mz <- mean_mz
  # }
  
  filename <- str_split(names(gc_hplc_icp)[i], pattern = "/", simplify = TRUE)
  writexl::write_xlsx(x = sub_cat, path = paste0("Wilcoxon_test_Subcategory_",format(Sys.Date()), "_", filename[ncol(filename)]))
  i <- i + 1
}
```



##### ATDGCMS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
gc_subcat_wilcox <- wilcox_subcategory(gc_subcat)


# for (i in 1:length(dflist)) {
  df <- data.frame(Feature=character(), comparison_pair=character(), pval_wilcox_test=integer())
  dat <- df_percentage
  # Loop through each chemical feature columns
  for (c in 3:ncol(dat)) {
    # if the compound is GC then only include data point of GC sample in the Wilcoxon test, then subset the data to only include GC sample
    if (str_detect(colnames(dat)[c], "GC")) {
      tempdf <- dat[which(dat$technique == "GC"), c(1,c)]
    }
    # if the compound is HPLC then only include data point of HPLC sample in the Wilcoxon test
    else if (str_detect(colnames(dat)[c], "HPLC")) {
      tempdf <- dat[which(dat$technique == "HPLC"), c(1,c)]
    }
    # if the compound is ICP then only include data point of ICP sample in the Wilcoxon test
    else {
      tempdf <- dat[which(dat$technique == "ICP"), c(1,c)]
    }
    
    # remove any Subcategory that have less than 3 samples
    tempdf_filtered <- tempdf[!(tempdf$Subcategory %in% names(table(tempdf$Subcategory)[table(tempdf$Subcategory) < 3])),]
    
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(unique(tempdf_filtered$Subcategory), 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(unique(tempdf_filtered$Subcategory), 2)[1,col]
      p_2 <- utils::combn(unique(tempdf_filtered$Subcategory), 2)[2,col]
      
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(as.numeric(unlist(tempdf_filtered[which(tempdf_filtered$Subcategory == p_1), 2])), 
                                      as.numeric(unlist(tempdf_filtered[which(tempdf_filtered$Subcategory == p_2), 2])))$p.value
      # assigning row information
      df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                             paste0(p_1, " & ", p_2), 
                             pval_wilcox_test)
    }
  }
  
  
  
  pvaluecorrect <- c("holm", "hochberg", "hommel", "BH", "BY")
  for (m in pvaluecorrect) {
    df$adjusted_pvalue <- stats::p.adjust(df$pval_wilcox_test, method = m)
    # writexl::write_xlsx(x = df %>%
    #                       filter(., adjusted_pvalue < 0.05) %>%
    #                       arrange(adjusted_pvalue), 
    #                     path = paste0("Wilcoxon_test_Subcategory_", m, "_", format(Sys.Date()), "_", filename[ncol(filename)]))
    print(df %>%
            filter(., adjusted_pvalue < 0.05) %>%
            arrange(adjusted_pvalue))

  }

```

###### With Missing Observation method - ZeroImputed18Jan2024

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- gc_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) != 0) {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
      idx <- base::which(gc_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
      mean_rt <- c(mean_rt, round(mean(gc_list[[1]][idx, ]$RT), 3))
      mean_mz <- c(mean_mz, round(mean(gc_list[[1]][idx, ]$m.z), 3))
    }
    
    list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
    list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_GC_data_by_Subcategory_Missing-observation_18Jan2024.xlsx")
```

###### With NA imputation method - ZeroImputed18Jan2024

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# stats <- gc_list[[1]] %>%
#   select(File, Feature, Subcategory, Values) %>%
#   # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
#   group_by(File, Subcategory, Feature) %>%
#   summarise(across(Values, mean)) %>%
#   pivot_wider(names_from = Feature, values_from = Values) %>%
#   column_to_rownames(., var = "File")
#   
# for (r in 1:nrow(stats)) {
#   stats[r, which(base::is.na(stats[r,]))] <- 0
# }

i <- 1
for (dat in gc) {
  df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(dat)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(unique(dat$Subcategory), 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(unique(dat$Subcategory), 2)[1,col]
      p_2 <- utils::combn(unique(dat$Subcategory), 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(as.numeric(unlist(dat[which(dat$Subcategory == p_1), c])), 
                                      as.numeric(unlist(dat[which(dat$Subcategory == p_2), c])))$p.value
      # assigning row information
      df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                             paste0(p_1, " & ", p_2), 
                             pval_wilcox_test)
    }
  }
  
  df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")
  
  sub_cat <- df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sub_cat %>% arrange(comp))
  
  # # Check if the df is empty before create mean_rt and mean_mz column
  # if (dim(sub_cat)[1] != 0) {
  #   mean_rt <- c()
  #   mean_mz <- c()
  #   for (row in 1:nrow(sub_cat)) {
  #     idx <- base::which(gc_hplc_list[[1]]$Feature == sub_cat[row,]$comp)
  #     mean_rt <- c(mean_rt, mean(gc_hplc_list[[1]][idx, ]$RT))
  #     mean_mz <- c(mean_mz, mean(gc_hplc_list[[1]][idx, ]$m.z))
  #   }
  #   
  #   sub_cat$mean_rt <- mean_rt
  #   sub_cat$mean_mz <- mean_mz
  # }
  
  filename <- str_split(names(gc)[i], pattern = "/", simplify = TRUE)
  writexl::write_xlsx(x = sub_cat, path = paste0("Wilcoxon_test_Subcategory_",format(Sys.Date()), "_", filename[ncol(filename)]))
  i <- i + 1
}
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Subcategory_Missing-observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_GC_data_by_Subcategory_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_GC_data_by_Subcategory_all_significant compounds.xlsx")
```




##### HPLCTOFMS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
hplc_subcat_wilcox <- wilcox_subcategory(hplc_subcat)
```

###### With Missing Observation method - ZeroImputed18Jan2024

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
list_subcat <- list()

stats <- hplc_list[[1]] %>%
    select(File, Feature, Subcategory, Values) %>%
    # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Subcategory, Feature) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = Feature, values_from = Values) %>%
    column_to_rownames(., var = "File")

for (min_obs in 1:10) {
  sp_df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(stats)) {
    satisfied_sp <- c()
    # Loop through each suspected_polymer
    for (sub_cat in unique(stats$Subcategory)) {
      # get all Subcategory that has > n obs
      if (sum(!is.na(stats[which(stats$Subcategory == sub_cat), c])) > min_obs) {
        satisfied_sp <- c(satisfied_sp, sub_cat)
      } else {
        next
      }
    }
    # IF there is only 1 product_cat have more than n obs, then go to the next column (aka. compound)
    if (length(satisfied_sp) < 2) {
      next
    } else {
      # looping through the combinations of product categories 
      for (col in 1:ncol(utils::combn(satisfied_sp, 2))) {
        # extract the combinations of product category pairs
        p_1 <- utils::combn(satisfied_sp, 2)[1,col]
        p_2 <- utils::combn(satisfied_sp, 2)[2,col]
        # calculating the p-value between each product category pair 
        pval_wilcox_test <- wilcox.test(stats[which(stats$Subcategory == p_1), c], 
                                        stats[which(stats$Subcategory == p_2), c])$p.value
       
        # assigning row information
        sp_df[nrow(sp_df) + 1,] <- c(colnames(stats)[c], 
                                     paste0(p_1, " & ", p_2), 
                                     pval_wilcox_test)
      }
    }
  }
  
  sp_df$adjusted_pvalue_holm <- stats::p.adjust(sp_df$pval_wilcox_test, method = "holm")
  
  list_subcat[[paste0("Min_obs_", min_obs)]] <- sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sp_df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(comp))
  
  # Check if the df is empty before create mean_rt and mean_mz column
  if (nrow(list_subcat[[paste0("Min_obs_", min_obs)]]) != 0) {
    # Add RT and m.z info to the significant table
    mean_rt <- c()
    mean_mz <- c()
    for (row in 1:nrow(list_subcat[[paste0("Min_obs_", min_obs)]])) {
      idx <- base::which(hplc_list[[1]]$Feature == list_subcat[[paste0("Min_obs_", min_obs)]][row,]$comp)
      mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 3))
      mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 3))
    }
    
    list_subcat[[paste0("Min_obs_", min_obs)]]$mean_rt <- mean_rt
    list_subcat[[paste0("Min_obs_", min_obs)]]$mean_mz <- mean_mz
  }
}

writexl::write_xlsx(x = list_subcat, path = "Wilcoxon_test_HPLC_data_by_Subcategory_Missing-observation_18Jan2024.xlsx")
```

###### With NA imputation method - ZeroImputed18Jan2024

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
# stats <- hplc_list[[1]] %>%
#   select(File, Feature, Subcategory, Values) %>%
#   # since we have duplicates with different values of the same compound in some samples, we summarize these values by taking the mean of them
#   group_by(File, Subcategory, Feature) %>%
#   summarise(across(Values, mean)) %>%
#   pivot_wider(names_from = Feature, values_from = Values) %>%
#   column_to_rownames(., var = "File")
#   
# for (r in 1:nrow(stats)) {
#   stats[r, which(base::is.na(stats[r,]))] <- 0
# }

i <- 1
for (dat in hplc) {
  df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())
  
  # Loop through each collapsed_compound (as columns)
  for (c in 2:ncol(dat)) {
    # looping through the combinations of product categories 
    for (col in 1:ncol(utils::combn(unique(dat$Subcategory), 2))) {
      # extract the combinations of product category pairs
      p_1 <- utils::combn(unique(dat$Subcategory), 2)[1,col]
      p_2 <- utils::combn(unique(dat$Subcategory), 2)[2,col]
      # calculating the p-value between each product category pair 
      pval_wilcox_test <- wilcox.test(as.numeric(unlist(dat[which(dat$Subcategory == p_1), c])), 
                                      as.numeric(unlist(dat[which(dat$Subcategory == p_2), c])))$p.value
      # assigning row information
      df[nrow(df) + 1,] <- c(colnames(dat)[c], 
                             paste0(p_1, " & ", p_2), 
                             pval_wilcox_test)
    }
  }
  
  df$adjusted_pvalue_holm <- stats::p.adjust(df$pval_wilcox_test, method = "holm")
  
  sub_cat <- df %>%
    filter(., adjusted_pvalue_holm < 0.05) %>%
    arrange(adjusted_pvalue_holm)
  
  print(sub_cat %>% arrange(comp))
  
  # # Check if the df is empty before create mean_rt and mean_mz column
  # if (dim(sub_cat)[1] != 0) {
  #   mean_rt <- c()
  #   mean_mz <- c()
  #   for (row in 1:nrow(sub_cat)) {
  #     idx <- base::which(gc_hplc_list[[1]]$Feature == sub_cat[row,]$comp)
  #     mean_rt <- c(mean_rt, mean(gc_hplc_list[[1]][idx, ]$RT))
  #     mean_mz <- c(mean_mz, mean(gc_hplc_list[[1]][idx, ]$m.z))
  #   }
  #   
  #   sub_cat$mean_rt <- mean_rt
  #   sub_cat$mean_mz <- mean_mz
  # }
  
  filename <- str_split(names(hplc)[i], pattern = "/", simplify = TRUE)
  writexl::write_xlsx(x = sub_cat, path = paste0("Wilcoxon_test_Subcategory_",format(Sys.Date()), "_", filename[ncol(filename)]))
  i <- i + 1
}
```

###### Summary of significant compounds - DONE

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Subcategory_Missing-observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_HPLC_data_by_Subcategory_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$collapsed_compound == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

writexl::write_xlsx(x = all_unique, path = "Wilcoxon_test_HPLC_data_by_Subcategory_all_significant compounds.xlsx")
```



##### ICP

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
icp <- list()

# remove any Subcategory that have less than 3 samples
icpsubcat_filtered <- icp_subcat[!(icp_subcat$Subcategory %in% names(table(icp_subcat$Subcategory)[table(icp_subcat$Subcategory) < 3])),]
      
df <- data.frame(comp=character(), comparison_pair=character(), pval_wilcox_test=integer())

for (c in 2:ncol(icpsubcat_filtered)) {
  # looping through the combinations of product categories 
  for (col in 1:ncol(utils::combn(unique(icpsubcat_filtered$Subcategory), 2))) {
    # extract the combinations of product category pairs
    p_1 <- utils::combn(unique(icpsubcat_filtered$Subcategory), 2)[1,col]
    p_2 <- utils::combn(unique(icpsubcat_filtered$Subcategory), 2)[2,col]
    # calculating the p-value between each product category pair 
    pval_wilcox_test <- wilcox.test(as.numeric(unlist(icpsubcat_filtered[which(icpsubcat_filtered$Subcategory == p_1), c])), 
                                    as.numeric(unlist(icpsubcat_filtered[which(icpsubcat_filtered$Subcategory == p_2), c])))$p.value
    # assigning row information
    df[nrow(df) + 1,] <- c(colnames(icpsubcat_filtered)[c], 
                           paste0(p_1, " & ", p_2), 
                           pval_wilcox_test)
    
  }
}

pvaluecorrect <- c("holm", "hochberg", "hommel", "BH", "BY")
for (i in 1:length(pvaluecorrect)) {
  df$adjusted_pvalue <- stats::p.adjust(df$pval_wilcox_test, method = pvaluecorrect[i])
  
  icp[[i]] <- df %>%
    filter(., adjusted_pvalue < 0.05) %>%
    arrange(adjusted_pvalue)
  print(icp[[i]])
  
  # writexl::write_xlsx(x = df, path = paste0("Wilcoxon_test_Subcategory_", pvaluecorrect[i], "_", format(Sys.Date()), "_", "icp.xlsx"))
}

```

### Retention time and mass-to-charge of top 10 most significant features from ATD-GC-MS

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
top10 <- unique((gc_subcat_wilcox[[6]] %>% arrange(adjusted_pvalue))[1:10,]$comp)

# Plot bar plot of top 10 most significant features
# plotdat <- gc_subcat[[2]] %>%
#   select(c(Subcategory, top10)) %>%
#   pivot_longer(cols = 2:ncol(.), names_to = "Features", values_to = "Values")
# 
# data_summary <- plotdat %>%
#   group_by(Subcategory, Features) %>%
#   summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')
# 
# ggplot(data = data_summary, aes(x = Subcategory, y = mean, fill = Features)) +
#   geom_bar(stat = "identity", position = 'dodge') +
#   geom_errorbar(aes(ymin = mean - sd,
#                     ymax = mean + sd),
#                 position = position_dodge(width = 0.9), width = 0.25) +
#   labs(x = "Plastic product group", y = "Normalized Concentration") +
#   theme_classic(base_size = 30) +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Scatter plot of Retention time and mass-to-charge of top 10 most significant features
rtmz <- gc_list[[1]] %>%
  filter(Feature %in% colnames(gc_subcat[[1]][, 3:ncol(gc_subcat[[1]])])) %>%
  select(Feature, RT, m.z) %>% 
  group_by(Feature) %>% 
  summarise_all(.funs = mean, na.rm = TRUE)

# Merge rtmz with pvalues output
plotdat <- right_join(x = rtmz,
                     y = gc_subcat_wilcox[[6]] %>% filter(., adjusted_pvalue < 0.05), 
                     by = 'Feature')

ggplot(plotdat, aes(x = RT, y = m.z, colour = comparison_pair)) +
  geom_point(size = 2.5) +
  labs(x = "Retention Time", y = "Mass-to-charge (m/z)", colour = "Comparison pair") +
  theme_minimal(base_size = 30) +
  guides(colour = guide_legend(override.aes = list(size = 10))) # Adjust size of the points representing each label on the legend
```

### Retention time and mass-to-charge of Top 10 GC//HPLC features that most important in RandomForest and Gradient Boosting machine

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
top10_rf_gbm <- read.csv(paste0(getwd(), "/toprf_gbm.csv"))

plotdat <- gc_list[[1]] %>% # hplc_list[[1]]
  # filter(Feature %in% colnames(hplc_subcat[[1]][, 3:ncol(hplc_subcat[[1]])])) %>%
  filter(., Feature %in% top10_rf_gbm$Feature) %>%
  select(Feature, RT, m.z) %>% 
  group_by(Feature) %>% 
  summarise_all(.funs = mean, na.rm = TRUE)

ggplot(plotdat, aes(x = RT, y = m.z, colour = Feature)) +
  geom_point(size = 5) +
  labs(x = "Retention Time", y = "Mass-to-charge (m/z)", colour = "Comparison pair") +
  theme_minimal(base_size = 30) +
  guides(colour = guide_legend(override.aes = list(size = 10))) # Adjust size of the points representing each label on the legend

# Plot bar plot of top 10 most significant features
plotdat <- df_percentage %>%
  select(c(Subcategory, top10_rf_gbm$Feature)) %>%
  pivot_longer(cols = 2:ncol(.), names_to = "Features", values_to = "Values")

data_summary <- plotdat %>%
  group_by(Subcategory, Features) %>%
  summarise(mean = mean(Values), sd = sd(Values), .groups = 'drop')

ggplot(data = data_summary, aes(x = Subcategory, y = mean, fill = Features)) +
  geom_bar(stat = "identity", position = 'dodge') +
  geom_errorbar(aes(ymin = mean - sd,
                    ymax = mean + sd),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Plastic product group", y = "Normalized Concentration") +
  theme_classic(base_size = 30) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Matching HPLC Wilcoxon significant compounds from HPLC suspect screening hits (EF)

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
sus_scr <- readxl::read_xlsx(path = paste0(getwd(), "/data/PMT Suspect Screening Plastics Results EF.xlsx"), skip = 1)

# Remove all legacy empty rows from Eric's Excel file 
sus_scr <- sus_scr[rowSums(is.na(sus_scr)) != (ncol(sus_scr) - 1), ]
```

### 1. With HCA

(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from HCA clustering of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - Zeroimputed: AMong 2737 significant compounds -> we found 13 compounds that matches with EF's Suspect Screening
 [1] "4-ethenyl-cyclohexene"                             "4-Aminophenol"                                     "1,2,3,4-tetrahydro-naphthalene"                   
 [4] "1,1'-oxybis[2-methoxy-ethane]"                     "Benzothiazole"                                     "Acetaminophen (Paracetemol)"                      
 [7] "1-methyl-1-phenylethylhydroperoxide"               "Hexahydro-1,3-isobenzofurandione"                  "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"         
[10] "2-amino-5-methyl-benzenesulfonic acid"             "Dibutyl ester phosphoric acid"                     "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[13] "2-(2H-benzotriazol-2-yl)-4-methyl-phenol"  


```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO130-159_18Dec2023.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO151-162_06Dec2023.xlsx")
p3 <- paste0(getwd(), "/Wilcoxon test results/old test results/Wilcoxon_test_Combined GC-HPLC_by_Robust Aichitson_HCA clustering result_CO161-172_06Dec2023.xlsx")

sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Inspect which subdf has maximum number of significant compounds
lapply(sus_pol_combined1, dim)
# Get the no of unique compounds in that subdf
length(unique(sus_pol_combined1[[]]$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unique(sus_pol_combined1[[71]]$comp))

all_unique <- data.frame(comp=unique(c(unique1))) %>%
  # get rid off all GC compounds if HPLC is in combination with other data sources.
  filter(., str_detect(comp,"HPLCTOFMS"))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  # print(idx)
  if (base::identical(idx, integer(0))) {
    next
  } else {
    # print(sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),])
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Polymer_Type_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

### 2. Without HCA
#### 2.1 Polymer Type level
[Update 17 Jan 2024:]
- For HPLC only data, we found the same 6 Suspects as with the "Category" level (Section 2.2 below)
[1] "4-Aminophenol"                       "2,6-dimethyl-benzenamine"           
[3] "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide"
[5] "TEP / Triethyl phosphate"            "N,N'-diphenyl-guanidine" 

- For GC + HPLC data, we found 4 suspects, all 4 of them are nested in the HPLC-only case ==> GC+HPLC found less suspect hit than HPLC-only data:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"   
[4] "N,N'-diphenyl-guanidine"

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_polymer_Missing-Observation.xlsx")
p2 <- paste0(getwd(), "/Wilcoxon test results/Wilcoxon_test_COMBINED GC_HPLC_by_Suspected_Polymer_NA imputed.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list1 <- lapply(sus_pol_combined1, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique1 <- unique(unlist(unique_values_list1))

## NA imputed
sheet_names2 <- excel_sheets(path = p2)
sus_pol_combined2 <- lapply(sheet_names2, function(sheet) read_excel(path = p2, sheet = sheet))

# Using lapply to get unique values from each data frame
unique_values_list2 <- lapply(sus_pol_combined2, function(df) unique(df$comp))

# Unlisting and then finding unique values across all data frames
unique2 <- unique(unlist(unique_values_list2))

all_unique <- data.frame(comp=unique(c(unique1, unique2)))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(gc_hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(gc_hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(gc_hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Polymer_Type_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

#### 2.2 Category level
(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from "Category" of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - MO method: Among 24 significant compounds -> we found 3 compounds that matches with EF's Suspect Screening
 "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"
 
2. HPLC only - Zeroimputed: Among 664 significant compounds -> we found 11 compounds that matches with EF's Suspect Screening
[1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
 [4] "1,2,3,4-tetrahydro-naphthalene"                    "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "TEP / Triethyl phosphate"                          "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"         
[10] "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]" "2-(2H-benzotriazol-2-yl)-4-methyl-phenol"

3. HPLC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"      

4. HPLC + ICP - zeroimputed method:
 [1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
 [4] "1,2,3,4-tetrahydro-naphthalene"                    "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "1,2-benzisothiazol-3(2H)-one 1,1-dioxide"          "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[10] "2-(2H-benzotriazol-2-yl)-4-methyl-phenol" 

5. HPLC + GC - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"   

6. HPLC + GC - Zeroimputed method:
 [1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1H-Benzotriazole"                                 
 [4] "1,3,5-Triazine-2,4,6-triamine"                     "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"              
 [7] "Hexahydro-1,3-isobenzofurandione"                  "TEP / Triethyl phosphate"                          "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"
[10] "Oxybenzone (Benzophenone-3)" 

7. HPLC + GC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"    "Acetaminophen (Paracetemol)" "TEP / Triethyl phosphate"    "N,N'-diphenyl-guanidine"    

8.HPLC + GC + ICP - zeroimputed method:
[1] "N-methyl-benzenamine"                              "4-Aminophenol"                                     "1,3,5-Triazine-2,4,6-triamine"                    
[4] "Acetaminophen (Paracetemol)"                       "1-methyl-1-phenylethylhydroperoxide"               "Hexahydro-1,3-isobenzofurandione"                 
[7] "2,2'-(1,2-diazenediyl)bis[2-methyl-butanenitrile]"

[Update 17 Jan 2024:]
- For HPLC only data, we found also 6 suspect hits that match with Eric Fries's suspect screening excel file:
[1] "4-Aminophenol"                       "2,6-dimethyl-benzenamine"           
[3] "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide"
[5] "TEP / Triethyl phosphate"            "N,N'-diphenyl-guanidine" 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Zeroimputed_HPLCbenchmarksremoved/Wilcoxon_test_COMBINED GC_ICP_by_Subcategory_NA imputed_zeroimputed_18Jan2024.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))
lapply(sus_pol_combined1, dim)

# Unlisting and then finding unique values across all data frames
unique1 <- unique(sus_pol_combined1[[1]]$comp)

all_unique <- data.frame(comp=unique(c(unique1))) %>%
   filter(., str_detect(comp,"HPLCTOFMS"))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,]) # Here we use just hplc_list because when we merge hplc data with other data, the compounds stay the same anyway
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Category_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

#### 2.3 Subcategory level
(Update 26 Jan 2024:) Huy is recording the maximum number of significant compounds from "Subcategory" of all data combinations that include HPLC data and record the number of suspect that matches in each HPLC data combination

1. HPLC only - MO method: Among 24 total significant compounds -> we found 5 compounds that matches with EF's Suspect Screening
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine"

2. HPLC only - Zeroimputed: There is ZERO significant compound

3. HPLC + ICP - MO method: 
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine" 

4. HPLC + ICP - zeroimputed method: There is ZERO significant compound

5. HPLC + GC - MO method:
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine" 

6. HPLC + GC - Zeroimputed method: There is ZERO significant compound

7. HPLC + GC + ICP - MO method:
[1] "2,6-dimethyl-benzenamine"            "Acetaminophen (Paracetemol)"         "1-methyl-1-phenylethylhydroperoxide" "TEP / Triethyl phosphate"           
[5] "N,N'-diphenyl-guanidine"  

8. HPLC + Gc + ICP - zeroimputed method: There is ZERO significant compound

[UPDATE 17 Jan 2024:] 
- For HPLC only resulted in ZERO significant compounds from Wilcoxon tests so we don't include it in this summary
- For GC + HPLC data, there are 4 compounds but they all ATDGCMS so we also don't include it in this summary 

```{r class.source = 'fold-hide', echo=FALSE, warning = FALSE, message=FALSE}
p1 <- paste0(getwd(), "/Wilcoxon test results/Zeroimputed_HPLCbenchmarksremoved/Wilcoxon_test_COMBINED GC_HPLC_ICP_by_Subcategory_Missing-observation_zeroimputedbatch_18Jan2024.xlsx")

## Missing observation
sheet_names1 <- excel_sheets(path = p1)
sus_pol_combined1 <- lapply(sheet_names1, function(sheet) read_excel(path = p1, sheet = sheet))
lapply(sus_pol_combined1, dim)

# Unlisting and then finding unique values across all data frames
unique1 <- unique(sus_pol_combined1[[7]]$comp)

all_unique <- data.frame(comp=unique(c(unique1)))

# Adding RT and molecular ions info of significant compounds:
mean_rt <- c()
mean_mz <- c()
for (row in 1:nrow(all_unique)) {
  idx <- base::which(hplc_list[[1]]$Feature == all_unique[row,])
  mean_rt <- c(mean_rt, round(mean(hplc_list[[1]][idx, ]$RT), 4))
  mean_mz <- c(mean_mz, round(mean(hplc_list[[1]][idx, ]$m.z), 4))
}

all_unique$mean_rt <- mean_rt
all_unique$mean_mz <- mean_mz

# Find in all_unique the compounds that match with suspect hit list in all_unique with molecualr ions in the range of +- 0.0005
# Initiate list of df
df_list <- list()
i <- 1
# Go through each row of sig.comp df
for (suspect_mz in unique(sus_scr$`[M+H]+`)) {
  # if the of the sus_scr fall into the windows +-0.0005 of the sig comp, then report the sig comp and the fetch all row in sus_scr that match
  idx <- which(all_unique$mean_mz <= (suspect_mz + 0.0005) & all_unique$mean_mz >= (suspect_mz - 0.0005))
  if (base::identical(idx, integer(0))) {
    next
  } else {
    temp <- sus_scr[which(sus_scr$`[M+H]+` == suspect_mz),]
    temp$Feature <- paste(all_unique[idx,]$comp, collapse = ', ')
    df_list[[i]] <- temp
    i <- i + 1
  }
}

all_matching_sus_scr <- bind_rows(df_list)
unique(all_matching_sus_scr$`Suspect Name`)

# writexl::write_xlsx(x = all_matching_sus_scr, path = "Wilcoxon_test_HPLC_data_by_Category_matching_with_EF_Suspect_screening_17Jan2024.xlsx")
```

## Demo RF

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(randomForestSRC)

rfsrc.result <- function(dat, split.ratio) {
  set.seed(1234)
  
  plastic_idx <- caret::createDataPartition(dat$Category, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]
  
  # Train

  rf <- randomForestSRC::rfsrc(Category ~ ., 
                                    ntree=1000, 
                                    splitrule = "auc", 
                                    nodesize = 1, #Minumum size of terminal node for classification (1)
                                    # mtry = 21,
                                    importance = "permute", 
                                    samptype = "swr",
                                    membership = TRUE,
                                    perf.type="misclass",
                                    block.size = 1, # cumulative error rate on every tree
                                    data = plastic_trn
  )
  
  # Prediction results
  pred_res <- predict(rf, newdata = plastic_tst, outcome = "test")$predicted
  rownames(pred_res) <- rownames(plastic_tst)
  
  # Variable importance
  imp_var <- vimp(rf, importance = "permute")$importance
  
  # md.obj <- max.subtree(mergePC.rf)
  # best.feature <- md.obj$topvars # extracts the names of the variables in the object md.obj
  
  return(list(rf, 
              pred_res, 
              # pred_avg, 
              imp_var))
}

rfsrc.plots <- function(preddat, rf.mod) {
  newdat <- as.data.frame(preddat) %>%
    tibble::rownames_to_column(., var = "NewFile") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  
  # plots <- list()
  # Make bar graph of prediction probability -----
  a <- ggplot(data = newdat) + 
    geom_col(aes(x = NewFile, y = prob, fill = Category), 
             position = "dodge" # separating stacking prob cols
    ) +
    scale_fill_brewer(palette = "Set2") +
    scale_y_continuous(n.breaks = 10) +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 90))
  
  # plot OOB error rate against the number of trees -------
  b <- plot(ggRandomForests::gg_error(rf.mod))
  
  # Estimate the variables importance --------
  my.rf.vimp <- ggRandomForests::gg_vimp(rf.mod, nvar = 100) # provides the predictor's importance of top 100 predictors
  # plots[[3]] <- 
  c <- plot(my.rf.vimp) # visualises the predictor’s importance

  return(list(a,b,c))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df <- gc_list[[1]] %>%
  dplyr::select(NewFile, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(NewFile, Category, collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(gc_list[[1]]$Values)[1],
                                             max = sort(gc_list[[1]]$Values)[2])
}

rfsrc.result(dat = df, split.ratio = 0.6)[[2]]

rfsrc.plots(preddat = rfsrc.result(dat = df, split.ratio = 0.6)[[2]],
            rf.mod = rfsrc.result(dat = df, split.ratio = 0.6)[[1]])

```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(gc_list)) {
  df_pca <- input_df(gc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[item]][which(gc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

### With just HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
df <- hplctofms %>%
  dplyr::select(File, product_cat, collapsed_compound, Values) %>%
  mutate(product_cat = factor(product_cat, levels = unique(product_cat))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, product_cat, collapsed_compound) %>%
  dplyr::summarise(across(Values, base::mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(hplctofms$Values)[1],
                                             max = sort(hplctofms$Values)[2])
}

RFresult <- rfsrc.result(dat = df, split.ratio = 0.6)
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[item]][which(hplc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE,message=FALSE}
df <- gc_hplc_list[[1]] %>%
  dplyr::select(File, Category, collapsed_compound, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, collapsed_compound) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min1 = sort(gc_hplc_list[[1]]$Values)[1],
                                             min2 = sort(gc_hplc_list[[1]]$Values)[2])
}

RFresult <- rfsrc.result(dat = df, split.ratio = 0.6)

# Make column of true Category of each row
preddat <- base::as.data.frame(RFresult[[2]]) %>% 
  tibble::rownames_to_column(., var = "File") 

preddat$true_category <- gc_hplc_list[[1]][match(preddat$File, gc_hplc_list[[1]]$File),]$Category

# For each row of prediction df, return the column name (aka. Category) with largest prediction probability
predres<- data.frame(predict_cat = colnames(preddat[, 2:6])[apply(preddat[, 2:6], 1, which.max)], 
                     true_cat = preddat$true_category)

rfsrc.plots(preddat = RFresult[[2]],
            rf.mod = RFresult[[1]])
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
rf_list <- list()
for (item in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[item]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[item]][which(gc_hplc_list[[item]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  rf_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  rf_list[[item]] <- rfsrc.result(rf_df, split.ratio = 0.6)
}

rfsrc.plots(preddat = rf_list[[4]][[2]],
            rf.mod = rf_list[[4]][[1]])
```

## CONCLUSION OF RANDOMFOREST ON COMBINATION OF ANALYTICAL DATA

#### =\> For single ATDGCMS or HPLCTOF or Combined ATD/HPLC, all of them gave out the same (OOB) Requested performance error \~ 0.4. All of them have the same confusion matrix results, in every single category was mistakenly categorized as Food contact materials !

### With minimum sample size - Justa test, maybe irrelevant later on

#### PCAtools

```{r, echo=FALSE, warning = FALSE}
PCAtools_mindf <- ptype_n_samp(PCAtools_mergePC, use = "min")
PCAtools_mindf.RFresult <- rfsrc.result(PCAtools_mindf, split.ratio = 0.6, sep = FALSE)
rfsrc.plots(preddat = PCAtools_mindf.RFresult[[2]],
            rf.mod = PCAtools_mindf.RFresult[[1]])
```

### What is the average accuracy for USSB and USE for each product category?

For Mixed_Plastic_Waste, Other, Plastic_Bottles_and_Bottle_Caps, Food_Packaging_Waste, there is a lack of Store-Bought samples.

```{r, echo=FALSE, warning = FALSE}
print(PCAtools_alldf.RFresult[[3]] %>% arrange(plastic_type))
```

### What happens to the prediction accuracy if we use only USSB to predict USE samples? If product doesn't have USSB, then train w/ USE

```{r, echo=FALSE, warning = FALSE, }
PCAtools_alldf.RFresult <- rfsrc.result(PCAtools_alldf, split.ratio = 0.6, sep = TRUE)
rfsrc.plots(preddat = PCAtools_alldf.RFresult[[2]],
            rf.mod = PCAtools_alldf.RFresult[[1]])
```

## Demo LightGBM

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(lightgbm)
library(gridExtra)

lightgbm.result <- function(dat, split.ratio) { # dat must have same format as PCATools input
  
  test <- copy(dat)
  # Must convert product_cat from factors to numeric
  test$Category <- as.numeric(as.factor(test$Category)) - 1L
  # Split train and test sets
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(test$Category, p = 0.6, list = F)
  plastic_trn <- as.matrix(test[plastic_idx, ])
  plastic_tst <- as.matrix(test[-plastic_idx, ])
  
  dtrain <- lgb.Dataset(data = plastic_trn[, 2:ncol(plastic_trn)], # data is all the columns, aka. all the compounds
                        label = plastic_trn[, 1]) # label is Category column
  dtest <- lgb.Dataset.create.valid(dtrain, 
                                    data = plastic_tst[, 2:ncol(plastic_tst)], 
                                    label = plastic_tst[, 1])
  
  # Validation set to be used during training, not testing!!!
  valids <-  list(test = dtest)
  
  # Setup parameters
  params <- list(
    min_data = 1L
    , learning_rate = 0.1
    , objective = "multiclass"
    , metric = "multi_error"
    , num_class = length(unique(dat$Category))
    , boosting = "dart"
    , xgboost_dart_mode ="true"
    , num_threads = 2
  ) 
  
  lgb.model <- lgb.train(params,
                         dtrain,
                         nrounds = 1000,
                         valids, 
                         early_stopping_rounds = 25L
  )
  
  # prediction
  my_preds <- round(predict(object = lgb.model, 
                      data = plastic_tst[, 2:ncol(plastic_tst)],
                      # params = list(output_result = "response"),
                      reshape = TRUE),
                    3)

  colnames(my_preds) <- levels(dat$Category)
  rownames(my_preds) <- rownames(plastic_tst)
  
  newdat <- as.data.frame(my_preds) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  
  # Important features
  tree_imp <- lgb.importance(lgb.model, percentage = T)
  varimpmat <- lgb.plot.importance(tree_imp, measure = "Gain")

  return(list(my_preds, varimpmat, newdat))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# gbm_list <- list()
# for (i in 1:length(gc_list)) {
  df <- gc_list[[2]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    summarise(across(Values, mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_list[[2]]$Values)[1],
                                               max = sort(gc_list[[2]]$Values)[2])
  }
  
  # gbm_list[[i]]
gbm <- lightgbm.result(df, split.ratio = 0.6)
# }

# Make bar graph of prediction probability
ggplot(data = gbm[[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

Here, same problem happened where all most all the sample in test set were misclassified as "Food contact materials"

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_list)) {
  
  df_pca <- input_df(gc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[i]][which(gc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

With PCA df, there is a slight improvement, not everything is classified as food contact material anymore, but there are still lots of misclassification, for ex, some Balloons samples are classified as MPW or as clothes or Misc

### With just HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(hplc_list)) {
  df <- hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(hplc_list[[i]]$Values)[1],
                                               max = sort(hplc_list[[i]]$Values)[2])
  }
  
  gbm_list[[i]] <- lightgbm.result(df, split.ratio = 0.6)
}

# Make bar graph of prediction probability
ggplot(data = gbm_list[[1]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[i]][which(hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df <- gc_hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_hplc_list[[i]]$Values)[1],
                                               max = sort(gc_hplc_list[[i]]$Values)[2])
  }
  
  gbm_list[[i]] <- lightgbm.result(df, split.ratio = 0.6)
}

# Make bar graph of prediction probability
ggplot(data = gbm_list[[1]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
gbm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[i]][which(gc_hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  gbm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  gbm_list[[i]] <- lightgbm.result(gbm_df, split.ratio = 0.6)
}


# Make bar graph of prediction probability
ggplot(data = gbm_list[[4]][[3]]) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

## CONCLUSION OF LIGHTGBM ON COMBINATION OF ANALYTICAL DATA

#### =\> Regardless of single or combined dataset, the same misclassification pattern occured, in which almost everything was misclassified as Food Contact Materials (The error is always \> 0.3, EXCEPT FOR 1 Scenario, where single ATDGCMS-PCA merge df was used --\> error in this is \~ 0.24)

## Demo SVM

#### Data format should be same as PCATools input (column as "sample name", row as "collapsed_compounds")

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(e1071)
library(caret)
library(ROCR)
library(MLmetrics)


# e1071 package: ===========================
# PArtitioning train&test sets / training / predict on test set
e1071.SVM.result <- function(dat, split.ratio){
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(dat$Category, p = split.ratio, list = F)
  plastic_trn <- dat[plastic_idx, ]
  plastic_tst <- dat[-plastic_idx, ]  
  
  # Create 1 million evenly spaced values on a log scale between 10^-6 and 10^2
  # lseq <- function(from = 0.000001, to = 100, length.out=1000000) {
  #   # logarithmic spaced sequence
  #   exp(seq(log(from), log(to), length.out = length.out))
  # }
  
  # Model tuning
  tune.out <- e1071::tune(e1071::svm, Category ~ ., 
                          data = plastic_trn,
                          kernel = "radial",
                          scale = TRUE,
                          # ranges = list(cost = lseq()),
                          decision.values = TRUE, 
                          probability = TRUE)
  bestmod <- tune.out$best.model
  
  # Prediction results
  pred_prob <- predict(bestmod, newdata = plastic_tst,
                       decision.values = TRUE, probability = TRUE)
  pred_res <- attr(pred_prob, "probabilities")
  
  return(pred_res)
}

plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}  
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
svm_list <- list()
for (i in 1:length(gc_list)) {
  df <- gc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_list[[i]]$Values)[1],
                                               max = sort(gc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE, message=FALSE}
# PCA-based feature data
svm_list <- list()
for (i in 1:length(gc_list)) {
  df_pca <- input_df(gc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_list[[i]][which(gc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With just HPLC data

#### Original Data

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(hplc_list)) {
  df <- hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(hplc_list[[i]]$Values)[1],
                                               max = sort(hplc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(hplc_list)) {
  df_pca <- input_df(hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(hplc_list[[i]][which(hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df <- gc_hplc_list[[i]] %>%
    dplyr::select(File, Category, collapsed_compound, Values) %>%
    mutate(Category = factor(Category, levels = unique(Category))) %>%
    # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
    group_by(File, Category, collapsed_compound) %>%
    dplyr::summarise(across(Values, base::mean)) %>%
    pivot_wider(names_from = collapsed_compound, values_from = Values) %>%
    column_to_rownames(., var = "File")
  
  for (r in 1:nrow(df)) {
    df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                               min = sort(gc_hplc_list[[i]]$Values)[1],
                                               max = sort(gc_hplc_list[[i]]$Values)[2])
  }
  
  svm_list[[i]] <- e1071.SVM.result(df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE}
svm_list <- list()
for (i in 1:length(gc_hplc_list)) {
  df_pca <- input_df(gc_hplc_list[[i]])
  
  # PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
  p <- PCAtools::pca(mat = df_pca[[1]], 
                     metadata = df_pca[[2]], 
                     # center = FALSE,
                     scale = FALSE 
  )
  
  df <- p$rotated %>%
    tibble::rownames_to_column(., var = "File")
  
  cat <- c()
  for (file in unique(df$File)) {
    cat <- c(cat, unique(gc_hplc_list[[i]][which(gc_hplc_list[[i]]$File == file),]$Category))
  }  
  df$Category <- cat
  
  svm_df <- df %>%
    dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
    dplyr::relocate(c(Category
    ), .before = 1) %>%
    tibble::column_to_rownames(., var = "File")
  
  
  svm_list[[i]] <- e1071.SVM.result(svm_df, split.ratio = 0.6)
}

mydat <- plot.dat(svm_list[[1]])

# Make bar graph of prediction probability
ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

## CONCLUSION SVM ON COMBINATION OF ANALYTICAL DATA

#### =\> The same problem with RF and GBM: Almost ALL of the classes are misclassified as Food contact material -\__\-

## Demo Multinomial Logistic Regression

```{r, echo=FALSE, warning = FALSE, message=FALSE}
library(class)
library(caret)
library(nnet)
library(parallel)
library(doParallel)

## Link: https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/

# PArtitioning train&test sets / training / predict on test set
caret.multinomlog.result <- function(df, split.ratio){
  cl <- parallel::makeCluster(parallel::detectCores() - 1)
  doParallel::registerDoParallel(cl)
  
  set.seed(1234)
  plastic_idx <- caret::createDataPartition(df$Category, p = 0.5, list = F)
  plastic_trn <- df[plastic_idx, ] %>%
    relocate(Category, .after = 1)
  plastic_tst <- df[-plastic_idx, ] %>%
    relocate(Category, .after = 1)
  
  plastic_multinomlog_mod <- caret::train(
    Category ~ .,
    data = plastic_trn,
    method = "multinom", # Penalized Multinomial Regression -> https://remiller1450.github.io/s230f19/caret3.html
    trControl = caret::trainControl(method = "cv", # "cv", repeatedcv",
                                    # number = 1,
                                    # repeats = 1, # for repeated k-fold cv only
                                    verboseIter = TRUE
    ),
    trace = FALSE,
    allowParallel= TRUE,
    MaxNWts = 6*ncol(plastic_trn) # maximum allowable number of weights
  )
  
  # Prediction results
  pred_res <- round(predict(plastic_multinomlog_mod, newdata = plastic_tst, type = "prob"), 5)
  
  # Extract the prediction label with highest probability for each sample in test set
  max_columns <- factor(unlist(apply(pred_res, 1, function(x) names(pred_res)[which.max(x)]),
                               recursive = FALSE, use.names = FALSE)
                        , levels = unique(plastic_tst$Category)
  )
  
  # Creating the Confusion Matrix
  cm <- caret::confusionMatrix(max_columns, plastic_tst$Category)
  print(cm)
  
  # Visualizing the Confusion Matrix
  ggplot(as.data.frame(cm$table), aes(Reference, Prediction)) +
    geom_tile(aes(fill = Freq), colour = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    geom_text(aes(label = sprintf("%0.0f", Freq)), vjust = 1) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Interpreting variable importance for multinomial logistic regression - `nnet::multinom()` and `caret::varImp()`
  # https://stackoverflow.com/questions/60060292/interpreting-variable-importance-for-multinomial-logistic-regression-nnetmu
  varimp <- caret::varImp(plastic_multinomlog_mod)

  return(list(pred_res, varimp))
}
```

### With just ATDGCMS data

#### Original data

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df <- gc_list[[2]] %>%
  dplyr::select(File, Category, Feature, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(gc_list[[2]]$Values)[1],
                                             max = sort(gc_list[[2]]$Values)[2])
}

multinom <- caret.multinomlog.result(df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- multinom[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(multinom[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools -merged df

```{r, echo=FALSE, warning = FALSE}
# PCA-based feature data
df_pca <- input_df(gc_list[[1]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(gc_list[[1]][which(gc_list[[1]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_atdgcms.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_atdgcms.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_atdgcms.multinomlog.result[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With just HPLC data

#### Original Data

```{r, echo=FALSE, warning = FALSE}
df <- hplc_list[[2]] %>%
  dplyr::select(File, Category, Feature, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- runif(length(which(base::is.na(df[r,]))),
                                             min = sort(hplc_list[[2]]$Values)[1],
                                             max = sort(hplc_list[[2]]$Values)[2])
}

multinom <- caret.multinomlog.result(df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- multinom[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(multinom[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools-merged df

```{r, echo=FALSE, warning = FALSE}
df_pca <- input_df(hplc_list[[1]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(hplc_list[[1]][which(hplc_list[[1]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_hplc.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_hplc.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_hplc.multinomlog.result[[1]])

ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90))
```

### With combinded ATDGCMS and HPLC data

#### Original data

gc_list (1,2,3,4): DONE
icp_list (1,2,3,4): DONE
gc_icp_list (1,2,3,4): DONE

```{r, echo=FALSE, warning = FALSE}
# cl <- parallel::makeCluster(parallel::detectCores() /2)
# doParallel::registerDoParallel(cl)

df <- hplc_list[[2]] %>%
  dplyr::select(File, Category, Feature, Values) %>%
  mutate(Category = factor(Category, levels = unique(Category))) %>%
  # since we have multiple different values of the same compound in some samples, we summarize these values by taking the mean of them
  group_by(File, Category, Feature) %>%
  summarise(across(Values, mean)) %>%
  pivot_wider(names_from = Feature, values_from = Values) %>%
  column_to_rownames(., var = "File")

for (r in 1:nrow(df)) {
  df[r, which(base::is.na(df[r,]))] <- 0 
  # runif(length(which(base::is.na(df[r,]))),
  #                                            min = sort(gc_hplc_list[[1]]$Values)[1],
  #                                            max = sort(gc_hplc_list[[1]]$Values)[2])
}

set.seed(1234)
plastic_idx <- caret::createDataPartition(df$Category, p = 0.5, list = F)
plastic_trn <- df[plastic_idx, ] %>%
  relocate(Category, .after = 1)
plastic_tst <- df[-plastic_idx, ] %>%
  relocate(Category, .after = 1)

table(plastic_trn$Category)
table(plastic_tst$Category)

system.time(plastic_multinomlog_mod <- caret::train(
  Category ~ .,
  data = plastic_trn,
  method = "multinom", # Penalized Multinomial Regression -> https://remiller1450.github.io/s230f19/caret3.html
  trControl = caret::trainControl(method = "cv", # "cv", repeatedcv",
                                  # number = 1,
                                  # repeats = 1, # for repeated k-fold cv only
                                  verboseIter = TRUE
  ),
  trace = FALSE,
  # allowParallel= TRUE,
  MaxNWts = 9*ncol(plastic_trn) # maximum allowable number of weights
)
)

# Prediction results
pred_res <- round(predict(plastic_multinomlog_mod, newdata = plastic_tst, type = "prob"), 5)

# Extract the prediction label with highest probability for each sample in test set
max_columns <- factor(unlist(apply(pred_res, 1, function(x) names(pred_res)[which.max(x)]),
                             recursive = FALSE, use.names = FALSE)
                      , levels = unique(plastic_tst$Category)
)

# Creating the Confusion Matrix
cm <- caret::confusionMatrix(max_columns, plastic_tst$Category)
print(cm)

# Visualizing the Confusion Matrix
ggplot(as.data.frame(cm$table), aes(Reference, Prediction)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  geom_text(aes(label = sprintf("%0.0f", Freq)), vjust = 1) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# gc_hplc.multinomlog.result <- caret.multinomlog.result(df, split.ratio = 0.5)

# # Plot top 20 variable important
# varImp <- gc_hplc.multinomlog.result[[2]]$importance %>% 
#   rownames_to_column(., var = "comp") %>%
#   filter(., Overall > 50) %>% # this is a random threshold
#   arrange(Overall)
# 
# ggplot(data = varImp, 
#        aes(x=Overall,y=comp)) +
#   geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
#   geom_point(color='skyblue') + 
#   xlab(" Importance Score")+
#   ggtitle("Variable Importance") + 
#   theme(plot.title = element_text(hjust = 0.5)) +
#   theme(panel.background = element_rect(fill = 'white', colour = 'black'))


# # Make bar graph of prediction probability
# plot.dat <- function(dat) {
#   newdat <- as.data.frame(dat) %>%
#     tibble::rownames_to_column(., var = "File") %>%
#     tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "product_cat", values_to = "prob")
#   return(newdat)
# }   
# 
# mydat <- plot.dat(gc_hplc.multinomlog.result[[1]])
# 
# ggplot(data = mydat) + 
#   geom_col(aes(x = File, y = prob, fill = product_cat), 
#            position = "dodge" # separating stacking prob cols
#   ) +
#   scale_fill_brewer(palette = "Set2") +
#   scale_y_continuous(n.breaks = 10) +
#   theme_bw() + 
#   theme(axis.text.x = element_text(angle = 90))
```

#### PCAtools - merged df

```{r, echo=FALSE, warning = FALSE, message=FALSE}
df_pca <- input_df(gc_hplc_list[[3]])

# PCAtools::pca requires mat input (columns as "sample name", rows as "collapsed_compound")
p <- PCAtools::pca(mat = df_pca[[1]], 
                   metadata = df_pca[[2]], 
                   # center = FALSE,
                   scale = FALSE 
)

df <- p$rotated %>%
  tibble::rownames_to_column(., var = "File")

cat <- c()
for (file in unique(df$File)) {
  cat <- c(cat, unique(gc_hplc_list[[3]][which(gc_hplc_list[[3]]$File == file),]$Category))
}  
df$Category <- cat

multinom_df <- df %>%
  dplyr::mutate(Category = factor(Category, levels = unique(Category))) %>%
  dplyr::relocate(c(Category
  ), .before = 1) %>%
  tibble::column_to_rownames(., var = "File")


PCATools_gchplc.multinomlog.result <- caret.multinomlog.result(multinom_df, split.ratio = 0.6)

# Plot top 20 variable important
varImp <- PCATools_gchplc.multinomlog.result[[2]]$importance %>% 
  rownames_to_column(., var = "comp") %>%
  filter(., Overall > 50) %>% # this is a random threshold
  arrange(Overall)

print(ggplot(data = varImp, 
       aes(x=Overall,y=comp)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "black") + 
  geom_point(color='skyblue') + 
  xlab(" Importance Score")+
  ggtitle("Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.background = element_rect(fill = 'white', colour = 'black')))


# Make bar graph of prediction probability
plot.dat <- function(dat) {
  newdat <- as.data.frame(dat) %>%
    tibble::rownames_to_column(., var = "File") %>%
    tidyr::pivot_longer(., cols = 2:ncol(.), names_to = "Category", values_to = "prob")
  return(newdat)
}   

mydat <- plot.dat(PCATools_gchplc.multinomlog.result[[1]])

print(ggplot(data = mydat) + 
  geom_col(aes(x = File, y = prob, fill = Category), 
           position = "dodge" # separating stacking prob cols
  ) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(n.breaks = 10) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90)))
```

Food contact materials may have a lot of chemical properties in them that shares with other categories.
